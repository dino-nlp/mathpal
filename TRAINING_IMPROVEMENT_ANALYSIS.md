# MathPal Training Pipeline - PhÃ¢n TÃ­ch vÃ  Äá» Xuáº¥t Cáº£i Thiá»‡n

## ðŸ” PhÃ¢n TÃ­ch Code Hiá»‡n Táº¡i

### Váº¥n Ä‘á» chÃ­nh vá»›i `train_gemma.py` (454 dÃ²ng):

#### 1. **QuÃ¡ nhiá»u Command Line Arguments** âŒ
- **Hiá»‡n táº¡i**: ~50 arguments cáº§n parse manually
- **Váº¥n Ä‘á»**: 
  - Code dÃ i vÃ  khÃ³ Ä‘á»c (dÃ²ng 22-202)
  - KhÃ³ maintain khi thÃªm features má»›i
  - Default values bá»‹ hardcode trong parser
  - Validation logic phÃ¢n tÃ¡n

```python
# VÃ­ dá»¥ code hiá»‡n táº¡i - quÃ¡ phá»©c táº¡p
parser.add_argument("--model-name", type=str, default="unsloth/gemma-3n-E4B-it")
parser.add_argument("--max-seq-length", type=int, default=2048)
parser.add_argument("--load-in-4bit", action="store_true", default=True)
# ... 47 arguments khÃ¡c
```

#### 2. **Logic Override Phá»©c Táº¡p** âŒ
- **Hiá»‡n táº¡i**: 80+ dÃ²ng code Ä‘á»ƒ override config tá»« CLI args (dÃ²ng 221-283)
- **Váº¥n Ä‘á»**:
  - Hardcode default values á»Ÿ nhiá»u nÆ¡i
  - Logic phá»©c táº¡p vÃ  dá»… lá»—i
  - KhÃ³ test cÃ¡c trÆ°á»ng há»£p edge case

```python
# VÃ­ dá»¥ logic override phá»©c táº¡p hiá»‡n táº¡i
if args.model_name != "unsloth/gemma-3n-E4B-it":
    override_dict["model_name"] = args.model_name
if args.max_seq_length != 2048:
    override_dict["max_seq_length"] = args.max_seq_length
# ... 20+ kiá»ƒm tra tÆ°Æ¡ng tá»±
```

#### 3. **Cáº¥u TrÃºc Monolithic** âŒ
- **Hiá»‡n táº¡i**: Má»™t function `main()` khá»•ng lá»“ (dÃ²ng 313-454)
- **Váº¥n Ä‘á»**:
  - KhÃ³ Ä‘á»c vÃ  debug
  - Impossible Ä‘á»ƒ unit test tá»«ng pháº§n
  - Tight coupling giá»¯a cÃ¡c components
  - KhÃ³ reuse code cho cÃ¡c tasks khÃ¡c

#### 4. **Error Handling KÃ©m** âŒ
- **Hiá»‡n táº¡i**: Chá»‰ cÃ³ try-catch cÆ¡ báº£n
- **Váº¥n Ä‘á»**:
  - KhÃ´ng cÃ³ validation rÃµ rÃ ng cho config
  - Error messages khÃ´ng há»¯u Ã­ch
  - KhÃ´ng graceful recovery

## ðŸš€ Äá» Xuáº¥t Kiáº¿n TrÃºc Má»›i

### 1. **Config-Driven Approach** âœ…

**Thay vÃ¬**: 50+ CLI arguments  
**DÃ¹ng**: YAML config files vá»›i CLI override tá»‘i thiá»ƒu

```bash
# CÃ¡ch má»›i - Ä‘Æ¡n giáº£n vÃ  rÃµ rÃ ng
python -m training_pipeline.cli.train_gemma --config configs/complete_training_config.yaml
python -m training_pipeline.cli.train_gemma --config configs/development.yaml --experiment-name my-test
```

### 2. **Modular Architecture** âœ…

```
training_pipeline/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ training_manager.py      # Orchestrate toÃ n bá»™ quÃ¡ trÃ¬nh
â”‚   â”œâ”€â”€ config_loader.py         # Load vÃ  validate config
â”‚   â””â”€â”€ exceptions.py            # Custom exceptions
â”œâ”€â”€ factories/
â”‚   â”œâ”€â”€ model_factory.py         # Táº¡o models theo config
â”‚   â”œâ”€â”€ dataset_factory.py       # Táº¡o datasets theo config
â”‚   â”œâ”€â”€ trainer_factory.py       # Táº¡o trainers theo config
â”‚   â””â”€â”€ optimizer_factory.py     # Táº¡o optimizers theo config
â”œâ”€â”€ managers/
â”‚   â”œâ”€â”€ experiment_manager.py    # Quáº£n lÃ½ experiments
â”‚   â”œâ”€â”€ checkpoint_manager.py    # Quáº£n lÃ½ model saving/loading
â”‚   â””â”€â”€ evaluation_manager.py    # Quáº£n lÃ½ evaluation
â””â”€â”€ cli/
    â””â”€â”€ train_gemma_v2.py        # New CLI interface (100 lines)
```

### 3. **Factory Pattern Implementation** âœ…

#### ModelFactory
```python
class ModelFactory:
    @staticmethod
    def create_model(config: TrainingConfig) -> tuple[Any, Any]:
        """Táº¡o model vÃ  tokenizer theo config."""
        if config.model.name.startswith("unsloth/"):
            return UnslothModelFactory.create(config)
        elif config.model.name.startswith("google/"):
            return HFModelFactory.create(config)
        else:
            raise UnsupportedModelError(f"Model {config.model.name} not supported")
```

#### TrainerFactory 
```python
class TrainerFactory:
    @staticmethod
    def create_trainer(config: TrainingConfig, model, tokenizer, dataset) -> Any:
        """Táº¡o trainer theo config."""
        if config.training.method == "sft":
            return SFTTrainerBuilder.build(config, model, tokenizer, dataset)
        elif config.training.method == "dpo":
            return DPOTrainerBuilder.build(config, model, tokenizer, dataset)
        else:
            raise UnsupportedTrainingMethodError(f"Method {config.training.method} not supported")
```

### 4. **TrainingManager - Orchestrator** âœ…

```python
class TrainingManager:
    def __init__(self, config: TrainingConfig):
        self.config = config
        self.model_factory = ModelFactory()
        self.dataset_factory = DatasetFactory()
        self.trainer_factory = TrainerFactory()
        self.experiment_manager = ExperimentManager(config)
        
    def run_training(self) -> TrainingResults:
        """Main training orchestration."""
        try:
            # 1. Setup experiment tracking
            self.experiment_manager.setup()
            
            # 2. Load model and tokenizer
            model, tokenizer = self.model_factory.create_model(self.config)
            
            # 3. Prepare dataset
            dataset = self.dataset_factory.create_dataset(self.config, tokenizer)
            
            # 4. Create trainer
            trainer = self.trainer_factory.create_trainer(
                self.config, model, tokenizer, dataset
            )
            
            # 5. Run training
            results = trainer.train()
            
            # 6. Save model
            self.save_model(model, tokenizer, results)
            
            # 7. Run evaluation if enabled
            if self.config.inference.test_after_training:
                eval_results = self.run_evaluation(model, tokenizer)
                results.evaluation = eval_results
                
            return results
            
        except Exception as e:
            self.experiment_manager.log_error(e)
            raise
        finally:
            self.experiment_manager.cleanup()
```

### 5. **New CLI Interface** âœ…

```python
# train_gemma_v2.py - CHá»ˆ 100 dÃ²ng thay vÃ¬ 454 dÃ²ng
def main():
    parser = argparse.ArgumentParser(
        description="Train Gemma3N model for Vietnamese math tutoring"
    )
    
    # CHá»ˆ Cáº¦N 5-7 arguments chÃ­nh
    parser.add_argument(
        "--config", "-c", 
        type=str, 
        required=True,
        help="Path to training configuration YAML file"
    )
    parser.add_argument(
        "--experiment-name",
        type=str,
        help="Override experiment name"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        help="Override output directory"
    )
    parser.add_argument(
        "--quick-test",
        action="store_true",
        help="Run quick test with minimal steps"
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="Enable debug logging"
    )
    
    args = parser.parse_args()
    
    try:
        # Load vÃ  validate config
        config = ConfigLoader.load_from_yaml(args.config)
        
        # Apply CLI overrides (chá»‰ vÃ i cÃ¡i quan trá»ng)
        if args.experiment_name:
            config.output.experiment_name = args.experiment_name
        if args.output_dir:
            config.output.base_dir = args.output_dir
        if args.quick_test:
            config = ConfigLoader.apply_quick_test_profile(config)
            
        # Validate config
        config.validate()
        
        # Setup logging
        setup_logging(
            level="DEBUG" if args.debug else config.logging.level,
            log_file=config.logging.log_file
        )
        
        # Run training
        manager = TrainingManager(config)
        results = manager.run_training()
        
        logger.info(f"ðŸŽ‰ Training completed successfully!")
        logger.info(f"ðŸ“Š Final loss: {results.final_loss:.4f}")
        logger.info(f"â±ï¸  Training time: {results.training_time:.2f}s")
        
    except ValidationError as e:
        logger.error(f"âŒ Configuration error: {e}")
        sys.exit(1)
    except TrainingError as e:
        logger.error(f"âŒ Training error: {e}")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ Unexpected error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
```

## ðŸ”§ Config Validation Pipeline

```python
class ConfigValidator:
    @staticmethod
    def validate(config: TrainingConfig) -> List[ValidationError]:
        """Comprehensive config validation."""
        errors = []
        
        # Model validation
        if not ModelFactory.is_supported(config.model.name):
            errors.append(ValidationError(f"Unsupported model: {config.model.name}"))
            
        # Memory validation  
        estimated_memory = MemoryEstimator.estimate(config)
        available_memory = SystemInfo.get_gpu_memory()
        if estimated_memory > available_memory:
            errors.append(MemoryError(
                f"Estimated memory {estimated_memory}GB > Available {available_memory}GB"
            ))
            
        # Training validation
        if config.training.max_steps <= 0 and config.training.num_train_epochs <= 0:
            errors.append(ValidationError("Must specify either max_steps or num_train_epochs"))
            
        # LoRA validation
        if config.lora.r <= 0 or config.lora.alpha <= 0:
            errors.append(ValidationError("LoRA rank and alpha must be positive"))
            
        return errors
```

## ðŸ“Š So SÃ¡nh Before/After

| Aspect | Before (train_gemma.py) | After (New Architecture) |
|--------|------------------------|-------------------------|
| **Lines of Code** | 454 lines | ~100 lines (CLI) |
| **CLI Arguments** | ~50 arguments | ~5 arguments |
| **Configuration** | Hardcoded defaults | YAML-driven |
| **Modularity** | Monolithic | Highly modular |
| **Testability** | Hard to test | Easy unit testing |
| **Maintainability** | Poor | Excellent |
| **Error Handling** | Basic try-catch | Comprehensive validation |
| **Extensibility** | Difficult | Easy to extend |
| **Code Reuse** | Limited | High reusability |

## ðŸ› ï¸ Migration Plan

### Phase 1: Create New Architecture (1 week)
1. âœ… Táº¡o config schema vÃ  validation
2. â³ Implement factories vÃ  managers  
3. â³ Create new CLI interface
4. â³ Write comprehensive tests

### Phase 2: Parallel Implementation (1 week)
1. â³ Keep old `train_gemma.py` working
2. â³ Implement new `train_gemma_v2.py`
3. â³ Test both versions side by side
4. â³ Ensure identical results

### Phase 3: Migration (3 days)
1. â³ Update Makefile to use new version
2. â³ Update documentation
3. â³ Migrate existing configs
4. â³ Remove old implementation

## ðŸŽ¯ Lá»£i Ãch Cá»§a Kiáº¿n TrÃºc Má»›i

### 1. **Developer Experience** âœ…
- **Dá»… Ä‘á»c**: Code ngáº¯n gá»n, cÃ³ cáº¥u trÃºc rÃµ rÃ ng
- **Dá»… debug**: Má»—i component cÃ³ responsibility riÃªng
- **Dá»… test**: Unit test tá»«ng factory, manager riÃªng biá»‡t
- **Dá»… extend**: ThÃªm model má»›i chá»‰ cáº§n implement factory interface

### 2. **Configuration Management** âœ…  
- **Centralized**: Táº¥t cáº£ config á»Ÿ má»™t file YAML
- **Documented**: Má»—i parameter cÃ³ comment giáº£i thÃ­ch
- **Validated**: Comprehensive validation trÆ°á»›c khi training
- **Profileable**: Dá»… dÃ ng táº¡o profiles cho dev/prod/custom

### 3. **Production Ready** âœ…
- **Error Handling**: Graceful error recovery
- **Monitoring**: Built-in experiment tracking
- **Scalability**: Easy to add distributed training
- **Deployment**: Clear separation between training vÃ  inference

### 4. **Vietnamese Math Specific** âœ…
- **Domain Logic**: TÃ¡ch biá»‡t logic specific cho Vietnamese math
- **Evaluation Metrics**: Custom metrics cho math problems  
- **Dataset Handling**: Specialized preprocessing cho Vietnamese text
- **Model Validation**: Test vá»›i Vietnamese mathematical notation

## ðŸ”® Future Enhancements

### 1. **Advanced Features**
- Distributed training support
- Hyperparameter tuning vá»›i Optuna
- Model compression vÃ  quantization
- Custom evaluation metrics cho math problems

### 2. **MLOps Integration**  
- Model versioning vá»›i DVC
- Automated testing pipeline
- Performance monitoring
- Model serving vá»›i FastAPI

### 3. **Vietnamese Math Specific**
- Vietnamese math notation parser
- Specialized tokenization cho math symbols
- Domain-specific evaluation benchmarks
- Integration vá»›i Vietnamese educational standards

## ðŸ“ Káº¿t Luáº­n

Kiáº¿n trÃºc má»›i sáº½ giÃºp:
- âœ… **Giáº£m 80% lines of code** trong CLI
- âœ… **TÄƒng 10x testability** vá»›i modular design  
- âœ… **Cáº£i thiá»‡n developer experience** vá»›i config-driven approach
- âœ… **Chuáº©n bá»‹ sáºµn sÃ ng cho production** vá»›i proper error handling
- âœ… **Dá»… dÃ ng maintain vÃ  extend** cho tÆ°Æ¡ng lai

**Recommendation**: Implement kiáº¿n trÃºc má»›i song song vá»›i version hiá»‡n táº¡i, sau Ä‘Ã³ migrate dáº§n dáº§n Ä‘á»ƒ Ä‘áº£m báº£o stability.
