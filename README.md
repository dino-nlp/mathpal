# MathPal ğŸ¤–

 An AI-powered math buddy to help students transition from 5th to 6th grade.

## GUIDE

- Install libs: `make install`
- Start infra: `make local-start`
- Stop infra: `make local-stop`
- Crawl test data: `make local-test-crawler`
- Crawl full data: `make local-ingest-data`
- Test retriever: `make local-test-retriever`

## **IMPORTANT NOTE:**

- For MongoDB to work with multiple replicas (as we use it in our Docker setup) on macOS or Linux systems, you have to add the following lines of code to `/etc/hosts`:

  - 127.0.0.1       mongo1
  - 127.0.0.1       mongo2 
  - 127.0.0.1       mongo3

-  Qdrant UI: `localhost:6333/dashboard`

## ğŸš€ Gemma3N Fine-tuning Pipeline

Má»™t pipeline modular vÃ  dá»… má»Ÿ rá»™ng Ä‘á»ƒ fine-tune mÃ´ hÃ¬nh Gemma3N cho bÃ i toÃ¡n há»— trá»£ há»c toÃ¡n lá»›p 6 báº±ng tiáº¿ng Viá»‡t. Pipeline Ä‘Æ°á»£c xÃ¢y dá»±ng vá»›i Unsloth, TRL, PEFT vÃ  Comet ML.

### âœ¨ TÃ­nh nÄƒng

- **Modular Architecture**: Cáº¥u trÃºc module rÃµ rÃ ng, dá»… báº£o trÃ¬ vÃ  má»Ÿ rá»™ng
- **Unsloth Integration**: Tá»‘i Æ°u hÃ³a tá»‘c Ä‘á»™ vÃ  memory vá»›i Unsloth
- **Multiple Training Methods**: Há»— trá»£ LoRA, QLoRA vÃ  full fine-tuning
- **Experiment Tracking**: TÃ­ch há»£p vá»›i Comet ML Ä‘á»ƒ theo dÃµi thÃ­ nghiá»‡m
- **Flexible Configuration**: Há»— trá»£ cáº¥u hÃ¬nh qua YAML/JSON files
- **Multiple Save Formats**: LÆ°u model á»Ÿ nhiá»u Ä‘á»‹nh dáº¡ng (LoRA, merged, GGUF)
- **Inference Engine**: Engine inference vá»›i nhiá»u tÃ¹y chá»n generation
- **Command Line Interface**: CLI Ä‘Æ¡n giáº£n vÃ  máº¡nh máº½

### ğŸ—ï¸ Cáº¥u trÃºc dá»± Ã¡n

```
src/training_pipeline/
â”œâ”€â”€ config/                     # Quáº£n lÃ½ cáº¥u hÃ¬nh
â”‚   â”œâ”€â”€ base_config.py          # Base configuration class
â”‚   â”œâ”€â”€ training_config.py      # Training configuration
â”‚   â””â”€â”€ comet_config.py         # Comet ML configuration
â”œâ”€â”€ data/                       # Xá»­ lÃ½ dá»¯ liá»‡u
â”‚   â”œâ”€â”€ dataset_processor.py    # Dataset loading vÃ  processing
â”‚   â””â”€â”€ chat_formatter.py       # Chat template formatting
â”œâ”€â”€ models/                     # Quáº£n lÃ½ model
â”‚   â”œâ”€â”€ model_loader.py         # Model loading vá»›i Unsloth
â”‚   â”œâ”€â”€ lora_config.py          # LoRA configuration
â”‚   â””â”€â”€ model_saver.py          # Model saving utilities
â”œâ”€â”€ training/                   # Training logic
â”‚   â”œâ”€â”€ trainer_factory.py      # SFTTrainer setup
â”‚   â””â”€â”€ training_utils.py       # Training utilities
â”œâ”€â”€ experiments/                # Experiment tracking
â”‚   â””â”€â”€ comet_tracker.py        # Comet ML integration
â”œâ”€â”€ inference/                  # Inference engine
â”‚   â””â”€â”€ inference_engine.py     # Model inference
â”œâ”€â”€ cli/                        # Command line interface
â”‚   â””â”€â”€ train_gemma.py          # Main training script
â””â”€â”€ utils/                      # Utilities
    â”œâ”€â”€ logging.py              # Logging utilities
    â””â”€â”€ device_utils.py         # Device management
```

### ğŸ› ï¸ CÃ i Ä‘áº·t

#### YÃªu cáº§u há»‡ thá»‘ng

- Python 3.8+
- CUDA 11.8+ (khuyáº¿n nghá»‹)
- GPU vá»›i Ã­t nháº¥t 8GB VRAM

#### CÃ i Ä‘áº·t dependencies

```bash
# CÃ i Ä‘áº·t core dependencies
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# CÃ i Ä‘áº·t Unsloth vÃ  cÃ¡c thÆ° viá»‡n liÃªn quan
pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
pip install --no-deps xformers trl peft accelerate bitsandbytes

# CÃ i Ä‘áº·t experiment tracking (optional)
pip install comet-ml wandb

# CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n khÃ¡c
pip install transformers datasets tokenizers sentencepiece
pip install pyyaml rich click
```

#### CÃ i Ä‘áº·t tá»« source

```bash
git clone <repository-url>
cd gemma3n-training-pipeline
pip install -e .
```

### ğŸš€ Sá»­ dá»¥ng

#### 1. Cáº¥u hÃ¬nh environment variables

```bash
# Comet ML (optional)
export COMET_API_KEY="your-api-key"
export COMET_WORKSPACE="your-workspace" 
export COMET_PROJECT="your-project"

# HuggingFace Hub (optional, for model upload)
export HF_TOKEN="your-token"
```

#### 2. Training cÆ¡ báº£n

```bash
# Training vá»›i cáº¥u hÃ¬nh máº·c Ä‘á»‹nh
python -m training_pipeline.cli.train_gemma

# Training vá»›i cáº¥u hÃ¬nh tÃ¹y chá»‰nh
python -m training_pipeline.cli.train_gemma \
    --config configs/training_config.yaml \
    --experiment-name my_experiment \
    --max-steps 500
```

#### 3. Training vá»›i cÃ¡c cáº¥u hÃ¬nh cÃ³ sáºµn

```bash
# Development (quick test)
python -m training_pipeline.cli.train_gemma \
    --config configs/development.yaml

# Production (full training)
python -m training_pipeline.cli.train_gemma \
    --config configs/production.yaml \
    --push-to-hub \
    --hub-username your-username
```

#### 4. Training vá»›i cÃ¡c tÃ¹y chá»n nÃ¢ng cao

```bash
python -m training_pipeline.cli.train_gemma \
    --model-name unsloth/gemma-3n-E4B-it \
    --dataset-name your-dataset \
    --max-steps 1000 \
    --batch-size 2 \
    --learning-rate 1e-4 \
    --lora-r 16 \
    --lora-alpha 32 \
    --save-formats lora merged_fp16 gguf_q8 \
    --test-model
```

### âš™ï¸ Cáº¥u hÃ¬nh

#### Training Configuration

Táº¡o file cáº¥u hÃ¬nh YAML:

```yaml
# training_config.yaml
model_name: "unsloth/gemma-3n-E4B-it"
max_seq_length: 2048
dataset_name: "ngohongthai/exam-sixth_grade-instruct-dataset"

# Training settings
max_steps: 100
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 0.0002

# LoRA settings
lora_r: 8
lora_alpha: 8
lora_dropout: 0.0

# Output
output_dir: "outputs/my-experiment"
experiment_name: "baseline"
```


#### Comet ML Configuration

```python
from training_pipeline.config import CometConfig

comet_config = CometConfig(
    workspace="your-workspace",
    project="gemma3n-finetuning",
    experiment_name="my_experiment",
    tags=["gemma3n", "math-tutor", "vietnamese"]
)
```

### ğŸ“Š Experiment Tracking

Pipeline há»— trá»£ tracking vá»›i:

- **Comet ML**: Tracking metrics, parameters, models
- **TensorBoard**: Local tracking
- **Weights & Biases**: Community platform

VÃ­ dá»¥ vá»›i Comet ML:

```python
from training_pipeline.experiments import CometTracker

tracker = CometTracker(comet_config)
experiment = tracker.setup_experiment(training_config)

# Training sáº½ tá»± Ä‘á»™ng log metrics
# trainer.train()

tracker.log_model("path/to/saved/model")
tracker.end_experiment()
```

### ğŸ¯ Inference

Sá»­ dá»¥ng trained model cho inference:

```python
from training_pipeline.inference import InferenceEngine
from training_pipeline.models import ModelLoader

# Load model
model_loader = ModelLoader(config)
model, tokenizer = model_loader.load_complete_model()

# Setup inference
engine = InferenceEngine(model, tokenizer)

# Generate response
question = "TÃ­nh tá»•ng cá»§a 15 + 27 = ?"
answer = engine.generate(question)
print(f"Q: {question}")
print(f"A: {answer}")

# Test vá»›i nhiá»u cÃ¢u há»i
test_results = engine.test_model()
```

### ğŸ’¾ Model Saving

Pipeline há»— trá»£ lÆ°u model á»Ÿ nhiá»u Ä‘á»‹nh dáº¡ng:

```python
from training_pipeline.models import ModelSaver

saver = ModelSaver(model, tokenizer)

# LÆ°u LoRA adapters
saver.save_lora_adapters("models/lora-adapters")

# LÆ°u merged model
saver.save_merged_model("models/merged-fp16", precision="fp16")

# LÆ°u GGUF cho llama.cpp
saver.save_gguf_model("models/gguf", quantization="q8_0")

# LÆ°u táº¥t cáº£ formats
results = saver.save_all_formats(
    base_save_path="models",
    model_name="gemma3n-math-tutor",
    formats={
        "lora": {},
        "merged_fp16": {"precision": "fp16"},
        "gguf_q8": {"quantization": "q8_0"}
    }
)
```

### ğŸ”§ Customization

#### ThÃªm dataset processor má»›i

```python
from training_pipeline.data import DatasetProcessor

class CustomDatasetProcessor(DatasetProcessor):
    def custom_preprocessing(self, sample):
        # Custom logic
        return processed_sample
```

#### ThÃªm LoRA configuration

```python
from training_pipeline.models import LoRAConfigManager

custom_config = LoRAConfigManager.create_lora_config(
    r=32,
    lora_alpha=64,
    target_modules=["q_proj", "v_proj", "gate_proj"],
    lora_dropout=0.1
)
```

#### Custom training logic

```python
from training_pipeline.training import TrainerFactory

class CustomTrainerFactory(TrainerFactory):
    def create_custom_trainer(self, model, tokenizer, dataset):
        # Custom trainer setup
        return trainer
```

### ğŸ“ˆ Monitoring vÃ  Debugging

#### Memory monitoring

```python
from training_pipeline.utils import DeviceUtils

# Print device info
DeviceUtils.print_device_info()

# Monitor memory during training
result, memory_stats = DeviceUtils.monitor_memory_usage(trainer.train)
```

#### Logging

```python
from training_pipeline.utils import setup_logging, TrainingLogger

# Setup logging
setup_logging(log_level="DEBUG", log_file="training.log")

# Use training logger
logger = TrainingLogger()
logger.info("Training started")
logger.metric("loss", 0.5, step=100)
logger.success("Training completed")
```

### ğŸ§ª Testing

#### Quick test

```bash
python -m training_pipeline.cli.train_gemma --quick-test
```

#### Unit tests

```bash
pytest tests/
```

#### Benchmark inference

```python
questions = ["CÃ¢u há»i 1", "CÃ¢u há»i 2", "CÃ¢u há»i 3"]
results = engine.benchmark_inference(questions, num_runs=3)
print(f"Average tokens/second: {results['avg_tokens_per_second']:.1f}")
```

### ğŸ¤ Contributing

1. Fork repository
2. Táº¡o feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Táº¡o Pull Request

### ğŸ“ License

Distributed under the MIT License. See `LICENSE` for more information.

### ğŸ™ Acknowledgments

- [Unsloth](https://github.com/unslothai/unsloth) - Fast LLM fine-tuning
- [TRL](https://github.com/huggingface/trl) - Transformer Reinforcement Learning
- [PEFT](https://github.com/huggingface/peft) - Parameter-Efficient Fine-Tuning
- [Comet ML](https://www.comet.ml/) - Experiment tracking
- [Gemma](https://deepmind.google/technologies/gemma/) - Base model

### ğŸ“§ Contact

Dino - ngohongthai.uet@gmail.com

Project Link: [https://github.com/username/gemma3n-training-pipeline](https://github.com/username/gemma3n-training-pipeline)