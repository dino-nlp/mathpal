# ğŸ§® MathPal - Vietnamese Math Education AI Platform

A comprehensive AI platform for Vietnamese math education, featuring data crawling, processing, model training, and inference capabilities. MathPal is designed to help students learn mathematics through intelligent tutoring and personalized assistance.

## ğŸŒŸ Features

### ğŸ“Š Data Pipeline
- **Web Crawling**: Automated data collection from Vietnamese math education websites
- **Data Processing**: Intelligent cleaning, chunking, and embedding of math problems
- **Stream Processing**: Real-time data flow with Bytewax integration
- **Quality Control**: Automated validation and filtering of educational content

### ğŸ¤– AI Training Pipeline
- **Modular Architecture**: Clean, maintainable, and extensible training pipeline
- **Hardware Optimization**: Pre-configured for Tesla T4 (16GB) and A100 (40GB/80GB)
- **Unsloth Integration**: Optimized speed and memory usage
- **Multiple Training Methods**: Support for LoRA, QLoRA, and full fine-tuning
- **Experiment Tracking**: Comet ML integration for experiment monitoring
- **Flexible Configuration**: YAML-based configuration management
- **Multiple Save Formats**: Model saving in various formats (LoRA, merged, GGUF)

### ğŸ¯ Inference Engine
- **Real-time Generation**: Fast inference with optimized models
- **Vietnamese Math Focus**: Specialized for Vietnamese mathematics
- **Batch Processing**: Efficient handling of multiple queries
- **Quality Assessment**: Built-in evaluation metrics

## ğŸ—ï¸ Project Architecture

```
mathpal/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_crawling/           # Web crawling and data collection
â”‚   â”‚   â”œâ”€â”€ crawlers/            # Website-specific crawlers
â”‚   â”‚   â”œâ”€â”€ dispatcher.py        # Crawling orchestration
â”‚   â”‚   â””â”€â”€ main.py             # Crawling entry point
â”‚   â”‚
â”‚   â”œâ”€â”€ feature_pipeline/        # Data processing and feature engineering
â”‚   â”‚   â”œâ”€â”€ data_flow/          # Stream processing with Bytewax
â”‚   â”‚   â”œâ”€â”€ data_logic/         # Data transformation logic
â”‚   â”‚   â”œâ”€â”€ models/             # Data models and schemas
â”‚   â”‚   â”œâ”€â”€ utils/              # Processing utilities
â”‚   â”‚   â””â”€â”€ main.py             # Pipeline orchestration
â”‚   â”‚
â”‚   â”œâ”€â”€ training_pipeline/       # Model training and fine-tuning
â”‚   â”‚   â”œâ”€â”€ config/             # Configuration management
â”‚   â”‚   â”œâ”€â”€ managers/           # Training orchestration
â”‚   â”‚   â”œâ”€â”€ factories/          # Object factories
â”‚   â”‚   â”œâ”€â”€ training/           # Training logic
â”‚   â”‚   â”œâ”€â”€ inference/          # Inference engine
â”‚   â”‚   â”œâ”€â”€ experiments/        # Experiment tracking
â”‚   â”‚   â”œâ”€â”€ cli/               # Command line interface
â”‚   â”‚   â””â”€â”€ utils/             # Training utilities
â”‚   â”‚
â”‚   â””â”€â”€ core/                   # Shared core components
â”‚       â”œâ”€â”€ db/                # Database connections
â”‚       â”œâ”€â”€ rag/               # Retrieval-Augmented Generation
â”‚       â”œâ”€â”€ mq/                # Message queue integration
â”‚       â””â”€â”€ utils/             # Shared utilities
â”‚
â”œâ”€â”€ configs/                    # Configuration files
â”‚   â”œâ”€â”€ tesla_t4_optimized.yaml # Tesla T4 optimized config
â”‚   â”œâ”€â”€ a100_optimized.yaml     # A100 optimized config
â”‚   â”œâ”€â”€ quick_test.yaml         # Quick development testing
â”‚   â”œâ”€â”€ production.yaml         # Production configuration
â”‚   â””â”€â”€ README_OPTIMIZATION.md  # Hardware optimization guide
â”‚
â”œâ”€â”€ data/                       # Data storage
â”œâ”€â”€ outputs/                    # Training outputs
â”œâ”€â”€ notebooks/                  # Jupyter notebooks
â””â”€â”€ scripts/                    # Utility scripts
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- CUDA 11.8+ (recommended)
- GPU with at least 8GB VRAM
- Poetry (for dependency management)

### Installation

```bash
# Clone the repository
git clone <repository-url>
cd mathpal

# Install dependencies
make install

# Setup environment
make setup-env

# Check environment
make env-check
```

### Data Collection

```bash
# Start data crawling
make local-start

# Crawl specific websites
make local-test-crawler

# Ingest data from links
make local-ingest-data
```

### Model Training

```bash
# Tesla T4 (16GB VRAM)
make train-tesla-t4

# A100 (40GB/80GB VRAM)
make train-a100

# Quick test
make train-quick

# Production training
make train-prod
```

## ğŸ”§ Hardware Optimization

MathPal provides pre-optimized configurations for different hardware:

### Tesla T4 (16GB VRAM)
- **Model**: Gemma-3n-E2B (smaller)
- **Mixed Precision**: fp16 (required)
- **Quantization**: 4-bit
- **Batch Size**: 2 + gradient accumulation 8
- **Training Time**: ~2-3 hours

### A100 (40GB/80GB VRAM)
- **Model**: Gemma-3n-E4B (full)
- **Mixed Precision**: bf16 (optimal)
- **Quantization**: 4-bit
- **Batch Size**: 8 + gradient accumulation 4
- **Training Time**: ~30-45 minutes

```bash
# View hardware optimization guide
make show-hardware-guide

# List all configurations
make list-configs
```

## ğŸ“Š Data Pipeline

### Web Crawling

The data crawling system automatically collects Vietnamese math problems from educational websites:

```python
from src.data_crawling.main import start_crawling

# Start crawling with configuration
start_crawling(
    websites=["loigiaihay.com", "vietjack.com"],
    grade_levels=["grade_5", "grade_6"],
    max_pages=1000
)
```

### Data Processing

The feature pipeline processes raw data into training-ready format:

```python
from src.feature_pipeline.main import run_pipeline

# Process data with streaming
run_pipeline(
    input_data="data/raw/",
    output_data="data/processed/",
    chunk_size=512,
    embedding_model="sentence-transformers/all-MiniLM-L6-v2"
)
```

## ğŸ¤– Training Pipeline

### Configuration Management

MathPal uses a unified configuration system:

```yaml
# configs/tesla_t4_optimized.yaml
model:
  name: "unsloth/gemma-3n-E2B-it"
  max_seq_length: 1024
  load_in_4bit: true

training:
  max_steps: 200
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 8
  learning_rate: 3.0e-4

lora:
  r: 16
  alpha: 32
  dropout: 0.0
```

### Training Commands

```bash
# Hardware-specific training
make train-tesla-t4          # Tesla T4 optimized
make train-a100              # A100 optimized

# Development and testing
make train-quick             # Quick test (20 steps)
make train-dry-run-tesla-t4  # Validate Tesla T4 config

# Custom training
make train-custom-config CONFIG=my-config.yaml
make train-custom EXPERIMENT=my-experiment
```

### Experiment Tracking

```python
from src.training_pipeline.experiments import CometTracker

# Setup experiment tracking
tracker = CometTracker(
    workspace="mathpal",
    project="vietnamese-math",
    experiment_name="tesla-t4-training"
)

# Training automatically logs metrics
trainer.train()
```

## ğŸ¯ Inference

### Basic Inference

```python
from src.training_pipeline.inference import InferenceEngine

# Load trained model
engine = InferenceEngine(
    model_path="outputs/mathpal-tesla-t4-optimized/",
    device="cuda"
)

# Generate responses
questions = [
    "TÃ­nh tá»•ng cá»§a 15 + 27 = ?",
    "Má»™t hÃ¬nh chá»¯ nháº­t cÃ³ chiá»u dÃ i 8cm vÃ  chiá»u rá»™ng 5cm. TÃ­nh chu vi.",
    "Lan cÃ³ 24 cÃ¡i káº¹o, Lan cho báº¡n 8 cÃ¡i. Há»i Lan cÃ²n láº¡i bao nhiÃªu?"
]

for question in questions:
    answer = engine.generate(question)
    print(f"Q: {question}")
    print(f"A: {answer}\n")
```

### Batch Processing

```python
# Process multiple questions efficiently
answers = engine.generate_batch(
    questions=questions,
    batch_size=4,
    temperature=0.7
)
```

### Quality Evaluation

```python
# Evaluate model performance
evaluation_results = engine.evaluate_model(
    test_dataset="data/test/",
    metrics=["accuracy", "bleu", "rouge"]
)

print(f"Accuracy: {evaluation_results['accuracy']:.2f}")
print(f"BLEU Score: {evaluation_results['bleu']:.2f}")
```

## ğŸ“ˆ Monitoring and Debugging

### Environment Information

```bash
# Check system information
make env-info

# Monitor GPU usage
make show-gpu-usage

# Check memory usage
make memory-info
```

### Training Monitoring

```bash
# Monitor training logs
make monitor-training

# Test configurations
make test-configs

# Validate specific config
make validate-config CONFIG=configs/tesla_t4_optimized.yaml
```

## ğŸ§ª Testing

### Quick Testing

```bash
# Quick test on Tesla T4
make train-tesla-t4-quick

# Quick test on A100
make train-a100-quick

# Test all configurations
make test-hardware-configs
```

### Data Pipeline Testing

```bash
# Test crawler
make local-test-crawler

# Test retriever
make local-test-retriever
```

## ğŸ“Š Performance Benchmarks

| Hardware | Training Time | VRAM Usage | Throughput | Model Size |
|----------|---------------|------------|------------|------------|
| Tesla T4 | ~2-3 hours | ~12-14GB | ~2-3 samples/sec | 2B parameters |
| A100 | ~30-45 min | ~25-35GB | ~8-12 samples/sec | 4B parameters |

## ğŸ”§ Customization

### Adding New Data Sources

```python
from src.data_crawling.crawlers.base import BaseCrawler

class CustomCrawler(BaseCrawler):
    def extract_math_problems(self, page_content):
        # Custom extraction logic
        return problems
```

### Custom Training Configuration

```python
from src.training_pipeline.config import ConfigManager

# Create custom config
config = ConfigManager()
config.model.name = "custom-model"
config.training.max_steps = 500
config.save_config("configs/custom.yaml")
```

### Custom Evaluation Metrics

```python
from src.training_pipeline.managers import EvaluationManager

class CustomEvaluator(EvaluationManager):
    def custom_metric(self, predictions, targets):
        # Custom evaluation logic
        return score
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Development Setup

```bash
# Setup development environment
make setup-env

# Run tests
make test-configs
make test-hardware-configs

# Check code quality
make env-check
```

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [Unsloth](https://github.com/unslothai/unsloth) - Fast LLM fine-tuning
- [TRL](https://github.com/huggingface/trl) - Transformer Reinforcement Learning
- [PEFT](https://github.com/huggingface/peft) - Parameter-Efficient Fine-Tuning
- [Comet ML](https://www.comet.ml/) - Experiment tracking
- [Gemma](https://deepmind.google/technologies/gemma/) - Base model
- [Bytewax](https://bytewax.io/) - Stream processing
- [Vietnamese Math Education Community](https://loigiaihay.com/) - Data sources

## ğŸ“§ Contact

- **Project Maintainer**: [Your Name]
- **Email**: your.email@example.com
- **Project Link**: [https://github.com/username/mathpal](https://github.com/username/mathpal)

## ğŸ“š Documentation

- [Hardware Optimization Guide](configs/README_OPTIMIZATION.md)
- [Makefile Updates](MAKEFILE_UPDATES.md)
- [Training Pipeline Architecture](src/training_pipeline/README.md)
- [Data Pipeline Documentation](src/data_crawling/README.md)

---

**MathPal** - Empowering Vietnamese students with AI-powered math education! ğŸ§®âœ¨