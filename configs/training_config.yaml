# Gemma3N Training Configuration
# This is an example configuration file for the Gemma3N fine-tuning pipeline

# Model settings
model_name: "unsloth/gemma-3n-E4B-it"
max_seq_length: 2048
load_in_4bit: true
full_finetuning: false

# Dataset settings
dataset_name: "ngohongthai/exam-sixth_grade-instruct-dataset"
train_split: "train"
test_split: "test"

# Output settings
output_dir: "outputs/gemma3n-finetune"
experiment_name: "baseline"

# Training hyperparameters
max_steps: 100
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 0.0002
warmup_ratio: 0.03
weight_decay: 0.01
logging_steps: 5
save_steps: 50

# LoRA settings
lora_r: 8
lora_alpha: 8
lora_dropout: 0.0
lora_bias: "none"
lora_target_modules: null  # Auto-detect for Gemma3N

# System settings
use_gradient_checkpointing: false
report_to: "comet_ml"  # Options: "comet_ml", "tensorboard", "wandb", "none"
seed: 42

# Optimizer settings
optim: "adamw_torch_fused"
lr_scheduler_type: "cosine"

# Additional training settings
train_on_responses_only: true
dataset_text_field: "text"
max_length: null  # Uses max_seq_length if null