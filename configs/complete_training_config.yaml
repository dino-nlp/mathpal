# =============================================================================
# MathPal Training Configuration - Complete Setup
# =============================================================================
# This is a comprehensive configuration file for training Vietnamese math tutor
# models using Gemma-3n with Unsloth optimization framework.
#
# Based on:
# - Unsloth best practices for optimization
# - Gemma model specific recommendations  
# - Vietnamese educational content requirements
# =============================================================================

# =============================================================================
# MODEL CONFIGURATION
# =============================================================================
model:
  # Model identifier from HuggingFace Hub
  # Gemma-3n-E4B-it: 4B parameters, instruction-tuned variant optimized for chat
  name: "unsloth/gemma-3n-E4B-it"
  
  # Maximum sequence length for input/output processing
  # Recommended: 1024-2048 for Gemma models to balance performance and memory
  max_seq_length: 2048
  
  # Use 4-bit quantization to reduce memory usage by ~75%
  # Essential for training large models on consumer hardware
  load_in_4bit: true
  
  # Use 8-bit quantization (alternative to 4-bit, slightly more accurate)
  # Only enable one quantization method at a time
  load_in_8bit: false
  
  # Enable full fine-tuning instead of LoRA (requires significantly more memory)
  # Recommended: false for most use cases to enable LoRA efficiency
  full_finetuning: false

# =============================================================================
# DATASET CONFIGURATION  
# =============================================================================
dataset:
  # Primary dataset for Vietnamese 6th grade math problems
  name: "ngohongthai/exam-sixth_grade-instruct-dataset"
  
  # Training data split - typically "train"
  train_split: "train"
  
  # Test/validation split - use for evaluation if available
  test_split: "test"
  
  # Text field in dataset containing the training examples
  # Standard format for chat datasets
  text_field: "text"
  
  # Maximum length for training sequences (uses model.max_seq_length if null)
  max_length: null
  
  # Number of CPU processes for dataset preprocessing
  # Increase for faster data loading on multi-core systems
  num_proc: 2
  
  # Pack multiple short sequences together to improve efficiency
  # Recommended: true for datasets with variable length sequences  
  packing: true

# =============================================================================
# TRAINING HYPERPARAMETERS
# =============================================================================
training:
  # Total number of training steps
  # Development: 20-100, Production: 500-2000, Full training: 5000+
  max_steps: 100
  
  # Number of training epochs (alternative to max_steps)
  # Use either max_steps OR num_train_epochs, not both
  num_train_epochs: null
  
  # Batch size per GPU/device for training
  # Gemma recommendation: 1-4 depending on available memory
  per_device_train_batch_size: 2
  
  # Batch size per GPU/device for evaluation  
  per_device_eval_batch_size: 2
  
  # Number of gradient accumulation steps
  # Effective batch size = per_device_train_batch_size × gradient_accumulation_steps × num_gpus
  gradient_accumulation_steps: 8
  
  # Learning rate - critical hyperparameter
  # Gemma recommendation: 1e-4 to 5e-4 for LoRA fine-tuning
  learning_rate: 2.0e-4
  
  # Fraction of training steps for learning rate warmup
  # Helps stabilize training at the beginning
  warmup_ratio: 0.03
  
  # Weight decay for regularization (prevents overfitting)
  # Standard range: 0.01-0.1
  weight_decay: 0.01
  
  # Optimizer selection
  # "adamw_8bit": Memory efficient AdamW (recommended for Unsloth)
  # "adamw_torch_fused": Faster but uses more memory
  # "adafactor": Alternative optimizer, good for large models
  optim: "adamw_8bit"
  
  # Learning rate scheduler type
  # "cosine": Gradually decreases LR following cosine curve (recommended)
  # "linear": Linear decrease
  # "constant": No scheduling
  lr_scheduler_type: "cosine"
  
  # Maximum gradient norm for clipping (prevents exploding gradients)
  max_grad_norm: 1.0
  
  # Enable mixed precision training for speed and memory savings
  fp16: false  # Use fp16 on older GPUs
  bf16: true   # Use bf16 on modern GPUs (A100, H100, RTX 30/40 series)
  
  # Train only on response tokens, not prompts
  # Improves efficiency for instruction-following tasks
  train_on_responses_only: true

# =============================================================================
# LORA CONFIGURATION
# =============================================================================
# Low-Rank Adaptation: Efficient fine-tuning technique that reduces
# trainable parameters by 99%+ while maintaining performance
lora:
  # LoRA rank - number of trainable parameters in adaptation matrices
  # Higher rank = more parameters = better adaptation but slower training
  # Gemma recommendation: 8-32 (16 is sweet spot for most tasks)
  r: 16
  
  # LoRA alpha - scaling parameter for adaptation strength  
  # Rule of thumb: set equal to or 2x the rank value
  alpha: 32
  
  # Dropout probability for LoRA layers (regularization)
  # 0.0 = no dropout (recommended by Unsloth for optimization)
  # 0.05-0.1 = light regularization if overfitting occurs
  dropout: 0.0
  
  # Bias handling in LoRA
  # "none": No bias adaptation (recommended, most efficient)
  # "all": Adapt all biases (slower, potentially better performance)
  bias: "none"
  
  # Target modules for LoRA adaptation
  # Gemma-specific modules for attention and feed-forward layers
  # null = auto-detect (Unsloth will choose optimal modules)
  target_modules:
    - "q_proj"      # Query projection in attention
    - "k_proj"      # Key projection in attention  
    - "v_proj"      # Value projection in attention
    - "o_proj"      # Output projection in attention
    - "gate_proj"   # Gate projection in FFN
    - "up_proj"     # Up projection in FFN
    - "down_proj"   # Down projection in FFN
  
  # Use rank-stabilized LoRA (experimental)
  use_rslora: false
  
  # LoRA Fine-Tuning-Aware Quantization config
  loftq_config: null

# =============================================================================
# SYSTEM AND OPTIMIZATION
# =============================================================================
system:
  # Random seed for reproducibility
  seed: 42
  
  # Enable gradient checkpointing to reduce memory usage
  # "unsloth": Unsloth's optimized checkpointing (30% less VRAM)
  # true: Standard PyTorch checkpointing  
  # false: No checkpointing (fastest but highest memory)
  use_gradient_checkpointing: "unsloth"
  
  # Dataloader settings
  dataloader_drop_last: true
  dataloader_pin_memory: true
  dataloader_num_workers: 0  # Set to 0 if using Windows

# =============================================================================
# OUTPUT AND SAVING
# =============================================================================
output:
  # Base directory for all outputs
  base_dir: "outputs"
  
  # Experiment name (will be appended to base_dir)
  experiment_name: "gemma3n-vietnamese-math"
  
  # Model saving strategy
  # "steps": Save every save_steps
  # "epoch": Save every epoch
  # "no": Don't save checkpoints during training
  save_strategy: "steps"
  
  # Number of steps between saves (if save_strategy = "steps")
  save_steps: 100
  
  # Maximum number of checkpoints to keep (saves disk space)
  save_total_limit: 3
  
  # Load best model at end of training (requires evaluation)
  load_best_model_at_end: false
  
  # Model formats to save after training
  save_formats:
    - "lora"           # LoRA adapters only (smallest, fastest)
    - "merged_16bit"   # Full model in 16-bit (for VLLM, production)
    # - "merged_4bit"  # Full model in 4-bit (for HuggingFace inference)
    # - "gguf_q8"      # GGUF format for llama.cpp
    # - "gguf_f16"     # GGUF format in 16-bit

# =============================================================================
# EVALUATION
# =============================================================================
evaluation:
  # Evaluation strategy
  # "steps": Evaluate every eval_steps
  # "epoch": Evaluate every epoch  
  # "no": No evaluation during training
  strategy: "steps"
  
  # Number of steps between evaluations (if strategy = "steps")
  eval_steps: 50
  
  # Metric to monitor for best model selection
  metric_for_best_model: "eval_loss"
  
  # Whether higher metric values are better
  greater_is_better: false
  
  # Enable full precision evaluation to avoid OOM
  fp16_full_eval: true
  
  # Evaluation batch accumulation steps (reduces memory usage)
  eval_accumulation_steps: 4

# =============================================================================
# LOGGING AND MONITORING
# =============================================================================
logging:
  # Number of steps between logging
  steps: 10
  
  # Report metrics to external services
  # Options: "tensorboard", "wandb", "comet_ml", "none"
  report_to: "none"
  
  # Logging level for console output
  level: "INFO"
  
  # Log file path (optional)
  log_file: null

# =============================================================================
# COMET ML CONFIGURATION
# =============================================================================
# Experiment tracking and monitoring (optional)
comet:
  # Enable Comet ML tracking
  enabled: false
  
  # Comet ML project settings (set via environment variables)
  # COMET_API_KEY: Your API key
  # COMET_WORKSPACE: Your workspace name  
  # COMET_PROJECT: Project name
  
  # Experiment name for Comet
  experiment_name: "gemma3n-math-tutor"
  
  # Tags for organizing experiments
  tags:
    - "gemma3n"
    - "vietnamese"
    - "math-education"
    - "6th-grade"
    - "instruction-tuning"
  
  # Automatic logging settings
  auto_metric_logging: true
  auto_param_logging: true
  auto_histogram_weight_logging: false
  auto_histogram_gradient_logging: false
  auto_histogram_activation_logging: false
  auto_output_logging: "default"
  
  # Model logging
  log_model: true
  log_graph: false
  log_code: true
  log_git_metadata: true

# =============================================================================
# INFERENCE AND TESTING
# =============================================================================
inference:
  # Enable model testing after training
  test_after_training: true
  
  # Number of test examples to run
  num_test_examples: 5
  
  # Generation parameters for testing
  generation:
    max_new_tokens: 512
    temperature: 0.7
    top_p: 0.9
    do_sample: true
    pad_token_id: null  # Will be set automatically

# =============================================================================
# HUGGINGFACE HUB INTEGRATION
# =============================================================================
hub:
  # Push model to HuggingFace Hub after training
  push_to_hub: false
  
  # HuggingFace Hub username (set via HF_USERNAME env var or here)
  username: null
  
  # Repository name (defaults to experiment_name if null)
  repo_name: null
  
  # Model visibility on Hub
  private: true
  
  # Use authentication token (set via HF_TOKEN env var)
  token: null

# =============================================================================
# DEVELOPMENT PROFILES
# =============================================================================
# Override settings for different environments
# Use these by copying values to the main sections above

profiles:
  # Quick development testing
  development:
    training:
      max_steps: 20
      per_device_train_batch_size: 1
      gradient_accumulation_steps: 4
    model:
      max_seq_length: 1024
    output:
      experiment_name: "dev-test"
      save_steps: 10
    logging:
      steps: 2
    evaluation:
      strategy: "steps"
      eval_steps: 10
  
  # Production training
  production:
    training:
      max_steps: 2000
      per_device_train_batch_size: 4
      gradient_accumulation_steps: 8
      learning_rate: 1.0e-4
    lora:
      r: 32
      alpha: 64
    output:
      experiment_name: "production-v1"
      save_steps: 200
    evaluation:
      strategy: "steps"
      eval_steps: 100
    comet:
      enabled: true
    hub:
      push_to_hub: true

# =============================================================================
# NOTES AND BEST PRACTICES
# =============================================================================
# 1. Memory Usage:
#    - Start with small batch sizes and increase gradually
#    - Use gradient checkpointing if running out of memory
#    - Enable 4-bit quantization for consumer GPUs
#
# 2. Training Speed:
#    - Use bf16 on modern GPUs, fp16 on older ones
#    - Increase batch size and reduce gradient accumulation if possible
#    - Use "adamw_torch_fused" optimizer if memory allows
#
# 3. Model Quality:
#    - Increase LoRA rank (r) for better adaptation (but slower training)
#    - Use learning rate around 2e-4 for LoRA fine-tuning
#    - Monitor eval loss to detect overfitting
#
# 4. Vietnamese Math Specific:
#    - Ensure dataset contains diverse problem types
#    - Test with various mathematical notation formats
#    - Validate model understanding of Vietnamese mathematical terms
#
# 5. Production Deployment:
#    - Save in "merged_16bit" format for VLLM deployment
#    - Use "gguf_q8" format for llama.cpp deployment
#    - Keep LoRA adapters for quick model updates
# =============================================================================
