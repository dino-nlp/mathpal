# MathPal Quick Evaluation Configuration
# For fast testing with limited samples

experiment_name: "quick_evaluation"
output_dir: "./evaluation_outputs"

model:
  name: "unsloth/gemma-3n-E2B-it"
  max_seq_length: 2048
  load_in_4bit: true
  batch_size: 4

dataset:
  source: "huggingface"
  dataset_id: "ngohongthai/exam-sixth_grade-instruct-dataset"
  split: "test"
  max_samples: 3  # Quick test with only 3 samples

evaluation:
  mode: "quick"
  save_predictions: false
  
  # Simplified metrics
  metrics:
    opik:
      enabled: true
      metrics: ["answer_relevance", "usefulness"]
    
    vietnamese_math:
      enabled: true
      metrics: ["mathematical_accuracy", "vietnamese_language_quality"]

openrouter:
  # API key will be read from OPENROUTER_API_KEY environment variable
  base_url: "https://openrouter.ai/api/v1"
  models:
    primary: "qwen/qwen3-8b:free"
    fallback: "openai/gpt-oss-20b:free"

opik:
  # API key will be read from OPIK_API_KEY environment variable
  workspace: "mathpal"
  project: "vietnamese-math-evaluation"
  batch_size: 4
  max_samples: 3
  
  # Metrics to evaluate
  metrics:
    - "answer_relevance"
    - "usefulness"
  
  # LLM-as-a-judge configuration
  llm_judge:
    provider: "openrouter"
    model: "openai/gpt-4o"
    temperature: 0.0
    max_tokens: 1000

hardware:
  device: "auto"
  memory_efficient: true

logging:
  level: "DEBUG"
  format: "json"
  output: "console"

performance:
  enable_profiling: false
  enable_metrics: false
