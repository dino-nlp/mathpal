# =============================================================================
# MathPal Unified Training Configuration v3
# =============================================================================
# This is the standardized configuration format for the new ConfigManager system.
# It solves inconsistency issues between different config formats and provides
# type-safe access to configuration sections.
#
# Compatible with: ConfigManager, train_gemma_v3.py
# =============================================================================

# =============================================================================
# MODEL CONFIGURATION
# =============================================================================
model:
  # Model identifier from HuggingFace Hub
  name: "unsloth/gemma-3n-E4B-it"
  
  # Maximum sequence length for input/output processing
  max_seq_length: 2048
  
  # Use 4-bit quantization to reduce memory usage
  load_in_4bit: true
  
  # Use 8-bit quantization (alternative to 4-bit)
  load_in_8bit: false
  
  # Enable full fine-tuning instead of LoRA (requires more memory)
  full_finetuning: false

# =============================================================================
# DATASET CONFIGURATION  
# =============================================================================
dataset:
  # Vietnamese 6th grade math problems
  name: "ngohongthai/exam-sixth_grade-instruct-dataset"
  
  # Standard splits
  train_split: "train"
  test_split: "test"
  
  # Text field configuration
  text_field: "text"
  
  # columns
  instruction_column: "question"
  answer_column: "solution"

# =============================================================================
# TRAINING HYPERPARAMETERS
# =============================================================================
training:
  # Total number of training steps
  max_steps: 100
  
  # Number of training epochs (use either max_steps OR num_train_epochs)
  num_train_epochs: null
  
  # Batch sizes per GPU/device
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  
  # Gradient accumulation steps (effective_batch_size = batch_size * grad_accum * num_gpus)
  gradient_accumulation_steps: 8
  
  # Learning rate for LoRA fine-tuning
  learning_rate: 2.0e-4
  
  # Learning rate warmup fraction
  warmup_ratio: 0.03
  
  # Weight decay for regularization
  weight_decay: 0.01
  
  # Optimizer selection
  optim: "adamw_8bit"
  
  # Learning rate scheduler
  lr_scheduler_type: "cosine"
  
  # Maximum gradient norm for clipping
  max_grad_norm: 1.0
  
  # Mixed precision training
  fp16: false
  bf16: true
  
  # Train only on response tokens, not prompts
  train_on_responses_only: true

# =============================================================================
# LORA CONFIGURATION
# =============================================================================
lora:
  # LoRA rank (number of trainable parameters in adaptation matrices)
  r: 16
  
  # LoRA alpha (scaling parameter for adaptation strength)
  alpha: 32
  
  # Dropout probability for LoRA layers
  dropout: 0.0
  
  # Bias handling in LoRA
  bias: "none"
  
  # Target modules for LoRA adaptation (Gemma-specific)
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  
  # Use rank-stabilized LoRA
  use_rslora: false

# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================
logging:
  logging_steps: 5
  report_to: null   # can be ["tensorboard", "wandb", "comet_ml"]

# =============================================================================
# SYSTEM AND OPTIMIZATION
# =============================================================================
system:
  # Random seed for reproducibility
  seed: 42
  
  # Gradient checkpointing ("unsloth", true, false)
  use_gradient_checkpointing: "unsloth"
  
  # Dataloader settings
  dataloader_drop_last: true
  dataloader_pin_memory: true
  dataloader_num_workers: 0

# =============================================================================
# OUTPUT AND SAVING
# =============================================================================
output:
  # Base directory for all outputs
  base_dir: "outputs"
  
  # Experiment name (appended to base_dir)
  experiment_name: "gemma3n-unified-training"
  
  # Model saving strategy
  save_strategy: "steps"
  
  # Number of steps between saves
  save_steps: 100
  
  # Maximum number of checkpoints to keep
  save_total_limit: 3
  
  # Load best model at end of training
  load_best_model_at_end: false
  
  # Model formats to save after training
  save_formats:
    - "lora"           # LoRA adapters only
    - "merged_16bit"   # Full model in 16-bit

# =============================================================================
# COMET ML CONFIGURATION
# =============================================================================
comet:
  # Experiment name for Comet
  experiment_name: "gemma3n-unified-experiment"
  
  # Tags for organizing experiments
  tags:
    - "gemma3n"
    - "vietnamese"
    - "math-education"
    - "unified-config"
  
  # Automatic logging settings
  auto_metric_logging: true
  auto_param_logging: true

# =============================================================================
# HUGGINGFACE HUB INTEGRATION
# =============================================================================
hub:
  # Enable automatic model push to Hub
  push_to_hub: true
  
  # HuggingFace configuration (use env vars)
  username: 'ngohongthai'  # Set via HF_USERNAME env var
  repo_name: "mathpal-gemma3n-vietnamese-math"
  private: false  # Make public for community use
  token: null     # Set via HF_TOKEN env var

evaluation:
  regression_test: true

# =============================================================================
# ENVIRONMENT PROFILES (for quick overrides)
# =============================================================================
profiles:
  # Quick development testing
  development:
    training:
      max_steps: 20
      per_device_train_batch_size: 1
      gradient_accumulation_steps: 4
    model:
      max_seq_length: 1024
    output:
      experiment_name: "dev-test-unified"
      save_steps: 10
  
  # Production training
  production:
    training:
      max_steps: 2000
      per_device_train_batch_size: 4
      gradient_accumulation_steps: 8
      learning_rate: 1.0e-4
    lora:
      r: 32
      alpha: 64
    output:
      experiment_name: "production-unified-v1"
      save_steps: 200
    comet:
      enabled: true
    hub:
      push_to_hub: true

# =============================================================================
# BACKWARD COMPATIBILITY MAPPING
# =============================================================================
# For legacy systems that still use flat config format:
# model_name -> model.name
# dataset_name -> dataset.name  
# output_dir -> output.base_dir
# experiment_name -> output.experiment_name
# lora_r -> lora.r
# lora_alpha -> lora.alpha
# etc.
# =============================================================================
