#!/usr/bin/env python3
"""
Test script for InferenceEngine after fixing tokenization issues.
"""

import sys
import os
sys.path.append('/content')  # Add to path for Colab

from src.training_pipeline.inference.inference_engine import InferenceEngine
import torch
from unsloth import FastLanguageModel

def setup_model_and_tokenizer():
    """Setup model and tokenizer for testing."""
    print("üîß Setting up model and tokenizer...")
    
    # Load model and tokenizer (adjust model name as needed)
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name="unsloth/Llama-3.2-1B-Instruct",  # Change to your model
        max_seq_length=512,
        dtype=None,
        load_in_4bit=True,
    )
    
    print(f"‚úÖ Model loaded: {type(model).__name__}")
    print(f"‚úÖ Tokenizer loaded: {type(tokenizer).__name__}")
    
    return model, tokenizer

def test_basic_inference(inference_engine):
    """Test basic inference functionality."""
    print("\n" + "="*50)
    print("üß™ TESTING BASIC INFERENCE")
    print("="*50)
    
    test_questions = [
        "T√≠nh 15 + 27 = ?",
        "T√¨m chu vi h√¨nh vu√¥ng c·∫°nh 5cm?",
        "Gi·∫£i ph∆∞∆°ng tr√¨nh: x + 10 = 25"
    ]
    
    for i, question in enumerate(test_questions, 1):
        print(f"\n--- Test {i} ---")
        print(f"Q: {question}")
        
        try:
            response = inference_engine.generate(question)
            print(f"A: {response}")
            print("‚úÖ Success")
        except Exception as e:
            print(f"‚ùå Error: {e}")

def test_different_configs(inference_engine):
    """Test different generation configs."""
    print("\n" + "="*50)
    print("üéõÔ∏è TESTING DIFFERENT CONFIGS")
    print("="*50)
    
    configs = InferenceEngine.get_recommended_configs()
    test_question = "T√≠nh di·ªán t√≠ch h√¨nh ch·ªØ nh·∫≠t c√≥ chi·ªÅu d√†i 8m, chi·ªÅu r·ªông 5m?"
    
    print(f"Test question: {test_question}")
    
    for config_name, config in configs.items():
        print(f"\n--- {config_name.upper()} CONFIG ---")
        print(f"Config: {config}")
        
        try:
            response = inference_engine.generate(
                question=test_question,
                generation_config=config
            )
            print(f"Response: {response}")
            print("‚úÖ Success")
        except Exception as e:
            print(f"‚ùå Error: {e}")

def test_benchmark_function(inference_engine):
    """Test the fixed benchmark function."""
    print("\n" + "="*50)
    print("‚è±Ô∏è TESTING BENCHMARK FUNCTION")
    print("="*50)
    
    benchmark_questions = [
        "T√≠nh 25 + 15 = ?",
        "T√¨m chu vi h√¨nh tr√≤n b√°n k√≠nh 3cm?",
        "Gi·∫£i ph∆∞∆°ng tr√¨nh: 2x + 4 = 14"
    ]
    
    try:
        print(f"Running benchmark with {len(benchmark_questions)} questions...")
        results = inference_engine.benchmark_inference(
            questions=benchmark_questions,
            num_runs=2
        )
        
        print("\nüìä BENCHMARK RESULTS:")
        print(f"- Average time: {results['avg_time']:.2f}s")
        print(f"- Average tokens: {results['avg_tokens']:.1f}")
        print(f"- Average tokens/second: {results['avg_tokens_per_second']:.1f}")
        print("‚úÖ Benchmark completed successfully!")
        
    except Exception as e:
        print(f"‚ùå Benchmark failed: {e}")
        import traceback
        traceback.print_exc()

def test_batch_generation(inference_engine):
    """Test batch generation."""
    print("\n" + "="*50)
    print("üì¶ TESTING BATCH GENERATION")
    print("="*50)
    
    batch_questions = [
        "T√≠nh 12 + 8 = ?",
        "T√¨m di·ªán t√≠ch h√¨nh vu√¥ng c·∫°nh 4cm?",
        "Trong l·ªõp c√≥ 30 h·ªçc sinh, 18 h·ªçc sinh nam. C√≥ bao nhi√™u h·ªçc sinh n·ªØ?"
    ]
    
    try:
        print(f"Generating responses for {len(batch_questions)} questions...")
        responses = inference_engine.generate_batch(
            questions=batch_questions,
            batch_size=2
        )
        
        print("\nüìù BATCH RESULTS:")
        for i, (q, a) in enumerate(zip(batch_questions, responses), 1):
            print(f"\n{i}. Q: {q}")
            print(f"   A: {a}")
        
        print("‚úÖ Batch generation completed!")
        
    except Exception as e:
        print(f"‚ùå Batch generation failed: {e}")

def test_edge_cases(inference_engine):
    """Test edge cases and error handling."""
    print("\n" + "="*50)
    print("üö® TESTING EDGE CASES")
    print("="*50)
    
    edge_cases = [
        "",  # Empty string
        "   ",  # Whitespace only
        "A" * 1000,  # Very long input
        "C√¢u h·ªèi n√†y c√≥ d·∫•u ti·∫øng Vi·ªát v·ªõi k√Ω t·ª± ƒë·∫∑c bi·ªát: !@#$%^&*()",  # Special characters
    ]
    
    for i, test_input in enumerate(edge_cases, 1):
        print(f"\n--- Edge Case {i} ---")
        print(f"Input: '{test_input[:50]}{'...' if len(test_input) > 50 else ''}'")
        
        try:
            response = inference_engine.generate(test_input)
            print(f"Response: {response[:100]}{'...' if len(response) > 100 else ''}")
            print("‚úÖ Handled successfully")
        except Exception as e:
            print(f"‚ùå Error: {e}")

def main():
    """Main test function."""
    print("üöÄ INFERENCE ENGINE TEST SUITE")
    print("="*50)
    
    try:
        # Setup
        model, tokenizer = setup_model_and_tokenizer()
        
        # Create inference engine
        inference_engine = InferenceEngine(
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=64,
            temperature=0.8,
            top_p=0.9
        )
        
        # Run tests
        test_basic_inference(inference_engine)
        test_different_configs(inference_engine)
        test_batch_generation(inference_engine)
        test_edge_cases(inference_engine)
        
        # Test the fixed benchmark function last
        test_benchmark_function(inference_engine)
        
        print("\n" + "="*50)
        print("üéâ ALL TESTS COMPLETED!")
        print("="*50)
        
    except Exception as e:
        print(f"\n‚ùå SETUP ERROR: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
