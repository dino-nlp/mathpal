# üèóÔ∏è Ki·∫øn tr√∫c Pipeline Training Gemma3N

## üìã T·ªïng quan

D·ª± √°n ƒë√£ ƒë∆∞·ª£c refactor t·ª´ m·ªôt Jupyter notebook th√†nh m·ªôt pipeline modular v·ªõi ki·∫øn tr√∫c r√µ r√†ng, d·ªÖ b·∫£o tr√¨ v√† m·ªü r·ªông. Pipeline ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ fine-tune m√¥ h√¨nh Gemma3N cho b√†i to√°n h·ªó tr·ª£ h·ªçc to√°n l·ªõp 6 b·∫±ng ti·∫øng Vi·ªát.

## üéØ M·ª•c ti√™u thi·∫øt k·∫ø

- **Modularity**: T√°ch bi·ªát c√°c ch·ª©c nƒÉng th√†nh modules ƒë·ªôc l·∫≠p
- **Reusability**: Code c√≥ th·ªÉ t√°i s·ª≠ d·ª•ng cho c√°c d·ª± √°n kh√°c
- **Maintainability**: D·ªÖ b·∫£o tr√¨ v√† debug
- **Extensibility**: D·ªÖ m·ªü r·ªông v·ªõi c√°c t√≠nh nƒÉng m·ªõi
- **Configurability**: Linh ho·∫°t trong c·∫•u h√¨nh
- **Testability**: D·ªÖ test t·ª´ng component

## üèõÔ∏è Ki·∫øn tr√∫c t·ªïng quan

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    CLI Interface                            ‚îÇ
‚îÇ                 (train_gemma.py)                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                Configuration Layer                         ‚îÇ
‚îÇ              (TrainingConfig, CometConfig)                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îò
      ‚îÇ                                                     ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ   Data     ‚îÇ  ‚îÇ     Models      ‚îÇ  ‚îÇ   Training      ‚îÇ   ‚îÇ
‚îÇ Processing ‚îÇ  ‚îÇ   Management    ‚îÇ  ‚îÇ    Engine       ‚îÇ   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
      ‚îÇ                   ‚îÇ                    ‚îÇ           ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  Dataset   ‚îÇ  ‚îÇ  Model Loader   ‚îÇ  ‚îÇ Trainer Factory ‚îÇ   ‚îÇ
‚îÇ Processor  ‚îÇ  ‚îÇ  LoRA Config    ‚îÇ  ‚îÇTraining Utils   ‚îÇ   ‚îÇ
‚îÇChat Format ‚îÇ  ‚îÇ  Model Saver    ‚îÇ  ‚îÇ                 ‚îÇ   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
                                                           ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚ñº‚îÄ‚îê
‚îÇ                  Supporting Services                    ‚îÇ   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§   ‚îÇ
‚îÇ   Experiments   ‚îÇ    Inference    ‚îÇ      Utilities      ‚îÇ   ‚îÇ
‚îÇ     Tracking    ‚îÇ     Engine      ‚îÇ    (Logging, etc)   ‚îÇ   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
                                                             ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îê
‚îÇ                    External Integrations                    ‚îÇ
‚îÇ         (Unsloth, TRL, PEFT, Comet ML, HF Hub)             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìÇ C·∫•u tr√∫c chi ti·∫øt

### 1. Configuration Layer (`config/`)
```python
config/
‚îú‚îÄ‚îÄ base_config.py      # Base configuration v·ªõi save/load
‚îú‚îÄ‚îÄ training_config.py  # C·∫•u h√¨nh training (model, dataset, hyperparams)
‚îî‚îÄ‚îÄ comet_config.py     # C·∫•u h√¨nh Comet ML tracking
```

**Ch·ª©c nƒÉng:**
- Qu·∫£n l√Ω t·∫•t c·∫£ c·∫•u h√¨nh c·ªßa pipeline
- H·ªó tr·ª£ load/save t·ª´ YAML, JSON
- Validation c·∫•u h√¨nh
- Type-safe configuration v·ªõi dataclasses

### 2. Data Processing (`data/`)
```python
data/
‚îú‚îÄ‚îÄ dataset_processor.py  # Load v√† process datasets
‚îî‚îÄ‚îÄ chat_formatter.py     # Format chat templates
```

**Ch·ª©c nƒÉng:**
- Load datasets t·ª´ HuggingFace Hub
- Chuy·ªÉn ƒë·ªïi ƒë·ªãnh d·∫°ng question/solution ‚Üí conversation
- Apply chat templates cho Gemma3N
- Preview v√† statistics datasets

### 3. Models Management (`models/`)
```python
models/
‚îú‚îÄ‚îÄ model_loader.py     # Load models v·ªõi Unsloth
‚îú‚îÄ‚îÄ lora_config.py      # LoRA configuration management
‚îî‚îÄ‚îÄ model_saver.py      # Save models ·ªü nhi·ªÅu format
```

**Ch·ª©c nƒÉng:**
- Load Gemma3N v·ªõi Unsloth optimizations
- Apply LoRA adapters v·ªõi configurations linh ho·∫°t
- Save models ·ªü nhi·ªÅu format (LoRA, merged, GGUF)
- Model memory management

### 4. Training Engine (`training/`)
```python
training/
‚îú‚îÄ‚îÄ trainer_factory.py  # T·∫°o SFTTrainer v·ªõi configs
‚îî‚îÄ‚îÄ training_utils.py   # Utilities cho training
```

**Ch·ª©c nƒÉng:**
- T·∫°o SFTTrainer v·ªõi SFTConfig
- Setup response-only training v·ªõi Unsloth
- Training monitoring v√† utilities
- Memory v√† device management

### 5. Experiment Tracking (`experiments/`)
```python
experiments/
‚îî‚îÄ‚îÄ comet_tracker.py    # Comet ML integration
```

**Ch·ª©c nƒÉng:**
- Setup Comet ML experiments
- Log metrics, parameters, models
- Handle errors gracefully
- Support multiple tracking platforms

### 6. Inference Engine (`inference/`)
```python
inference/
‚îî‚îÄ‚îÄ inference_engine.py  # Model inference v·ªõi optimizations
```

**Ch·ª©c nƒÉng:**
- Generate responses v·ªõi various configs
- Batch inference
- Streaming generation
- Benchmark inference performance

### 7. Utilities (`utils/`)
```python
utils/
‚îú‚îÄ‚îÄ logging.py         # Enhanced logging v·ªõi emojis
‚îî‚îÄ‚îÄ device_utils.py    # Device v√† memory management
```

**Ch·ª©c nƒÉng:**
- Rich logging v·ªõi colors v√† emojis
- CUDA memory monitoring
- Device detection v√† benchmarking
- Memory optimization utilities

### 8. CLI Interface (`cli/`)
```python
cli/
‚îî‚îÄ‚îÄ train_gemma.py     # Main command line interface
```

**Ch·ª©c nƒÉng:**
- Command line argument parsing
- Configuration override t·ª´ CLI
- Integration t·∫•t c·∫£ components
- Error handling v√† logging

## üîÑ Data Flow

```mermaid
graph TD
    A[CLI Args] --> B[Load Config]
    B --> C[Setup Environment]
    C --> D[Initialize Experiment Tracking]
    D --> E[Load Model + LoRA]
    E --> F[Process Dataset]
    F --> G[Create Trainer]
    G --> H[Train Model]
    H --> I[Save Model]
    I --> J[Test Inference]
    J --> K[Log Results]
    K --> L[Cleanup]
```

## üß© Key Components

### Configuration Management
```python
# Type-safe configuration v·ªõi validation
@dataclass
class TrainingConfig(BaseConfig):
    model_name: str = "unsloth/gemma-3n-E4B-it"
    max_seq_length: int = 2048
    # ... other fields
    
    def validate(self) -> None:
        # Validation logic
        
    def to_sft_config_kwargs(self) -> Dict[str, Any]:
        # Convert to SFTConfig format
```

### Model Loading v·ªõi Unsloth
```python
class ModelLoader:
    def load_complete_model(self) -> Tuple[Any, Any]:
        # Load v·ªõi Unsloth optimizations
        model, processor = FastModel.from_pretrained(...)
        model = FastModel.get_peft_model(model, **lora_config)
        return model, processor
```

### Flexible Training Setup
```python
class TrainerFactory:
    def create_trainer(self, model, tokenizer, dataset) -> SFTTrainer:
        # Setup SFTTrainer v·ªõi custom configs
        trainer = SFTTrainer(...)
        # Response-only training v·ªõi Unsloth
        trainer = train_on_responses_only(trainer, ...)
        return trainer
```

### Multi-format Model Saving
```python
class ModelSaver:
    def save_all_formats(self, formats: Dict[str, Dict]) -> Dict[str, str]:
        # Save LoRA adapters, merged models, GGUF, etc.
        # Support HuggingFace Hub upload
```

## üîß Extensibility Points

### 1. Custom Dataset Processors
```python
class CustomDatasetProcessor(DatasetProcessor):
    def custom_preprocessing(self, sample):
        # Implement custom logic
        return processed_sample
```

### 2. Custom LoRA Configurations
```python
# Easy LoRA config management
lora_config = LoRAConfigManager.create_lora_config(
    r=32, 
    target_modules=["custom_modules"],
    custom_param=value
)
```

### 3. Custom Training Strategies
```python
class CustomTrainerFactory(TrainerFactory):
    def create_custom_trainer(self, ...):
        # Implement custom training logic
        return trainer
```

### 4. Multiple Experiment Trackers
```python
# Easy to add new trackers
class WandBTracker(BaseTracker):
    def setup_experiment(self, config):
        # WandB setup logic
```

## üìä Monitoring v√† Debugging

### Rich Logging
```python
logger = TrainingLogger()
logger.info("Training started")
logger.metric("loss", 0.5, step=100)
logger.model_info("Gemma3N", 1000000, 50000)
```

### Memory Monitoring
```python
# Automatic memory tracking
result, memory_stats = DeviceUtils.monitor_memory_usage(trainer.train)
print(f"Peak memory: {memory_stats['peak_memory_gb']:.2f}GB")
```

### Experiment Tracking
```python
# Automatic metric logging
comet_tracker.setup_experiment(config)
# Training t·ª± ƒë·ªông log metrics
trainer.train()
comet_tracker.log_model(model_path)
```

## üöÄ Usage Patterns

### 1. Quick Development
```bash
make train-dev  # Development config
python scripts/quick_test.py  # Quick test
```

### 2. Production Training
```bash
make train-prod  # Production config v·ªõi full tracking
```

### 3. Custom Experiments
```python
# Programmatic usage
config = TrainingConfig.from_yaml("my_config.yaml")
config.max_steps = 1000
# ... rest of pipeline
```

### 4. Batch Experiments
```python
# Easy to run multiple experiments
for lr in [1e-4, 2e-4, 5e-4]:
    config.learning_rate = lr
    config.experiment_name = f"lr_{lr}"
    # Run training pipeline
```

## üîÆ Future Extensions

### 1. Multiple Model Support
- D·ªÖ d√†ng support Llama, Mistral, etc.
- Abstract model loading interface

### 2. Advanced Training Methods
- DPO, PPO training
- Multi-GPU support
- Distributed training

### 3. Production Deployment
- Model serving v·ªõi FastAPI
- Docker containerization
- Cloud deployment scripts

### 4. Evaluation Framework
- Automatic evaluation metrics
- A/B testing framework
- Performance monitoring

## üìà Benefits c·ªßa Ki·∫øn tr√∫c

### 1. Maintainability
- **Separation of Concerns**: M·ªói module c√≥ tr√°ch nhi·ªám ri√™ng bi·ªát
- **Clear Interfaces**: APIs r√µ r√†ng gi·ªØa c√°c components
- **Error Isolation**: L·ªói trong m·ªôt module kh√¥ng ·∫£nh h∆∞·ªüng to√†n b·ªô

### 2. Testability
- **Unit Testing**: Test t·ª´ng module ƒë·ªôc l·∫≠p
- **Integration Testing**: Test workflows end-to-end
- **Mock Dependencies**: D·ªÖ mock external services

### 3. Reusability
- **Config Templates**: T√°i s·ª≠ d·ª•ng configs cho experiments kh√°c
- **Model Components**: Reuse model loading, saving logic
- **Training Patterns**: Apply cho models kh√°c

### 4. Scalability
- **Experiment Management**: D·ªÖ scale s·ªë l∆∞·ª£ng experiments
- **Resource Management**: Optimal memory v√† compute usage
- **Batch Processing**: Support training multiple models

## üéì Lessons Learned

### 1. Configuration is Key
- Type-safe configs prevent runtime errors
- YAML/JSON configs enable easy experimentation
- Config validation catches issues early

### 2. Logging and Monitoring
- Rich logging significantly improves debugging
- Memory monitoring prevents OOM crashes
- Experiment tracking enables reproducibility

### 3. Error Handling
- Graceful degradation when optional services fail
- Clear error messages guide users
- Automatic cleanup prevents resource leaks

### 4. Documentation
- Code documentation improves maintainability
- Usage examples reduce learning curve
- Architecture docs help new contributors

## üîö K·∫øt lu·∫≠n

Ki·∫øn tr√∫c modular n√†y cung c·∫•p:
- **Flexibility**: D·ªÖ experiment v·ªõi c√°c configs kh√°c nhau
- **Reliability**: Robust error handling v√† monitoring
- **Performance**: Optimized v·ªõi Unsloth v√† best practices
- **Usability**: Simple CLI v√† clear documentation

Pipeline n√†y kh√¥ng ch·ªâ ho·∫°t ƒë·ªông t·ªët cho Gemma3N m√† c√≤n c√≥ th·ªÉ d·ªÖ d√†ng adapt cho c√°c models v√† tasks kh√°c.