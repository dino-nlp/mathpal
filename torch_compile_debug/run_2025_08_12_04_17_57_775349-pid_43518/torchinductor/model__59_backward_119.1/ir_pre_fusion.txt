op0: SchedulerNode(ComputedBuffer)
op0.writes = [MemoryDep('buf0', c0, {c0: s3 - 8*(((s3 + 8)//9))})]
op0.unmet_dependencies = []
op0.met_dependencies = 
    [   MemoryDep('convert_element_type_45', 0, {}),
        MemoryDep('ne_55', c0, {c0: s3 - 8*(((s3 + 8)//9))}),
        MemoryDep('tangents_1', 0, {}),
        MemoryDep('where_18', c0, {c0: s3 - 8*(((s3 + 8)//9))})]
op0.outputs = [
    buf0: ComputedBuffer
    buf0.layout = FixedLayout('cuda:0', torch.float32, size=[s3 - 8*(((s3 + 8)//9)), 1], stride=[1, s3 - 8*(((s3 + 8)//9))])
    buf0.users = [NodeUser(node=SchedulerNode(name='op1'), can_inplace=False, is_weak=False)]
]
op0.group.device = cuda:0
op0.group.iteration = (s3 - 8*(((s3 + 8)//9)), 262400)
op0.sizes = ([s3 - 8*(((s3 + 8)//9))], [262400])
where_18_layout = FixedLayout('cuda:0', torch.int64, size=[s0 - 8*(((s0 + 8)//9)), 1], stride=[1, 1])
ne_55_layout = FixedLayout('cuda:0', torch.bool, size=[s0 - 8*(((s0 + 8)//9)), 1], stride=[1, 1])
tangents_1_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
convert_element_type_45_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
buf0_layout = FixedLayout('cuda:0', torch.float32, size=[s3 - 8*(((s3 + 8)//9)), 1], stride=[1, s3 - 8*(((s3 + 8)//9))])
class op0_loop_body:
    var_ranges = {p0: s3 - 8*(((s3 + 8)//9)), p1: 262400}
    index0 = p0
    index1 = p1
    index2 = 0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('where_18', get_index)
        get_index_1 = self.get_index('index1')
        index_expr = ops.index_expr(get_index_1, torch.int64)
        eq = ops.eq(load, index_expr)
        constant = ops.constant(-1.0, torch.float32)
        constant_1 = ops.constant(0.0, torch.float32)
        where = ops.where(eq, constant, constant_1)
        get_index_2 = self.get_index('index0')
        load_1 = ops.load('ne_55', get_index_2)
        get_index_3 = self.get_index('index2')
        load_2 = ops.load('tangents_1', get_index_3)
        get_index_4 = self.get_index('index2')
        load_3 = ops.load('convert_element_type_45', get_index_4)
        truediv = ops.truediv(load_2, load_3)
        constant_2 = ops.constant(0.0, torch.float32)
        where_1 = ops.where(load_1, truediv, constant_2)
        mul = ops.mul(where, where_1)
        reduction = ops.reduction(torch.float32, torch.float32, 'sum', mul)
        get_index_5 = self.get_index('index0')
        store_reduction = ops.store_reduction('buf0', get_index_5, reduction)
        return store_reduction


op1: SchedulerNode(ComputedBuffer)
op1.writes = [MemoryDep('buf1', c0, {c0: 262400*s3 - 2099200*(((s3 + 8)//9))})]
op1.unmet_dependencies = [MemoryDep('buf0', c0, {c0: s3 - 8*(((s3 + 8)//9))})]
op1.met_dependencies = 
    [   MemoryDep('amax_default', c0, {c0: s3 - 8*(((s3 + 8)//9))}),
        MemoryDep('convert_element_type_45', 0, {}),
        MemoryDep('log_8', c0, {c0: s3 - 8*(((s3 + 8)//9))}),
        MemoryDep('mm_8', c0, {c0: 262400*s3 - 2099200*(((s3 + 8)//9))}),
        MemoryDep('ne_55', c0, {c0: s3 - 8*(((s3 + 8)//9))}),
        MemoryDep('tangents_1', 0, {}),
        MemoryDep('where_18', c0, {c0: s3 - 8*(((s3 + 8)//9))})]
op1.outputs = [
    buf1: ComputedBuffer
    buf1.layout = FixedLayout('cuda:0', torch.bfloat16, size=[s3 - 8*(((s3 + 8)//9)), 262400], stride=[262400, 1])
    buf1.users = [NodeUser(node=SchedulerNode(name='op2'), can_inplace=True, is_weak=False)]
]
op1.group.device = cuda:0
op1.group.iteration = (262400*s3 - 2099200*(((s3 + 8)//9)), 1)
op1.sizes = ([s3 - 8*(((s3 + 8)//9)), 262400], [])
where_18_layout = FixedLayout('cuda:0', torch.int64, size=[s0 - 8*(((s0 + 8)//9)), 1], stride=[1, 1])
ne_55_layout = FixedLayout('cuda:0', torch.bool, size=[s0 - 8*(((s0 + 8)//9)), 1], stride=[1, 1])
tangents_1_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
convert_element_type_45_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
mm_8_layout = FixedLayout('cuda:0', torch.bfloat16, size=[s3 - 8*(((s3 + 8)//9)), 262400], stride=[262400, 1])
amax_default_layout = FixedLayout('cuda:0', torch.float32, size=[s3 - 8*(((s3 + 8)//9)), 1], stride=[1, 1])
log_8_layout = FixedLayout('cuda:0', torch.float32, size=[s3 - 8*(((s3 + 8)//9)), 1], stride=[1, 1])
buf0_layout = FixedLayout('cuda:0', torch.float32, size=[s3 - 8*(((s3 + 8)//9)), 1], stride=[1, s3 - 8*(((s3 + 8)//9))])
buf1_layout = FixedLayout('cuda:0', torch.bfloat16, size=[s3 - 8*(((s3 + 8)//9)), 262400], stride=[262400, 1])
class op1_loop_body:
    var_ranges = {p0: s3 - 8*(((s3 + 8)//9)), p1: 262400}
    index0 = p0
    index1 = p1
    index2 = 0
    index3 = 262400*p0 + p1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('where_18', get_index)
        get_index_1 = self.get_index('index1')
        index_expr = ops.index_expr(get_index_1, torch.int64)
        eq = ops.eq(load, index_expr)
        constant = ops.constant(-1.0, torch.float32)
        constant_1 = ops.constant(0.0, torch.float32)
        where = ops.where(eq, constant, constant_1)
        get_index_2 = self.get_index('index0')
        load_1 = ops.load('ne_55', get_index_2)
        get_index_3 = self.get_index('index2')
        load_2 = ops.load('tangents_1', get_index_3)
        get_index_4 = self.get_index('index2')
        load_3 = ops.load('convert_element_type_45', get_index_4)
        truediv = ops.truediv(load_2, load_3)
        constant_2 = ops.constant(0.0, torch.float32)
        where_1 = ops.where(load_1, truediv, constant_2)
        mul = ops.mul(where, where_1)
        get_index_5 = self.get_index('index3')
        load_4 = ops.load('mm_8', get_index_5)
        constant_3 = ops.constant(0.03333333333333333, torch.bfloat16)
        mul_1 = ops.mul(load_4, constant_3)
        tanh = ops.tanh(mul_1)
        to_dtype = ops.to_dtype(tanh, torch.float32, src_dtype = torch.bfloat16)
        constant_4 = ops.constant(1.0, torch.float32)
        mul_2 = ops.mul(to_dtype, constant_4)
        get_index_6 = self.get_index('index0')
        load_5 = ops.load('amax_default', get_index_6)
        sub = ops.sub(mul_2, load_5)
        constant_5 = ops.constant(30.0, torch.float32)
        mul_3 = ops.mul(sub, constant_5)
        get_index_7 = self.get_index('index0')
        load_6 = ops.load('log_8', get_index_7)
        sub_1 = ops.sub(mul_3, load_6)
        exp = ops.exp(sub_1)
        get_index_8 = self.get_index('index0')
        load_7 = ops.load('buf0', get_index_8)
        mul_4 = ops.mul(exp, load_7)
        sub_2 = ops.sub(mul, mul_4)
        to_dtype_1 = ops.to_dtype(sub_2, torch.bfloat16, src_dtype = torch.float32)
        get_index_9 = self.get_index('index3')
        store = ops.store('buf1', get_index_9, to_dtype_1, None)
        return store


op2: SchedulerNode(ComputedBuffer)
op2.writes = [MemoryDep('buf2', c0, {c0: 262400*s3 - 2099200*(((s3 + 8)//9))})]
op2.unmet_dependencies = [MemoryDep('buf1', c0, {c0: 262400*s3 - 2099200*(((s3 + 8)//9))})]
op2.met_dependencies = [MemoryDep('mm_8', c0, {c0: 262400*s3 - 2099200*(((s3 + 8)//9))})]
op2.outputs = [
    buf2: ComputedBuffer
    buf2.layout = FixedLayout('cuda:0', torch.bfloat16, size=[s3 - 8*(((s3 + 8)//9)), 262400], stride=[262400, 1])
    buf2.users = [NodeUser(node=ExternKernelSchedulerNode(name='op3'), can_inplace=False, is_weak=False)]
]
op2.group.device = cuda:0
op2.group.iteration = (262400*s3 - 2099200*(((s3 + 8)//9)), 1)
op2.sizes = ([262400*s3 - 2099200*(((s3 + 8)//9))], [])
buf1_layout = FixedLayout('cuda:0', torch.bfloat16, size=[s3 - 8*(((s3 + 8)//9)), 262400], stride=[262400, 1])
mm_8_layout = FixedLayout('cuda:0', torch.bfloat16, size=[s3 - 8*(((s3 + 8)//9)), 262400], stride=[262400, 1])
buf2_layout = FixedLayout('cuda:0', torch.bfloat16, size=[s3 - 8*(((s3 + 8)//9)), 262400], stride=[262400, 1])
class op2_loop_body:
    var_ranges = {p0: 262400*s3 - 2099200*(((s3 + 8)//9))}
    index0 = p0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf1', get_index)
        constant = ops.constant(30.0, torch.bfloat16)
        mul = ops.mul(load, constant)
        to_dtype = ops.to_dtype(mul, torch.float32, src_dtype = torch.bfloat16)
        get_index_1 = self.get_index('index0')
        load_1 = ops.load('mm_8', get_index_1)
        constant_1 = ops.constant(0.03333333333333333, torch.bfloat16)
        mul_1 = ops.mul(load_1, constant_1)
        tanh = ops.tanh(mul_1)
        to_dtype_1 = ops.to_dtype(tanh, torch.float32, src_dtype = torch.bfloat16)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('mm_8', get_index_2)
        constant_2 = ops.constant(0.03333333333333333, torch.bfloat16)
        mul_2 = ops.mul(load_2, constant_2)
        tanh_1 = ops.tanh(mul_2)
        to_dtype_2 = ops.to_dtype(tanh_1, torch.float32, src_dtype = torch.bfloat16)
        mul_3 = ops.mul(to_dtype_1, to_dtype_2)
        constant_3 = ops.constant(1.0, torch.float32)
        sub = ops.sub(constant_3, mul_3)
        mul_4 = ops.mul(to_dtype, sub)
        to_dtype_3 = ops.to_dtype(mul_4, torch.bfloat16, src_dtype = torch.float32)
        constant_4 = ops.constant(0.03333333333333333, torch.bfloat16)
        mul_5 = ops.mul(to_dtype_3, constant_4)
        get_index_3 = self.get_index('index0')
        store = ops.store('buf2', get_index_3, mul_5, None)
        return store


op3: ExternKernelSchedulerNode(ExternKernelOut)
op3.writes = [StarDep(name='buf3', mode=None)]
op3.unmet_dependencies = [StarDep(name='buf2', mode=None)]
op3.met_dependencies = [StarDep(name='permute_10', mode=None)]
op3.outputs = [
    buf3: ExternKernelOut
    buf3.layout = FixedLayout('cuda:0', torch.bfloat16, size=[s3 - 8*(((s3 + 8)//9)), 2048], stride=[2048, 1])
    buf3.users = [NodeUser(node=SchedulerNode(name='op68'), can_inplace=False, is_weak=False)]
]
op3.node.kernel = extern_kernels.mm


op4: SchedulerNode(ComputedBuffer)
op4.writes = [MemoryDep('buf4', c0, {c0: 262400*(((s3 + 8)//9))})]
op4.unmet_dependencies = []
op4.met_dependencies = []
op4.outputs = [
    buf4: ComputedBuffer
    buf4.layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
    buf4.users = [NodeUser(node=SchedulerNode(name='op5'), can_inplace=False, is_weak=False)]
]
op4.group.device = cuda:0
op4.group.iteration = (262400*(((s3 + 8)//9)), 1)
op4.sizes = ([262400*(((s3 + 8)//9))], [])
buf4_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
class op4_loop_body:
    var_ranges = {p0: 262400*(((s3 + 8)//9))}
    index0 = p0
    def body(self, ops):
        constant = ops.constant(0.0, torch.float32)
        get_index = self.get_index('index0')
        store = ops.store('buf4', get_index, constant, None)
        return store


op5: SchedulerNode(ComputedBuffer)
op5.writes = [MemoryDep('buf5', 262400*c0 + tmp0, {c0: ((s0 + 8)//9)})]
op5.unmet_dependencies = [StarDep(name='buf4', mode=None)]
op5.met_dependencies = [MemoryDep('where_20', c0, {c0: ((s0 + 8)//9)})]
op5.outputs = [
    buf5: ComputedBuffer
    buf5.layout = MutationLayoutSHOULDREMOVE('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
    buf5.mutations = ['buf4']
    buf5.users = [
        NodeUser(node=SchedulerNode(name='op6'), can_inplace=False, is_weak=False),
        NodeUser(node=SchedulerNode(name='op8'), can_inplace=True, is_weak=False),
    ]
]
op5.group.device = cuda:0
op5.group.iteration = (((s0 + 8)//9), 1)
op5.sizes = ([((s0 + 8)//9)], [])
where_20_layout = FixedLayout('cuda:0', torch.int64, size=[((s0 + 8)//9), 1], stride=[1, 1])
buf4_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
buf5_layout = MutationLayoutSHOULDREMOVE('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
class op5_loop_body:
    var_ranges = {p0: ((s0 + 8)//9)}
    index0 = p0
    index1 = indirect0 + 262400*p0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('where_20', get_index)
        set_indirect0 = self.set_indirect0(load)
        constant = ops.constant(-1.0, torch.float32)
        get_index_1 = self.get_index('index1')
        store = ops.store('buf5', get_index_1, constant, None)
        return store


op6: SchedulerNode(ComputedBuffer)
op6.writes = [MemoryDep('buf6', c0, {c0: 5*(((s3 + 8)//9))})]
op6.unmet_dependencies = [MemoryDep('buf5', c0, {c0: 262400*(((s3 + 8)//9))})]
op6.met_dependencies = 
    [   MemoryDep('convert_element_type_45', 0, {}),
        MemoryDep('ne_57', c0, {c0: ((s3 + 8)//9)}),
        MemoryDep('tangents_1', 0, {})]
op6.outputs = [
    buf6: ComputedBuffer
    buf6.layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1, 5], stride=[5, 5*(((s3 + 8)//9)), 1])
    buf6.users = [NodeUser(node=SchedulerNode(name='op7'), can_inplace=False, is_weak=False)]
]
op6.group.device = cuda:0
op6.group.iteration = (5*(((s3 + 8)//9)), 52480)
op6.sizes = ([((s3 + 8)//9), 5], [52480])
buf5_layout = MutationLayoutSHOULDREMOVE('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
ne_57_layout = FixedLayout('cuda:0', torch.bool, size=[((s0 + 8)//9), 1], stride=[1, 1])
tangents_1_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
convert_element_type_45_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
buf6_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1, 5], stride=[5, 5*(((s3 + 8)//9)), 1])
class op6_loop_body:
    var_ranges = {p0: ((s3 + 8)//9), p1: 5, p2: 52480}
    index0 = 262400*p0 + 52480*p1 + p2
    index1 = p0
    index2 = 0
    index3 = 5*p0 + p1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf4', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('ne_57', get_index_1)
        get_index_2 = self.get_index('index2')
        load_2 = ops.load('tangents_1', get_index_2)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('convert_element_type_45', get_index_3)
        truediv = ops.truediv(load_2, load_3)
        constant = ops.constant(0.0, torch.float32)
        where = ops.where(load_1, truediv, constant)
        mul = ops.mul(load, where)
        reduction = ops.reduction(torch.float32, torch.float32, 'sum', mul)
        get_index_4 = self.get_index('index3')
        store_reduction = ops.store_reduction('buf6', get_index_4, reduction)
        return store_reduction


op7: SchedulerNode(ComputedBuffer)
op7.writes = [MemoryDep('buf7', c0, {c0: ((s3 + 8)//9)})]
op7.unmet_dependencies = [MemoryDep('buf6', c0, {c0: 5*(((s3 + 8)//9))})]
op7.met_dependencies = []
op7.outputs = [
    buf7: ComputedBuffer
    buf7.layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, ((s3 + 8)//9)])
    buf7.users = [NodeUser(node=SchedulerNode(name='op8'), can_inplace=False, is_weak=False)]
]
op7.group.device = cuda:0
op7.group.iteration = (((s3 + 8)//9), 5)
op7.sizes = ([((s3 + 8)//9)], [5])
buf6_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1, 5], stride=[5, 5*(((s3 + 8)//9)), 1])
buf7_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, ((s3 + 8)//9)])
class op7_loop_body:
    var_ranges = {p0: ((s3 + 8)//9), p1: 5}
    index0 = 5*p0 + p1
    index1 = p0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf6', get_index)
        reduction = ops.reduction(torch.float32, torch.float32, 'sum', load)
        get_index_1 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf7', get_index_1, reduction)
        return store_reduction


op8: SchedulerNode(ComputedBuffer)
op8.writes = [MemoryDep('buf8', c0, {c0: 262400*(((s3 + 8)//9))})]
op8.unmet_dependencies = 
    [   MemoryDep('buf5', c0, {c0: 262400*(((s3 + 8)//9))}),
        MemoryDep('buf7', c0, {c0: ((s3 + 8)//9)})]
op8.met_dependencies = 
    [   MemoryDep('amax_default_1', c0, {c0: ((s3 + 8)//9)}),
        MemoryDep('convert_element_type_45', 0, {}),
        MemoryDep('log_7', c0, {c0: ((s3 + 8)//9)}),
        MemoryDep('mm_7', c0, {c0: 262400*(((s3 + 8)//9))}),
        MemoryDep('ne_57', c0, {c0: ((s3 + 8)//9)}),
        MemoryDep('tangents_1', 0, {})]
op8.outputs = [
    buf8: ComputedBuffer
    buf8.layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
    buf8.users = [NodeUser(node=SchedulerNode(name='op9'), can_inplace=True, is_weak=False)]
]
op8.group.device = cuda:0
op8.group.iteration = (262400*(((s3 + 8)//9)), 1)
op8.sizes = ([((s3 + 8)//9), 262400], [])
buf5_layout = MutationLayoutSHOULDREMOVE('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
ne_57_layout = FixedLayout('cuda:0', torch.bool, size=[((s0 + 8)//9), 1], stride=[1, 1])
tangents_1_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
convert_element_type_45_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
mm_7_layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
amax_default_1_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, 1])
log_7_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, 1])
buf7_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, ((s3 + 8)//9)])
buf8_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
class op8_loop_body:
    var_ranges = {p0: ((s3 + 8)//9), p1: 262400}
    index0 = 262400*p0 + p1
    index1 = p0
    index2 = 0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf4', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('ne_57', get_index_1)
        get_index_2 = self.get_index('index2')
        load_2 = ops.load('tangents_1', get_index_2)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('convert_element_type_45', get_index_3)
        truediv = ops.truediv(load_2, load_3)
        constant = ops.constant(0.0, torch.float32)
        where = ops.where(load_1, truediv, constant)
        mul = ops.mul(load, where)
        get_index_4 = self.get_index('index0')
        load_4 = ops.load('mm_7', get_index_4)
        constant_1 = ops.constant(0.03333333333333333, torch.bfloat16)
        mul_1 = ops.mul(load_4, constant_1)
        tanh = ops.tanh(mul_1)
        to_dtype = ops.to_dtype(tanh, torch.float32, src_dtype = torch.bfloat16)
        constant_2 = ops.constant(1.0, torch.float32)
        mul_2 = ops.mul(to_dtype, constant_2)
        get_index_5 = self.get_index('index1')
        load_5 = ops.load('amax_default_1', get_index_5)
        sub = ops.sub(mul_2, load_5)
        constant_3 = ops.constant(30.0, torch.float32)
        mul_3 = ops.mul(sub, constant_3)
        get_index_6 = self.get_index('index1')
        load_6 = ops.load('log_7', get_index_6)
        sub_1 = ops.sub(mul_3, load_6)
        exp = ops.exp(sub_1)
        get_index_7 = self.get_index('index1')
        load_7 = ops.load('buf7', get_index_7)
        mul_4 = ops.mul(exp, load_7)
        sub_2 = ops.sub(mul, mul_4)
        to_dtype_1 = ops.to_dtype(sub_2, torch.bfloat16, src_dtype = torch.float32)
        constant_4 = ops.constant(30.0, torch.bfloat16)
        mul_5 = ops.mul(to_dtype_1, constant_4)
        to_dtype_2 = ops.to_dtype(mul_5, torch.float32, src_dtype = torch.bfloat16)
        get_index_8 = self.get_index('index0')
        load_8 = ops.load('mm_7', get_index_8)
        constant_5 = ops.constant(0.03333333333333333, torch.bfloat16)
        mul_6 = ops.mul(load_8, constant_5)
        tanh_1 = ops.tanh(mul_6)
        to_dtype_3 = ops.to_dtype(tanh_1, torch.float32, src_dtype = torch.bfloat16)
        get_index_9 = self.get_index('index0')
        load_9 = ops.load('mm_7', get_index_9)
        constant_6 = ops.constant(0.03333333333333333, torch.bfloat16)
        mul_7 = ops.mul(load_9, constant_6)
        tanh_2 = ops.tanh(mul_7)
        to_dtype_4 = ops.to_dtype(tanh_2, torch.float32, src_dtype = torch.bfloat16)
        mul_8 = ops.mul(to_dtype_3, to_dtype_4)
        constant_7 = ops.constant(1.0, torch.float32)
        sub_3 = ops.sub(constant_7, mul_8)
        mul_9 = ops.mul(to_dtype_2, sub_3)
        get_index_10 = self.get_index('index0')
        store = ops.store('buf8', get_index_10, mul_9, None)
        return store


op9: SchedulerNode(ComputedBuffer)
op9.writes = [MemoryDep('buf9', c0, {c0: 262400*(((s3 + 8)//9))})]
op9.unmet_dependencies = [MemoryDep('buf8', c0, {c0: 262400*(((s3 + 8)//9))})]
op9.met_dependencies = []
op9.outputs = [
    buf9: ComputedBuffer
    buf9.layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
    buf9.users = [NodeUser(node=ExternKernelSchedulerNode(name='op10'), can_inplace=False, is_weak=False)]
]
op9.group.device = cuda:0
op9.group.iteration = (262400*(((s3 + 8)//9)), 1)
op9.sizes = ([262400*(((s3 + 8)//9))], [])
buf8_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
buf9_layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
class op9_loop_body:
    var_ranges = {p0: 262400*(((s3 + 8)//9))}
    index0 = p0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf8', get_index)
        to_dtype = ops.to_dtype(load, torch.bfloat16, src_dtype = torch.float32)
        constant = ops.constant(0.03333333333333333, torch.bfloat16)
        mul = ops.mul(to_dtype, constant)
        get_index_1 = self.get_index('index0')
        store = ops.store('buf9', get_index_1, mul, None)
        return store


op10: ExternKernelSchedulerNode(ExternKernelOut)
op10.writes = [StarDep(name='buf10', mode=None)]
op10.unmet_dependencies = [StarDep(name='buf9', mode=None)]
op10.met_dependencies = [StarDep(name='permute_10', mode=None)]
op10.outputs = [
    buf10: ExternKernelOut
    buf10.layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 2048], stride=[2048, 1])
    buf10.users = [NodeUser(node=SchedulerNode(name='op67'), can_inplace=False, is_weak=False)]
]
op10.node.kernel = extern_kernels.mm


op11: SchedulerNode(ComputedBuffer)
op11.writes = [MemoryDep('buf11', c0, {c0: 262400*(((s3 + 8)//9))})]
op11.unmet_dependencies = []
op11.met_dependencies = []
op11.outputs = [
    buf11: ComputedBuffer
    buf11.layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
    buf11.users = [NodeUser(node=SchedulerNode(name='op12'), can_inplace=False, is_weak=False)]
]
op11.group.device = cuda:0
op11.group.iteration = (262400*(((s3 + 8)//9)), 1)
op11.sizes = ([262400*(((s3 + 8)//9))], [])
buf11_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
class op11_loop_body:
    var_ranges = {p0: 262400*(((s3 + 8)//9))}
    index0 = p0
    def body(self, ops):
        constant = ops.constant(0.0, torch.float32)
        get_index = self.get_index('index0')
        store = ops.store('buf11', get_index, constant, None)
        return store


op12: SchedulerNode(ComputedBuffer)
op12.writes = [MemoryDep('buf12', 262400*c0 + tmp0, {c0: ((s0 + 8)//9)})]
op12.unmet_dependencies = [StarDep(name='buf11', mode=None)]
op12.met_dependencies = [MemoryDep('where_22', c0, {c0: ((s0 + 8)//9)})]
op12.outputs = [
    buf12: ComputedBuffer
    buf12.layout = MutationLayoutSHOULDREMOVE('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
    buf12.mutations = ['buf11']
    buf12.users = [
        NodeUser(node=SchedulerNode(name='op13'), can_inplace=False, is_weak=False),
        NodeUser(node=SchedulerNode(name='op15'), can_inplace=True, is_weak=False),
    ]
]
op12.group.device = cuda:0
op12.group.iteration = (((s0 + 8)//9), 1)
op12.sizes = ([((s0 + 8)//9)], [])
where_22_layout = FixedLayout('cuda:0', torch.int64, size=[((s0 + 8)//9), 1], stride=[1, 1])
buf11_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
buf12_layout = MutationLayoutSHOULDREMOVE('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
class op12_loop_body:
    var_ranges = {p0: ((s0 + 8)//9)}
    index0 = p0
    index1 = indirect0 + 262400*p0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('where_22', get_index)
        set_indirect0 = self.set_indirect0(load)
        constant = ops.constant(-1.0, torch.float32)
        get_index_1 = self.get_index('index1')
        store = ops.store('buf12', get_index_1, constant, None)
        return store


op13: SchedulerNode(ComputedBuffer)
op13.writes = [MemoryDep('buf13', c0, {c0: 5*(((s3 + 8)//9))})]
op13.unmet_dependencies = [MemoryDep('buf12', c0, {c0: 262400*(((s3 + 8)//9))})]
op13.met_dependencies = 
    [   MemoryDep('convert_element_type_45', 0, {}),
        MemoryDep('ne_59', c0, {c0: ((s3 + 8)//9)}),
        MemoryDep('tangents_1', 0, {})]
op13.outputs = [
    buf13: ComputedBuffer
    buf13.layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1, 5], stride=[5, 5*(((s3 + 8)//9)), 1])
    buf13.users = [NodeUser(node=SchedulerNode(name='op14'), can_inplace=False, is_weak=False)]
]
op13.group.device = cuda:0
op13.group.iteration = (5*(((s3 + 8)//9)), 52480)
op13.sizes = ([((s3 + 8)//9), 5], [52480])
buf12_layout = MutationLayoutSHOULDREMOVE('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
ne_59_layout = FixedLayout('cuda:0', torch.bool, size=[((s0 + 8)//9), 1], stride=[1, 1])
tangents_1_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
convert_element_type_45_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
buf13_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1, 5], stride=[5, 5*(((s3 + 8)//9)), 1])
class op13_loop_body:
    var_ranges = {p0: ((s3 + 8)//9), p1: 5, p2: 52480}
    index0 = 262400*p0 + 52480*p1 + p2
    index1 = p0
    index2 = 0
    index3 = 5*p0 + p1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf11', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('ne_59', get_index_1)
        get_index_2 = self.get_index('index2')
        load_2 = ops.load('tangents_1', get_index_2)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('convert_element_type_45', get_index_3)
        truediv = ops.truediv(load_2, load_3)
        constant = ops.constant(0.0, torch.float32)
        where = ops.where(load_1, truediv, constant)
        mul = ops.mul(load, where)
        reduction = ops.reduction(torch.float32, torch.float32, 'sum', mul)
        get_index_4 = self.get_index('index3')
        store_reduction = ops.store_reduction('buf13', get_index_4, reduction)
        return store_reduction


op14: SchedulerNode(ComputedBuffer)
op14.writes = [MemoryDep('buf14', c0, {c0: ((s3 + 8)//9)})]
op14.unmet_dependencies = [MemoryDep('buf13', c0, {c0: 5*(((s3 + 8)//9))})]
op14.met_dependencies = []
op14.outputs = [
    buf14: ComputedBuffer
    buf14.layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, ((s3 + 8)//9)])
    buf14.users = [NodeUser(node=SchedulerNode(name='op15'), can_inplace=False, is_weak=False)]
]
op14.group.device = cuda:0
op14.group.iteration = (((s3 + 8)//9), 5)
op14.sizes = ([((s3 + 8)//9)], [5])
buf13_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1, 5], stride=[5, 5*(((s3 + 8)//9)), 1])
buf14_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, ((s3 + 8)//9)])
class op14_loop_body:
    var_ranges = {p0: ((s3 + 8)//9), p1: 5}
    index0 = 5*p0 + p1
    index1 = p0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf13', get_index)
        reduction = ops.reduction(torch.float32, torch.float32, 'sum', load)
        get_index_1 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf14', get_index_1, reduction)
        return store_reduction


op15: SchedulerNode(ComputedBuffer)
op15.writes = [MemoryDep('buf15', c0, {c0: 262400*(((s3 + 8)//9))})]
op15.unmet_dependencies = 
    [   MemoryDep('buf12', c0, {c0: 262400*(((s3 + 8)//9))}),
        MemoryDep('buf14', c0, {c0: ((s3 + 8)//9)})]
op15.met_dependencies = 
    [   MemoryDep('amax_default_2', c0, {c0: ((s3 + 8)//9)}),
        MemoryDep('convert_element_type_45', 0, {}),
        MemoryDep('log_6', c0, {c0: ((s3 + 8)//9)}),
        MemoryDep('mm_6', c0, {c0: 262400*(((s3 + 8)//9))}),
        MemoryDep('ne_59', c0, {c0: ((s3 + 8)//9)}),
        MemoryDep('tangents_1', 0, {})]
op15.outputs = [
    buf15: ComputedBuffer
    buf15.layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
    buf15.users = [NodeUser(node=SchedulerNode(name='op16'), can_inplace=True, is_weak=False)]
]
op15.group.device = cuda:0
op15.group.iteration = (262400*(((s3 + 8)//9)), 1)
op15.sizes = ([((s3 + 8)//9), 262400], [])
buf12_layout = MutationLayoutSHOULDREMOVE('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
ne_59_layout = FixedLayout('cuda:0', torch.bool, size=[((s0 + 8)//9), 1], stride=[1, 1])
tangents_1_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
convert_element_type_45_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
mm_6_layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
amax_default_2_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, 1])
log_6_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, 1])
buf14_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, ((s3 + 8)//9)])
buf15_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
class op15_loop_body:
    var_ranges = {p0: ((s3 + 8)//9), p1: 262400}
    index0 = 262400*p0 + p1
    index1 = p0
    index2 = 0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf11', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('ne_59', get_index_1)
        get_index_2 = self.get_index('index2')
        load_2 = ops.load('tangents_1', get_index_2)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('convert_element_type_45', get_index_3)
        truediv = ops.truediv(load_2, load_3)
        constant = ops.constant(0.0, torch.float32)
        where = ops.where(load_1, truediv, constant)
        mul = ops.mul(load, where)
        get_index_4 = self.get_index('index0')
        load_4 = ops.load('mm_6', get_index_4)
        constant_1 = ops.constant(0.03333333333333333, torch.bfloat16)
        mul_1 = ops.mul(load_4, constant_1)
        tanh = ops.tanh(mul_1)
        to_dtype = ops.to_dtype(tanh, torch.float32, src_dtype = torch.bfloat16)
        constant_2 = ops.constant(1.0, torch.float32)
        mul_2 = ops.mul(to_dtype, constant_2)
        get_index_5 = self.get_index('index1')
        load_5 = ops.load('amax_default_2', get_index_5)
        sub = ops.sub(mul_2, load_5)
        constant_3 = ops.constant(30.0, torch.float32)
        mul_3 = ops.mul(sub, constant_3)
        get_index_6 = self.get_index('index1')
        load_6 = ops.load('log_6', get_index_6)
        sub_1 = ops.sub(mul_3, load_6)
        exp = ops.exp(sub_1)
        get_index_7 = self.get_index('index1')
        load_7 = ops.load('buf14', get_index_7)
        mul_4 = ops.mul(exp, load_7)
        sub_2 = ops.sub(mul, mul_4)
        to_dtype_1 = ops.to_dtype(sub_2, torch.bfloat16, src_dtype = torch.float32)
        constant_4 = ops.constant(30.0, torch.bfloat16)
        mul_5 = ops.mul(to_dtype_1, constant_4)
        to_dtype_2 = ops.to_dtype(mul_5, torch.float32, src_dtype = torch.bfloat16)
        get_index_8 = self.get_index('index0')
        load_8 = ops.load('mm_6', get_index_8)
        constant_5 = ops.constant(0.03333333333333333, torch.bfloat16)
        mul_6 = ops.mul(load_8, constant_5)
        tanh_1 = ops.tanh(mul_6)
        to_dtype_3 = ops.to_dtype(tanh_1, torch.float32, src_dtype = torch.bfloat16)
        get_index_9 = self.get_index('index0')
        load_9 = ops.load('mm_6', get_index_9)
        constant_6 = ops.constant(0.03333333333333333, torch.bfloat16)
        mul_7 = ops.mul(load_9, constant_6)
        tanh_2 = ops.tanh(mul_7)
        to_dtype_4 = ops.to_dtype(tanh_2, torch.float32, src_dtype = torch.bfloat16)
        mul_8 = ops.mul(to_dtype_3, to_dtype_4)
        constant_7 = ops.constant(1.0, torch.float32)
        sub_3 = ops.sub(constant_7, mul_8)
        mul_9 = ops.mul(to_dtype_2, sub_3)
        get_index_10 = self.get_index('index0')
        store = ops.store('buf15', get_index_10, mul_9, None)
        return store


op16: SchedulerNode(ComputedBuffer)
op16.writes = [MemoryDep('buf16', c0, {c0: 262400*(((s3 + 8)//9))})]
op16.unmet_dependencies = [MemoryDep('buf15', c0, {c0: 262400*(((s3 + 8)//9))})]
op16.met_dependencies = []
op16.outputs = [
    buf16: ComputedBuffer
    buf16.layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
    buf16.users = [NodeUser(node=ExternKernelSchedulerNode(name='op17'), can_inplace=False, is_weak=False)]
]
op16.group.device = cuda:0
op16.group.iteration = (262400*(((s3 + 8)//9)), 1)
op16.sizes = ([262400*(((s3 + 8)//9))], [])
buf15_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
buf16_layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
class op16_loop_body:
    var_ranges = {p0: 262400*(((s3 + 8)//9))}
    index0 = p0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf15', get_index)
        to_dtype = ops.to_dtype(load, torch.bfloat16, src_dtype = torch.float32)
        constant = ops.constant(0.03333333333333333, torch.bfloat16)
        mul = ops.mul(to_dtype, constant)
        get_index_1 = self.get_index('index0')
        store = ops.store('buf16', get_index_1, mul, None)
        return store


op17: ExternKernelSchedulerNode(ExternKernelOut)
op17.writes = [StarDep(name='buf17', mode=None)]
op17.unmet_dependencies = [StarDep(name='buf16', mode=None)]
op17.met_dependencies = [StarDep(name='permute_10', mode=None)]
op17.outputs = [
    buf17: ExternKernelOut
    buf17.layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 2048], stride=[2048, 1])
    buf17.users = [NodeUser(node=SchedulerNode(name='op66'), can_inplace=False, is_weak=False)]
]
op17.node.kernel = extern_kernels.mm


op18: SchedulerNode(ComputedBuffer)
op18.writes = [MemoryDep('buf18', c0, {c0: 262400*(((s3 + 8)//9))})]
op18.unmet_dependencies = []
op18.met_dependencies = []
op18.outputs = [
    buf18: ComputedBuffer
    buf18.layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
    buf18.users = [NodeUser(node=SchedulerNode(name='op19'), can_inplace=False, is_weak=False)]
]
op18.group.device = cuda:0
op18.group.iteration = (262400*(((s3 + 8)//9)), 1)
op18.sizes = ([262400*(((s3 + 8)//9))], [])
buf18_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
class op18_loop_body:
    var_ranges = {p0: 262400*(((s3 + 8)//9))}
    index0 = p0
    def body(self, ops):
        constant = ops.constant(0.0, torch.float32)
        get_index = self.get_index('index0')
        store = ops.store('buf18', get_index, constant, None)
        return store


op19: SchedulerNode(ComputedBuffer)
op19.writes = [MemoryDep('buf19', 262400*c0 + tmp0, {c0: ((s0 + 8)//9)})]
op19.unmet_dependencies = [StarDep(name='buf18', mode=None)]
op19.met_dependencies = [MemoryDep('where_24', c0, {c0: ((s0 + 8)//9)})]
op19.outputs = [
    buf19: ComputedBuffer
    buf19.layout = MutationLayoutSHOULDREMOVE('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
    buf19.mutations = ['buf18']
    buf19.users = [
        NodeUser(node=SchedulerNode(name='op20'), can_inplace=False, is_weak=False),
        NodeUser(node=SchedulerNode(name='op22'), can_inplace=True, is_weak=False),
    ]
]
op19.group.device = cuda:0
op19.group.iteration = (((s0 + 8)//9), 1)
op19.sizes = ([((s0 + 8)//9)], [])
where_24_layout = FixedLayout('cuda:0', torch.int64, size=[((s0 + 8)//9), 1], stride=[1, 1])
buf18_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
buf19_layout = MutationLayoutSHOULDREMOVE('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
class op19_loop_body:
    var_ranges = {p0: ((s0 + 8)//9)}
    index0 = p0
    index1 = indirect0 + 262400*p0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('where_24', get_index)
        set_indirect0 = self.set_indirect0(load)
        constant = ops.constant(-1.0, torch.float32)
        get_index_1 = self.get_index('index1')
        store = ops.store('buf19', get_index_1, constant, None)
        return store


op20: SchedulerNode(ComputedBuffer)
op20.writes = [MemoryDep('buf20', c0, {c0: 5*(((s3 + 8)//9))})]
op20.unmet_dependencies = [MemoryDep('buf19', c0, {c0: 262400*(((s3 + 8)//9))})]
op20.met_dependencies = 
    [   MemoryDep('convert_element_type_45', 0, {}),
        MemoryDep('ne_61', c0, {c0: ((s3 + 8)//9)}),
        MemoryDep('tangents_1', 0, {})]
op20.outputs = [
    buf20: ComputedBuffer
    buf20.layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1, 5], stride=[5, 5*(((s3 + 8)//9)), 1])
    buf20.users = [NodeUser(node=SchedulerNode(name='op21'), can_inplace=False, is_weak=False)]
]
op20.group.device = cuda:0
op20.group.iteration = (5*(((s3 + 8)//9)), 52480)
op20.sizes = ([((s3 + 8)//9), 5], [52480])
buf19_layout = MutationLayoutSHOULDREMOVE('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
ne_61_layout = FixedLayout('cuda:0', torch.bool, size=[((s0 + 8)//9), 1], stride=[1, 1])
tangents_1_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
convert_element_type_45_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
buf20_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1, 5], stride=[5, 5*(((s3 + 8)//9)), 1])
class op20_loop_body:
    var_ranges = {p0: ((s3 + 8)//9), p1: 5, p2: 52480}
    index0 = 262400*p0 + 52480*p1 + p2
    index1 = p0
    index2 = 0
    index3 = 5*p0 + p1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf18', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('ne_61', get_index_1)
        get_index_2 = self.get_index('index2')
        load_2 = ops.load('tangents_1', get_index_2)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('convert_element_type_45', get_index_3)
        truediv = ops.truediv(load_2, load_3)
        constant = ops.constant(0.0, torch.float32)
        where = ops.where(load_1, truediv, constant)
        mul = ops.mul(load, where)
        reduction = ops.reduction(torch.float32, torch.float32, 'sum', mul)
        get_index_4 = self.get_index('index3')
        store_reduction = ops.store_reduction('buf20', get_index_4, reduction)
        return store_reduction


op21: SchedulerNode(ComputedBuffer)
op21.writes = [MemoryDep('buf21', c0, {c0: ((s3 + 8)//9)})]
op21.unmet_dependencies = [MemoryDep('buf20', c0, {c0: 5*(((s3 + 8)//9))})]
op21.met_dependencies = []
op21.outputs = [
    buf21: ComputedBuffer
    buf21.layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, ((s3 + 8)//9)])
    buf21.users = [NodeUser(node=SchedulerNode(name='op22'), can_inplace=False, is_weak=False)]
]
op21.group.device = cuda:0
op21.group.iteration = (((s3 + 8)//9), 5)
op21.sizes = ([((s3 + 8)//9)], [5])
buf20_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1, 5], stride=[5, 5*(((s3 + 8)//9)), 1])
buf21_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, ((s3 + 8)//9)])
class op21_loop_body:
    var_ranges = {p0: ((s3 + 8)//9), p1: 5}
    index0 = 5*p0 + p1
    index1 = p0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf20', get_index)
        reduction = ops.reduction(torch.float32, torch.float32, 'sum', load)
        get_index_1 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf21', get_index_1, reduction)
        return store_reduction


op22: SchedulerNode(ComputedBuffer)
op22.writes = [MemoryDep('buf22', c0, {c0: 262400*(((s3 + 8)//9))})]
op22.unmet_dependencies = 
    [   MemoryDep('buf19', c0, {c0: 262400*(((s3 + 8)//9))}),
        MemoryDep('buf21', c0, {c0: ((s3 + 8)//9)})]
op22.met_dependencies = 
    [   MemoryDep('amax_default_3', c0, {c0: ((s3 + 8)//9)}),
        MemoryDep('convert_element_type_45', 0, {}),
        MemoryDep('log_5', c0, {c0: ((s3 + 8)//9)}),
        MemoryDep('mm_5', c0, {c0: 262400*(((s3 + 8)//9))}),
        MemoryDep('ne_61', c0, {c0: ((s3 + 8)//9)}),
        MemoryDep('tangents_1', 0, {})]
op22.outputs = [
    buf22: ComputedBuffer
    buf22.layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
    buf22.users = [NodeUser(node=SchedulerNode(name='op23'), can_inplace=True, is_weak=False)]
]
op22.group.device = cuda:0
op22.group.iteration = (262400*(((s3 + 8)//9)), 1)
op22.sizes = ([((s3 + 8)//9), 262400], [])
buf19_layout = MutationLayoutSHOULDREMOVE('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
ne_61_layout = FixedLayout('cuda:0', torch.bool, size=[((s0 + 8)//9), 1], stride=[1, 1])
tangents_1_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
convert_element_type_45_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
mm_5_layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
amax_default_3_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, 1])
log_5_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, 1])
buf21_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, ((s3 + 8)//9)])
buf22_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
class op22_loop_body:
    var_ranges = {p0: ((s3 + 8)//9), p1: 262400}
    index0 = 262400*p0 + p1
    index1 = p0
    index2 = 0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf18', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('ne_61', get_index_1)
        get_index_2 = self.get_index('index2')
        load_2 = ops.load('tangents_1', get_index_2)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('convert_element_type_45', get_index_3)
        truediv = ops.truediv(load_2, load_3)
        constant = ops.constant(0.0, torch.float32)
        where = ops.where(load_1, truediv, constant)
        mul = ops.mul(load, where)
        get_index_4 = self.get_index('index0')
        load_4 = ops.load('mm_5', get_index_4)
        constant_1 = ops.constant(0.03333333333333333, torch.bfloat16)
        mul_1 = ops.mul(load_4, constant_1)
        tanh = ops.tanh(mul_1)
        to_dtype = ops.to_dtype(tanh, torch.float32, src_dtype = torch.bfloat16)
        constant_2 = ops.constant(1.0, torch.float32)
        mul_2 = ops.mul(to_dtype, constant_2)
        get_index_5 = self.get_index('index1')
        load_5 = ops.load('amax_default_3', get_index_5)
        sub = ops.sub(mul_2, load_5)
        constant_3 = ops.constant(30.0, torch.float32)
        mul_3 = ops.mul(sub, constant_3)
        get_index_6 = self.get_index('index1')
        load_6 = ops.load('log_5', get_index_6)
        sub_1 = ops.sub(mul_3, load_6)
        exp = ops.exp(sub_1)
        get_index_7 = self.get_index('index1')
        load_7 = ops.load('buf21', get_index_7)
        mul_4 = ops.mul(exp, load_7)
        sub_2 = ops.sub(mul, mul_4)
        to_dtype_1 = ops.to_dtype(sub_2, torch.bfloat16, src_dtype = torch.float32)
        constant_4 = ops.constant(30.0, torch.bfloat16)
        mul_5 = ops.mul(to_dtype_1, constant_4)
        to_dtype_2 = ops.to_dtype(mul_5, torch.float32, src_dtype = torch.bfloat16)
        get_index_8 = self.get_index('index0')
        load_8 = ops.load('mm_5', get_index_8)
        constant_5 = ops.constant(0.03333333333333333, torch.bfloat16)
        mul_6 = ops.mul(load_8, constant_5)
        tanh_1 = ops.tanh(mul_6)
        to_dtype_3 = ops.to_dtype(tanh_1, torch.float32, src_dtype = torch.bfloat16)
        get_index_9 = self.get_index('index0')
        load_9 = ops.load('mm_5', get_index_9)
        constant_6 = ops.constant(0.03333333333333333, torch.bfloat16)
        mul_7 = ops.mul(load_9, constant_6)
        tanh_2 = ops.tanh(mul_7)
        to_dtype_4 = ops.to_dtype(tanh_2, torch.float32, src_dtype = torch.bfloat16)
        mul_8 = ops.mul(to_dtype_3, to_dtype_4)
        constant_7 = ops.constant(1.0, torch.float32)
        sub_3 = ops.sub(constant_7, mul_8)
        mul_9 = ops.mul(to_dtype_2, sub_3)
        get_index_10 = self.get_index('index0')
        store = ops.store('buf22', get_index_10, mul_9, None)
        return store


op23: SchedulerNode(ComputedBuffer)
op23.writes = [MemoryDep('buf23', c0, {c0: 262400*(((s3 + 8)//9))})]
op23.unmet_dependencies = [MemoryDep('buf22', c0, {c0: 262400*(((s3 + 8)//9))})]
op23.met_dependencies = []
op23.outputs = [
    buf23: ComputedBuffer
    buf23.layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
    buf23.users = [NodeUser(node=ExternKernelSchedulerNode(name='op24'), can_inplace=False, is_weak=False)]
]
op23.group.device = cuda:0
op23.group.iteration = (262400*(((s3 + 8)//9)), 1)
op23.sizes = ([262400*(((s3 + 8)//9))], [])
buf22_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
buf23_layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
class op23_loop_body:
    var_ranges = {p0: 262400*(((s3 + 8)//9))}
    index0 = p0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf22', get_index)
        to_dtype = ops.to_dtype(load, torch.bfloat16, src_dtype = torch.float32)
        constant = ops.constant(0.03333333333333333, torch.bfloat16)
        mul = ops.mul(to_dtype, constant)
        get_index_1 = self.get_index('index0')
        store = ops.store('buf23', get_index_1, mul, None)
        return store


op24: ExternKernelSchedulerNode(ExternKernelOut)
op24.writes = [StarDep(name='buf24', mode=None)]
op24.unmet_dependencies = [StarDep(name='buf23', mode=None)]
op24.met_dependencies = [StarDep(name='permute_10', mode=None)]
op24.outputs = [
    buf24: ExternKernelOut
    buf24.layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 2048], stride=[2048, 1])
    buf24.users = [NodeUser(node=SchedulerNode(name='op65'), can_inplace=False, is_weak=False)]
]
op24.node.kernel = extern_kernels.mm


op25: SchedulerNode(ComputedBuffer)
op25.writes = [MemoryDep('buf25', c0, {c0: 262400*(((s3 + 8)//9))})]
op25.unmet_dependencies = []
op25.met_dependencies = []
op25.outputs = [
    buf25: ComputedBuffer
    buf25.layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
    buf25.users = [NodeUser(node=SchedulerNode(name='op26'), can_inplace=False, is_weak=False)]
]
op25.group.device = cuda:0
op25.group.iteration = (262400*(((s3 + 8)//9)), 1)
op25.sizes = ([262400*(((s3 + 8)//9))], [])
buf25_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
class op25_loop_body:
    var_ranges = {p0: 262400*(((s3 + 8)//9))}
    index0 = p0
    def body(self, ops):
        constant = ops.constant(0.0, torch.float32)
        get_index = self.get_index('index0')
        store = ops.store('buf25', get_index, constant, None)
        return store


op26: SchedulerNode(ComputedBuffer)
op26.writes = [MemoryDep('buf26', 262400*c0 + tmp0, {c0: ((s0 + 8)//9)})]
op26.unmet_dependencies = [StarDep(name='buf25', mode=None)]
op26.met_dependencies = [MemoryDep('where_26', c0, {c0: ((s0 + 8)//9)})]
op26.outputs = [
    buf26: ComputedBuffer
    buf26.layout = MutationLayoutSHOULDREMOVE('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
    buf26.mutations = ['buf25']
    buf26.users = [
        NodeUser(node=SchedulerNode(name='op27'), can_inplace=False, is_weak=False),
        NodeUser(node=SchedulerNode(name='op29'), can_inplace=True, is_weak=False),
    ]
]
op26.group.device = cuda:0
op26.group.iteration = (((s0 + 8)//9), 1)
op26.sizes = ([((s0 + 8)//9)], [])
where_26_layout = FixedLayout('cuda:0', torch.int64, size=[((s0 + 8)//9), 1], stride=[1, 1])
buf25_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
buf26_layout = MutationLayoutSHOULDREMOVE('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
class op26_loop_body:
    var_ranges = {p0: ((s0 + 8)//9)}
    index0 = p0
    index1 = indirect0 + 262400*p0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('where_26', get_index)
        set_indirect0 = self.set_indirect0(load)
        constant = ops.constant(-1.0, torch.float32)
        get_index_1 = self.get_index('index1')
        store = ops.store('buf26', get_index_1, constant, None)
        return store


op27: SchedulerNode(ComputedBuffer)
op27.writes = [MemoryDep('buf27', c0, {c0: 5*(((s3 + 8)//9))})]
op27.unmet_dependencies = [MemoryDep('buf26', c0, {c0: 262400*(((s3 + 8)//9))})]
op27.met_dependencies = 
    [   MemoryDep('convert_element_type_45', 0, {}),
        MemoryDep('ne_63', c0, {c0: ((s3 + 8)//9)}),
        MemoryDep('tangents_1', 0, {})]
op27.outputs = [
    buf27: ComputedBuffer
    buf27.layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1, 5], stride=[5, 5*(((s3 + 8)//9)), 1])
    buf27.users = [NodeUser(node=SchedulerNode(name='op28'), can_inplace=False, is_weak=False)]
]
op27.group.device = cuda:0
op27.group.iteration = (5*(((s3 + 8)//9)), 52480)
op27.sizes = ([((s3 + 8)//9), 5], [52480])
buf26_layout = MutationLayoutSHOULDREMOVE('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
ne_63_layout = FixedLayout('cuda:0', torch.bool, size=[((s0 + 8)//9), 1], stride=[1, 1])
tangents_1_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
convert_element_type_45_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
buf27_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1, 5], stride=[5, 5*(((s3 + 8)//9)), 1])
class op27_loop_body:
    var_ranges = {p0: ((s3 + 8)//9), p1: 5, p2: 52480}
    index0 = 262400*p0 + 52480*p1 + p2
    index1 = p0
    index2 = 0
    index3 = 5*p0 + p1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf25', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('ne_63', get_index_1)
        get_index_2 = self.get_index('index2')
        load_2 = ops.load('tangents_1', get_index_2)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('convert_element_type_45', get_index_3)
        truediv = ops.truediv(load_2, load_3)
        constant = ops.constant(0.0, torch.float32)
        where = ops.where(load_1, truediv, constant)
        mul = ops.mul(load, where)
        reduction = ops.reduction(torch.float32, torch.float32, 'sum', mul)
        get_index_4 = self.get_index('index3')
        store_reduction = ops.store_reduction('buf27', get_index_4, reduction)
        return store_reduction


op28: SchedulerNode(ComputedBuffer)
op28.writes = [MemoryDep('buf28', c0, {c0: ((s3 + 8)//9)})]
op28.unmet_dependencies = [MemoryDep('buf27', c0, {c0: 5*(((s3 + 8)//9))})]
op28.met_dependencies = []
op28.outputs = [
    buf28: ComputedBuffer
    buf28.layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, ((s3 + 8)//9)])
    buf28.users = [NodeUser(node=SchedulerNode(name='op29'), can_inplace=False, is_weak=False)]
]
op28.group.device = cuda:0
op28.group.iteration = (((s3 + 8)//9), 5)
op28.sizes = ([((s3 + 8)//9)], [5])
buf27_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1, 5], stride=[5, 5*(((s3 + 8)//9)), 1])
buf28_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, ((s3 + 8)//9)])
class op28_loop_body:
    var_ranges = {p0: ((s3 + 8)//9), p1: 5}
    index0 = 5*p0 + p1
    index1 = p0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf27', get_index)
        reduction = ops.reduction(torch.float32, torch.float32, 'sum', load)
        get_index_1 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf28', get_index_1, reduction)
        return store_reduction


op29: SchedulerNode(ComputedBuffer)
op29.writes = [MemoryDep('buf29', c0, {c0: 262400*(((s3 + 8)//9))})]
op29.unmet_dependencies = 
    [   MemoryDep('buf26', c0, {c0: 262400*(((s3 + 8)//9))}),
        MemoryDep('buf28', c0, {c0: ((s3 + 8)//9)})]
op29.met_dependencies = 
    [   MemoryDep('amax_default_4', c0, {c0: ((s3 + 8)//9)}),
        MemoryDep('convert_element_type_45', 0, {}),
        MemoryDep('log_4', c0, {c0: ((s3 + 8)//9)}),
        MemoryDep('mm_4', c0, {c0: 262400*(((s3 + 8)//9))}),
        MemoryDep('ne_63', c0, {c0: ((s3 + 8)//9)}),
        MemoryDep('tangents_1', 0, {})]
op29.outputs = [
    buf29: ComputedBuffer
    buf29.layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
    buf29.users = [NodeUser(node=SchedulerNode(name='op30'), can_inplace=True, is_weak=False)]
]
op29.group.device = cuda:0
op29.group.iteration = (262400*(((s3 + 8)//9)), 1)
op29.sizes = ([((s3 + 8)//9), 262400], [])
buf26_layout = MutationLayoutSHOULDREMOVE('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
ne_63_layout = FixedLayout('cuda:0', torch.bool, size=[((s0 + 8)//9), 1], stride=[1, 1])
tangents_1_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
convert_element_type_45_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
mm_4_layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
amax_default_4_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, 1])
log_4_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, 1])
buf28_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, ((s3 + 8)//9)])
buf29_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
class op29_loop_body:
    var_ranges = {p0: ((s3 + 8)//9), p1: 262400}
    index0 = 262400*p0 + p1
    index1 = p0
    index2 = 0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf25', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('ne_63', get_index_1)
        get_index_2 = self.get_index('index2')
        load_2 = ops.load('tangents_1', get_index_2)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('convert_element_type_45', get_index_3)
        truediv = ops.truediv(load_2, load_3)
        constant = ops.constant(0.0, torch.float32)
        where = ops.where(load_1, truediv, constant)
        mul = ops.mul(load, where)
        get_index_4 = self.get_index('index0')
        load_4 = ops.load('mm_4', get_index_4)
        constant_1 = ops.constant(0.03333333333333333, torch.bfloat16)
        mul_1 = ops.mul(load_4, constant_1)
        tanh = ops.tanh(mul_1)
        to_dtype = ops.to_dtype(tanh, torch.float32, src_dtype = torch.bfloat16)
        constant_2 = ops.constant(1.0, torch.float32)
        mul_2 = ops.mul(to_dtype, constant_2)
        get_index_5 = self.get_index('index1')
        load_5 = ops.load('amax_default_4', get_index_5)
        sub = ops.sub(mul_2, load_5)
        constant_3 = ops.constant(30.0, torch.float32)
        mul_3 = ops.mul(sub, constant_3)
        get_index_6 = self.get_index('index1')
        load_6 = ops.load('log_4', get_index_6)
        sub_1 = ops.sub(mul_3, load_6)
        exp = ops.exp(sub_1)
        get_index_7 = self.get_index('index1')
        load_7 = ops.load('buf28', get_index_7)
        mul_4 = ops.mul(exp, load_7)
        sub_2 = ops.sub(mul, mul_4)
        to_dtype_1 = ops.to_dtype(sub_2, torch.bfloat16, src_dtype = torch.float32)
        constant_4 = ops.constant(30.0, torch.bfloat16)
        mul_5 = ops.mul(to_dtype_1, constant_4)
        to_dtype_2 = ops.to_dtype(mul_5, torch.float32, src_dtype = torch.bfloat16)
        get_index_8 = self.get_index('index0')
        load_8 = ops.load('mm_4', get_index_8)
        constant_5 = ops.constant(0.03333333333333333, torch.bfloat16)
        mul_6 = ops.mul(load_8, constant_5)
        tanh_1 = ops.tanh(mul_6)
        to_dtype_3 = ops.to_dtype(tanh_1, torch.float32, src_dtype = torch.bfloat16)
        get_index_9 = self.get_index('index0')
        load_9 = ops.load('mm_4', get_index_9)
        constant_6 = ops.constant(0.03333333333333333, torch.bfloat16)
        mul_7 = ops.mul(load_9, constant_6)
        tanh_2 = ops.tanh(mul_7)
        to_dtype_4 = ops.to_dtype(tanh_2, torch.float32, src_dtype = torch.bfloat16)
        mul_8 = ops.mul(to_dtype_3, to_dtype_4)
        constant_7 = ops.constant(1.0, torch.float32)
        sub_3 = ops.sub(constant_7, mul_8)
        mul_9 = ops.mul(to_dtype_2, sub_3)
        get_index_10 = self.get_index('index0')
        store = ops.store('buf29', get_index_10, mul_9, None)
        return store


op30: SchedulerNode(ComputedBuffer)
op30.writes = [MemoryDep('buf30', c0, {c0: 262400*(((s3 + 8)//9))})]
op30.unmet_dependencies = [MemoryDep('buf29', c0, {c0: 262400*(((s3 + 8)//9))})]
op30.met_dependencies = []
op30.outputs = [
    buf30: ComputedBuffer
    buf30.layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
    buf30.users = [NodeUser(node=ExternKernelSchedulerNode(name='op31'), can_inplace=False, is_weak=False)]
]
op30.group.device = cuda:0
op30.group.iteration = (262400*(((s3 + 8)//9)), 1)
op30.sizes = ([262400*(((s3 + 8)//9))], [])
buf29_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
buf30_layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
class op30_loop_body:
    var_ranges = {p0: 262400*(((s3 + 8)//9))}
    index0 = p0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf29', get_index)
        to_dtype = ops.to_dtype(load, torch.bfloat16, src_dtype = torch.float32)
        constant = ops.constant(0.03333333333333333, torch.bfloat16)
        mul = ops.mul(to_dtype, constant)
        get_index_1 = self.get_index('index0')
        store = ops.store('buf30', get_index_1, mul, None)
        return store


op31: ExternKernelSchedulerNode(ExternKernelOut)
op31.writes = [StarDep(name='buf31', mode=None)]
op31.unmet_dependencies = [StarDep(name='buf30', mode=None)]
op31.met_dependencies = [StarDep(name='permute_10', mode=None)]
op31.outputs = [
    buf31: ExternKernelOut
    buf31.layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 2048], stride=[2048, 1])
    buf31.users = [NodeUser(node=SchedulerNode(name='op64'), can_inplace=False, is_weak=False)]
]
op31.node.kernel = extern_kernels.mm


op32: SchedulerNode(ComputedBuffer)
op32.writes = [MemoryDep('buf32', c0, {c0: 262400*(((s3 + 8)//9))})]
op32.unmet_dependencies = []
op32.met_dependencies = []
op32.outputs = [
    buf32: ComputedBuffer
    buf32.layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
    buf32.users = [NodeUser(node=SchedulerNode(name='op33'), can_inplace=False, is_weak=False)]
]
op32.group.device = cuda:0
op32.group.iteration = (262400*(((s3 + 8)//9)), 1)
op32.sizes = ([262400*(((s3 + 8)//9))], [])
buf32_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
class op32_loop_body:
    var_ranges = {p0: 262400*(((s3 + 8)//9))}
    index0 = p0
    def body(self, ops):
        constant = ops.constant(0.0, torch.float32)
        get_index = self.get_index('index0')
        store = ops.store('buf32', get_index, constant, None)
        return store


op33: SchedulerNode(ComputedBuffer)
op33.writes = [MemoryDep('buf33', 262400*c0 + tmp0, {c0: ((s0 + 8)//9)})]
op33.unmet_dependencies = [StarDep(name='buf32', mode=None)]
op33.met_dependencies = [MemoryDep('where_28', c0, {c0: ((s0 + 8)//9)})]
op33.outputs = [
    buf33: ComputedBuffer
    buf33.layout = MutationLayoutSHOULDREMOVE('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
    buf33.mutations = ['buf32']
    buf33.users = [
        NodeUser(node=SchedulerNode(name='op34'), can_inplace=False, is_weak=False),
        NodeUser(node=SchedulerNode(name='op36'), can_inplace=True, is_weak=False),
    ]
]
op33.group.device = cuda:0
op33.group.iteration = (((s0 + 8)//9), 1)
op33.sizes = ([((s0 + 8)//9)], [])
where_28_layout = FixedLayout('cuda:0', torch.int64, size=[((s0 + 8)//9), 1], stride=[1, 1])
buf32_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
buf33_layout = MutationLayoutSHOULDREMOVE('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
class op33_loop_body:
    var_ranges = {p0: ((s0 + 8)//9)}
    index0 = p0
    index1 = indirect0 + 262400*p0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('where_28', get_index)
        set_indirect0 = self.set_indirect0(load)
        constant = ops.constant(-1.0, torch.float32)
        get_index_1 = self.get_index('index1')
        store = ops.store('buf33', get_index_1, constant, None)
        return store


op34: SchedulerNode(ComputedBuffer)
op34.writes = [MemoryDep('buf34', c0, {c0: 5*(((s3 + 8)//9))})]
op34.unmet_dependencies = [MemoryDep('buf33', c0, {c0: 262400*(((s3 + 8)//9))})]
op34.met_dependencies = 
    [   MemoryDep('convert_element_type_45', 0, {}),
        MemoryDep('ne_65', c0, {c0: ((s3 + 8)//9)}),
        MemoryDep('tangents_1', 0, {})]
op34.outputs = [
    buf34: ComputedBuffer
    buf34.layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1, 5], stride=[5, 5*(((s3 + 8)//9)), 1])
    buf34.users = [NodeUser(node=SchedulerNode(name='op35'), can_inplace=False, is_weak=False)]
]
op34.group.device = cuda:0
op34.group.iteration = (5*(((s3 + 8)//9)), 52480)
op34.sizes = ([((s3 + 8)//9), 5], [52480])
buf33_layout = MutationLayoutSHOULDREMOVE('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
ne_65_layout = FixedLayout('cuda:0', torch.bool, size=[((s0 + 8)//9), 1], stride=[1, 1])
tangents_1_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
convert_element_type_45_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
buf34_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1, 5], stride=[5, 5*(((s3 + 8)//9)), 1])
class op34_loop_body:
    var_ranges = {p0: ((s3 + 8)//9), p1: 5, p2: 52480}
    index0 = 262400*p0 + 52480*p1 + p2
    index1 = p0
    index2 = 0
    index3 = 5*p0 + p1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf32', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('ne_65', get_index_1)
        get_index_2 = self.get_index('index2')
        load_2 = ops.load('tangents_1', get_index_2)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('convert_element_type_45', get_index_3)
        truediv = ops.truediv(load_2, load_3)
        constant = ops.constant(0.0, torch.float32)
        where = ops.where(load_1, truediv, constant)
        mul = ops.mul(load, where)
        reduction = ops.reduction(torch.float32, torch.float32, 'sum', mul)
        get_index_4 = self.get_index('index3')
        store_reduction = ops.store_reduction('buf34', get_index_4, reduction)
        return store_reduction


op35: SchedulerNode(ComputedBuffer)
op35.writes = [MemoryDep('buf35', c0, {c0: ((s3 + 8)//9)})]
op35.unmet_dependencies = [MemoryDep('buf34', c0, {c0: 5*(((s3 + 8)//9))})]
op35.met_dependencies = []
op35.outputs = [
    buf35: ComputedBuffer
    buf35.layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, ((s3 + 8)//9)])
    buf35.users = [NodeUser(node=SchedulerNode(name='op36'), can_inplace=False, is_weak=False)]
]
op35.group.device = cuda:0
op35.group.iteration = (((s3 + 8)//9), 5)
op35.sizes = ([((s3 + 8)//9)], [5])
buf34_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1, 5], stride=[5, 5*(((s3 + 8)//9)), 1])
buf35_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, ((s3 + 8)//9)])
class op35_loop_body:
    var_ranges = {p0: ((s3 + 8)//9), p1: 5}
    index0 = 5*p0 + p1
    index1 = p0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf34', get_index)
        reduction = ops.reduction(torch.float32, torch.float32, 'sum', load)
        get_index_1 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf35', get_index_1, reduction)
        return store_reduction


op36: SchedulerNode(ComputedBuffer)
op36.writes = [MemoryDep('buf36', c0, {c0: 262400*(((s3 + 8)//9))})]
op36.unmet_dependencies = 
    [   MemoryDep('buf33', c0, {c0: 262400*(((s3 + 8)//9))}),
        MemoryDep('buf35', c0, {c0: ((s3 + 8)//9)})]
op36.met_dependencies = 
    [   MemoryDep('amax_default_5', c0, {c0: ((s3 + 8)//9)}),
        MemoryDep('convert_element_type_45', 0, {}),
        MemoryDep('log_3', c0, {c0: ((s3 + 8)//9)}),
        MemoryDep('mm_3', c0, {c0: 262400*(((s3 + 8)//9))}),
        MemoryDep('ne_65', c0, {c0: ((s3 + 8)//9)}),
        MemoryDep('tangents_1', 0, {})]
op36.outputs = [
    buf36: ComputedBuffer
    buf36.layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
    buf36.users = [NodeUser(node=SchedulerNode(name='op37'), can_inplace=True, is_weak=False)]
]
op36.group.device = cuda:0
op36.group.iteration = (262400*(((s3 + 8)//9)), 1)
op36.sizes = ([((s3 + 8)//9), 262400], [])
buf33_layout = MutationLayoutSHOULDREMOVE('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
ne_65_layout = FixedLayout('cuda:0', torch.bool, size=[((s0 + 8)//9), 1], stride=[1, 1])
tangents_1_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
convert_element_type_45_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
mm_3_layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
amax_default_5_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, 1])
log_3_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, 1])
buf35_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, ((s3 + 8)//9)])
buf36_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
class op36_loop_body:
    var_ranges = {p0: ((s3 + 8)//9), p1: 262400}
    index0 = 262400*p0 + p1
    index1 = p0
    index2 = 0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf32', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('ne_65', get_index_1)
        get_index_2 = self.get_index('index2')
        load_2 = ops.load('tangents_1', get_index_2)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('convert_element_type_45', get_index_3)
        truediv = ops.truediv(load_2, load_3)
        constant = ops.constant(0.0, torch.float32)
        where = ops.where(load_1, truediv, constant)
        mul = ops.mul(load, where)
        get_index_4 = self.get_index('index0')
        load_4 = ops.load('mm_3', get_index_4)
        constant_1 = ops.constant(0.03333333333333333, torch.bfloat16)
        mul_1 = ops.mul(load_4, constant_1)
        tanh = ops.tanh(mul_1)
        to_dtype = ops.to_dtype(tanh, torch.float32, src_dtype = torch.bfloat16)
        constant_2 = ops.constant(1.0, torch.float32)
        mul_2 = ops.mul(to_dtype, constant_2)
        get_index_5 = self.get_index('index1')
        load_5 = ops.load('amax_default_5', get_index_5)
        sub = ops.sub(mul_2, load_5)
        constant_3 = ops.constant(30.0, torch.float32)
        mul_3 = ops.mul(sub, constant_3)
        get_index_6 = self.get_index('index1')
        load_6 = ops.load('log_3', get_index_6)
        sub_1 = ops.sub(mul_3, load_6)
        exp = ops.exp(sub_1)
        get_index_7 = self.get_index('index1')
        load_7 = ops.load('buf35', get_index_7)
        mul_4 = ops.mul(exp, load_7)
        sub_2 = ops.sub(mul, mul_4)
        to_dtype_1 = ops.to_dtype(sub_2, torch.bfloat16, src_dtype = torch.float32)
        constant_4 = ops.constant(30.0, torch.bfloat16)
        mul_5 = ops.mul(to_dtype_1, constant_4)
        to_dtype_2 = ops.to_dtype(mul_5, torch.float32, src_dtype = torch.bfloat16)
        get_index_8 = self.get_index('index0')
        load_8 = ops.load('mm_3', get_index_8)
        constant_5 = ops.constant(0.03333333333333333, torch.bfloat16)
        mul_6 = ops.mul(load_8, constant_5)
        tanh_1 = ops.tanh(mul_6)
        to_dtype_3 = ops.to_dtype(tanh_1, torch.float32, src_dtype = torch.bfloat16)
        get_index_9 = self.get_index('index0')
        load_9 = ops.load('mm_3', get_index_9)
        constant_6 = ops.constant(0.03333333333333333, torch.bfloat16)
        mul_7 = ops.mul(load_9, constant_6)
        tanh_2 = ops.tanh(mul_7)
        to_dtype_4 = ops.to_dtype(tanh_2, torch.float32, src_dtype = torch.bfloat16)
        mul_8 = ops.mul(to_dtype_3, to_dtype_4)
        constant_7 = ops.constant(1.0, torch.float32)
        sub_3 = ops.sub(constant_7, mul_8)
        mul_9 = ops.mul(to_dtype_2, sub_3)
        get_index_10 = self.get_index('index0')
        store = ops.store('buf36', get_index_10, mul_9, None)
        return store


op37: SchedulerNode(ComputedBuffer)
op37.writes = [MemoryDep('buf37', c0, {c0: 262400*(((s3 + 8)//9))})]
op37.unmet_dependencies = [MemoryDep('buf36', c0, {c0: 262400*(((s3 + 8)//9))})]
op37.met_dependencies = []
op37.outputs = [
    buf37: ComputedBuffer
    buf37.layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
    buf37.users = [NodeUser(node=ExternKernelSchedulerNode(name='op38'), can_inplace=False, is_weak=False)]
]
op37.group.device = cuda:0
op37.group.iteration = (262400*(((s3 + 8)//9)), 1)
op37.sizes = ([262400*(((s3 + 8)//9))], [])
buf36_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
buf37_layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
class op37_loop_body:
    var_ranges = {p0: 262400*(((s3 + 8)//9))}
    index0 = p0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf36', get_index)
        to_dtype = ops.to_dtype(load, torch.bfloat16, src_dtype = torch.float32)
        constant = ops.constant(0.03333333333333333, torch.bfloat16)
        mul = ops.mul(to_dtype, constant)
        get_index_1 = self.get_index('index0')
        store = ops.store('buf37', get_index_1, mul, None)
        return store


op38: ExternKernelSchedulerNode(ExternKernelOut)
op38.writes = [StarDep(name='buf38', mode=None)]
op38.unmet_dependencies = [StarDep(name='buf37', mode=None)]
op38.met_dependencies = [StarDep(name='permute_10', mode=None)]
op38.outputs = [
    buf38: ExternKernelOut
    buf38.layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 2048], stride=[2048, 1])
    buf38.users = [NodeUser(node=SchedulerNode(name='op63'), can_inplace=False, is_weak=False)]
]
op38.node.kernel = extern_kernels.mm


op39: SchedulerNode(ComputedBuffer)
op39.writes = [MemoryDep('buf39', c0, {c0: 262400*(((s3 + 8)//9))})]
op39.unmet_dependencies = []
op39.met_dependencies = []
op39.outputs = [
    buf39: ComputedBuffer
    buf39.layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
    buf39.users = [NodeUser(node=SchedulerNode(name='op40'), can_inplace=False, is_weak=False)]
]
op39.group.device = cuda:0
op39.group.iteration = (262400*(((s3 + 8)//9)), 1)
op39.sizes = ([262400*(((s3 + 8)//9))], [])
buf39_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
class op39_loop_body:
    var_ranges = {p0: 262400*(((s3 + 8)//9))}
    index0 = p0
    def body(self, ops):
        constant = ops.constant(0.0, torch.float32)
        get_index = self.get_index('index0')
        store = ops.store('buf39', get_index, constant, None)
        return store


op40: SchedulerNode(ComputedBuffer)
op40.writes = [MemoryDep('buf40', 262400*c0 + tmp0, {c0: ((s0 + 8)//9)})]
op40.unmet_dependencies = [StarDep(name='buf39', mode=None)]
op40.met_dependencies = [MemoryDep('where_30', c0, {c0: ((s0 + 8)//9)})]
op40.outputs = [
    buf40: ComputedBuffer
    buf40.layout = MutationLayoutSHOULDREMOVE('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
    buf40.mutations = ['buf39']
    buf40.users = [
        NodeUser(node=SchedulerNode(name='op41'), can_inplace=False, is_weak=False),
        NodeUser(node=SchedulerNode(name='op43'), can_inplace=True, is_weak=False),
    ]
]
op40.group.device = cuda:0
op40.group.iteration = (((s0 + 8)//9), 1)
op40.sizes = ([((s0 + 8)//9)], [])
where_30_layout = FixedLayout('cuda:0', torch.int64, size=[((s0 + 8)//9), 1], stride=[1, 1])
buf39_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
buf40_layout = MutationLayoutSHOULDREMOVE('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
class op40_loop_body:
    var_ranges = {p0: ((s0 + 8)//9)}
    index0 = p0
    index1 = indirect0 + 262400*p0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('where_30', get_index)
        set_indirect0 = self.set_indirect0(load)
        constant = ops.constant(-1.0, torch.float32)
        get_index_1 = self.get_index('index1')
        store = ops.store('buf40', get_index_1, constant, None)
        return store


op41: SchedulerNode(ComputedBuffer)
op41.writes = [MemoryDep('buf41', c0, {c0: 5*(((s3 + 8)//9))})]
op41.unmet_dependencies = [MemoryDep('buf40', c0, {c0: 262400*(((s3 + 8)//9))})]
op41.met_dependencies = 
    [   MemoryDep('convert_element_type_45', 0, {}),
        MemoryDep('ne_67', c0, {c0: ((s3 + 8)//9)}),
        MemoryDep('tangents_1', 0, {})]
op41.outputs = [
    buf41: ComputedBuffer
    buf41.layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1, 5], stride=[5, 5*(((s3 + 8)//9)), 1])
    buf41.users = [NodeUser(node=SchedulerNode(name='op42'), can_inplace=False, is_weak=False)]
]
op41.group.device = cuda:0
op41.group.iteration = (5*(((s3 + 8)//9)), 52480)
op41.sizes = ([((s3 + 8)//9), 5], [52480])
buf40_layout = MutationLayoutSHOULDREMOVE('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
ne_67_layout = FixedLayout('cuda:0', torch.bool, size=[((s0 + 8)//9), 1], stride=[1, 1])
tangents_1_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
convert_element_type_45_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
buf41_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1, 5], stride=[5, 5*(((s3 + 8)//9)), 1])
class op41_loop_body:
    var_ranges = {p0: ((s3 + 8)//9), p1: 5, p2: 52480}
    index0 = 262400*p0 + 52480*p1 + p2
    index1 = p0
    index2 = 0
    index3 = 5*p0 + p1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf39', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('ne_67', get_index_1)
        get_index_2 = self.get_index('index2')
        load_2 = ops.load('tangents_1', get_index_2)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('convert_element_type_45', get_index_3)
        truediv = ops.truediv(load_2, load_3)
        constant = ops.constant(0.0, torch.float32)
        where = ops.where(load_1, truediv, constant)
        mul = ops.mul(load, where)
        reduction = ops.reduction(torch.float32, torch.float32, 'sum', mul)
        get_index_4 = self.get_index('index3')
        store_reduction = ops.store_reduction('buf41', get_index_4, reduction)
        return store_reduction


op42: SchedulerNode(ComputedBuffer)
op42.writes = [MemoryDep('buf42', c0, {c0: ((s3 + 8)//9)})]
op42.unmet_dependencies = [MemoryDep('buf41', c0, {c0: 5*(((s3 + 8)//9))})]
op42.met_dependencies = []
op42.outputs = [
    buf42: ComputedBuffer
    buf42.layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, ((s3 + 8)//9)])
    buf42.users = [NodeUser(node=SchedulerNode(name='op43'), can_inplace=False, is_weak=False)]
]
op42.group.device = cuda:0
op42.group.iteration = (((s3 + 8)//9), 5)
op42.sizes = ([((s3 + 8)//9)], [5])
buf41_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1, 5], stride=[5, 5*(((s3 + 8)//9)), 1])
buf42_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, ((s3 + 8)//9)])
class op42_loop_body:
    var_ranges = {p0: ((s3 + 8)//9), p1: 5}
    index0 = 5*p0 + p1
    index1 = p0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf41', get_index)
        reduction = ops.reduction(torch.float32, torch.float32, 'sum', load)
        get_index_1 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf42', get_index_1, reduction)
        return store_reduction


op43: SchedulerNode(ComputedBuffer)
op43.writes = [MemoryDep('buf43', c0, {c0: 262400*(((s3 + 8)//9))})]
op43.unmet_dependencies = 
    [   MemoryDep('buf40', c0, {c0: 262400*(((s3 + 8)//9))}),
        MemoryDep('buf42', c0, {c0: ((s3 + 8)//9)})]
op43.met_dependencies = 
    [   MemoryDep('amax_default_6', c0, {c0: ((s3 + 8)//9)}),
        MemoryDep('convert_element_type_45', 0, {}),
        MemoryDep('log_2', c0, {c0: ((s3 + 8)//9)}),
        MemoryDep('mm_2', c0, {c0: 262400*(((s3 + 8)//9))}),
        MemoryDep('ne_67', c0, {c0: ((s3 + 8)//9)}),
        MemoryDep('tangents_1', 0, {})]
op43.outputs = [
    buf43: ComputedBuffer
    buf43.layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
    buf43.users = [NodeUser(node=SchedulerNode(name='op44'), can_inplace=True, is_weak=False)]
]
op43.group.device = cuda:0
op43.group.iteration = (262400*(((s3 + 8)//9)), 1)
op43.sizes = ([((s3 + 8)//9), 262400], [])
buf40_layout = MutationLayoutSHOULDREMOVE('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
ne_67_layout = FixedLayout('cuda:0', torch.bool, size=[((s0 + 8)//9), 1], stride=[1, 1])
tangents_1_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
convert_element_type_45_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
mm_2_layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
amax_default_6_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, 1])
log_2_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, 1])
buf42_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, ((s3 + 8)//9)])
buf43_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
class op43_loop_body:
    var_ranges = {p0: ((s3 + 8)//9), p1: 262400}
    index0 = 262400*p0 + p1
    index1 = p0
    index2 = 0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf39', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('ne_67', get_index_1)
        get_index_2 = self.get_index('index2')
        load_2 = ops.load('tangents_1', get_index_2)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('convert_element_type_45', get_index_3)
        truediv = ops.truediv(load_2, load_3)
        constant = ops.constant(0.0, torch.float32)
        where = ops.where(load_1, truediv, constant)
        mul = ops.mul(load, where)
        get_index_4 = self.get_index('index0')
        load_4 = ops.load('mm_2', get_index_4)
        constant_1 = ops.constant(0.03333333333333333, torch.bfloat16)
        mul_1 = ops.mul(load_4, constant_1)
        tanh = ops.tanh(mul_1)
        to_dtype = ops.to_dtype(tanh, torch.float32, src_dtype = torch.bfloat16)
        constant_2 = ops.constant(1.0, torch.float32)
        mul_2 = ops.mul(to_dtype, constant_2)
        get_index_5 = self.get_index('index1')
        load_5 = ops.load('amax_default_6', get_index_5)
        sub = ops.sub(mul_2, load_5)
        constant_3 = ops.constant(30.0, torch.float32)
        mul_3 = ops.mul(sub, constant_3)
        get_index_6 = self.get_index('index1')
        load_6 = ops.load('log_2', get_index_6)
        sub_1 = ops.sub(mul_3, load_6)
        exp = ops.exp(sub_1)
        get_index_7 = self.get_index('index1')
        load_7 = ops.load('buf42', get_index_7)
        mul_4 = ops.mul(exp, load_7)
        sub_2 = ops.sub(mul, mul_4)
        to_dtype_1 = ops.to_dtype(sub_2, torch.bfloat16, src_dtype = torch.float32)
        constant_4 = ops.constant(30.0, torch.bfloat16)
        mul_5 = ops.mul(to_dtype_1, constant_4)
        to_dtype_2 = ops.to_dtype(mul_5, torch.float32, src_dtype = torch.bfloat16)
        get_index_8 = self.get_index('index0')
        load_8 = ops.load('mm_2', get_index_8)
        constant_5 = ops.constant(0.03333333333333333, torch.bfloat16)
        mul_6 = ops.mul(load_8, constant_5)
        tanh_1 = ops.tanh(mul_6)
        to_dtype_3 = ops.to_dtype(tanh_1, torch.float32, src_dtype = torch.bfloat16)
        get_index_9 = self.get_index('index0')
        load_9 = ops.load('mm_2', get_index_9)
        constant_6 = ops.constant(0.03333333333333333, torch.bfloat16)
        mul_7 = ops.mul(load_9, constant_6)
        tanh_2 = ops.tanh(mul_7)
        to_dtype_4 = ops.to_dtype(tanh_2, torch.float32, src_dtype = torch.bfloat16)
        mul_8 = ops.mul(to_dtype_3, to_dtype_4)
        constant_7 = ops.constant(1.0, torch.float32)
        sub_3 = ops.sub(constant_7, mul_8)
        mul_9 = ops.mul(to_dtype_2, sub_3)
        get_index_10 = self.get_index('index0')
        store = ops.store('buf43', get_index_10, mul_9, None)
        return store


op44: SchedulerNode(ComputedBuffer)
op44.writes = [MemoryDep('buf44', c0, {c0: 262400*(((s3 + 8)//9))})]
op44.unmet_dependencies = [MemoryDep('buf43', c0, {c0: 262400*(((s3 + 8)//9))})]
op44.met_dependencies = []
op44.outputs = [
    buf44: ComputedBuffer
    buf44.layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
    buf44.users = [NodeUser(node=ExternKernelSchedulerNode(name='op45'), can_inplace=False, is_weak=False)]
]
op44.group.device = cuda:0
op44.group.iteration = (262400*(((s3 + 8)//9)), 1)
op44.sizes = ([262400*(((s3 + 8)//9))], [])
buf43_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
buf44_layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
class op44_loop_body:
    var_ranges = {p0: 262400*(((s3 + 8)//9))}
    index0 = p0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf43', get_index)
        to_dtype = ops.to_dtype(load, torch.bfloat16, src_dtype = torch.float32)
        constant = ops.constant(0.03333333333333333, torch.bfloat16)
        mul = ops.mul(to_dtype, constant)
        get_index_1 = self.get_index('index0')
        store = ops.store('buf44', get_index_1, mul, None)
        return store


op45: ExternKernelSchedulerNode(ExternKernelOut)
op45.writes = [StarDep(name='buf45', mode=None)]
op45.unmet_dependencies = [StarDep(name='buf44', mode=None)]
op45.met_dependencies = [StarDep(name='permute_10', mode=None)]
op45.outputs = [
    buf45: ExternKernelOut
    buf45.layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 2048], stride=[2048, 1])
    buf45.users = [NodeUser(node=SchedulerNode(name='op62'), can_inplace=False, is_weak=False)]
]
op45.node.kernel = extern_kernels.mm


op46: SchedulerNode(ComputedBuffer)
op46.writes = [MemoryDep('buf46', c0, {c0: 262400*(((s3 + 8)//9))})]
op46.unmet_dependencies = []
op46.met_dependencies = []
op46.outputs = [
    buf46: ComputedBuffer
    buf46.layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
    buf46.users = [NodeUser(node=SchedulerNode(name='op47'), can_inplace=False, is_weak=False)]
]
op46.group.device = cuda:0
op46.group.iteration = (262400*(((s3 + 8)//9)), 1)
op46.sizes = ([262400*(((s3 + 8)//9))], [])
buf46_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
class op46_loop_body:
    var_ranges = {p0: 262400*(((s3 + 8)//9))}
    index0 = p0
    def body(self, ops):
        constant = ops.constant(0.0, torch.float32)
        get_index = self.get_index('index0')
        store = ops.store('buf46', get_index, constant, None)
        return store


op47: SchedulerNode(ComputedBuffer)
op47.writes = [MemoryDep('buf47', 262400*c0 + tmp0, {c0: ((s0 + 8)//9)})]
op47.unmet_dependencies = [StarDep(name='buf46', mode=None)]
op47.met_dependencies = [MemoryDep('where_32', c0, {c0: ((s0 + 8)//9)})]
op47.outputs = [
    buf47: ComputedBuffer
    buf47.layout = MutationLayoutSHOULDREMOVE('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
    buf47.mutations = ['buf46']
    buf47.users = [
        NodeUser(node=SchedulerNode(name='op48'), can_inplace=False, is_weak=False),
        NodeUser(node=SchedulerNode(name='op50'), can_inplace=True, is_weak=False),
    ]
]
op47.group.device = cuda:0
op47.group.iteration = (((s0 + 8)//9), 1)
op47.sizes = ([((s0 + 8)//9)], [])
where_32_layout = FixedLayout('cuda:0', torch.int64, size=[((s0 + 8)//9), 1], stride=[1, 1])
buf46_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
buf47_layout = MutationLayoutSHOULDREMOVE('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
class op47_loop_body:
    var_ranges = {p0: ((s0 + 8)//9)}
    index0 = p0
    index1 = indirect0 + 262400*p0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('where_32', get_index)
        set_indirect0 = self.set_indirect0(load)
        constant = ops.constant(-1.0, torch.float32)
        get_index_1 = self.get_index('index1')
        store = ops.store('buf47', get_index_1, constant, None)
        return store


op48: SchedulerNode(ComputedBuffer)
op48.writes = [MemoryDep('buf48', c0, {c0: 5*(((s3 + 8)//9))})]
op48.unmet_dependencies = [MemoryDep('buf47', c0, {c0: 262400*(((s3 + 8)//9))})]
op48.met_dependencies = 
    [   MemoryDep('convert_element_type_45', 0, {}),
        MemoryDep('ne_69', c0, {c0: ((s3 + 8)//9)}),
        MemoryDep('tangents_1', 0, {})]
op48.outputs = [
    buf48: ComputedBuffer
    buf48.layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1, 5], stride=[5, 5*(((s3 + 8)//9)), 1])
    buf48.users = [NodeUser(node=SchedulerNode(name='op49'), can_inplace=False, is_weak=False)]
]
op48.group.device = cuda:0
op48.group.iteration = (5*(((s3 + 8)//9)), 52480)
op48.sizes = ([((s3 + 8)//9), 5], [52480])
buf47_layout = MutationLayoutSHOULDREMOVE('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
ne_69_layout = FixedLayout('cuda:0', torch.bool, size=[((s0 + 8)//9), 1], stride=[1, 1])
tangents_1_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
convert_element_type_45_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
buf48_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1, 5], stride=[5, 5*(((s3 + 8)//9)), 1])
class op48_loop_body:
    var_ranges = {p0: ((s3 + 8)//9), p1: 5, p2: 52480}
    index0 = 262400*p0 + 52480*p1 + p2
    index1 = p0
    index2 = 0
    index3 = 5*p0 + p1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf46', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('ne_69', get_index_1)
        get_index_2 = self.get_index('index2')
        load_2 = ops.load('tangents_1', get_index_2)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('convert_element_type_45', get_index_3)
        truediv = ops.truediv(load_2, load_3)
        constant = ops.constant(0.0, torch.float32)
        where = ops.where(load_1, truediv, constant)
        mul = ops.mul(load, where)
        reduction = ops.reduction(torch.float32, torch.float32, 'sum', mul)
        get_index_4 = self.get_index('index3')
        store_reduction = ops.store_reduction('buf48', get_index_4, reduction)
        return store_reduction


op49: SchedulerNode(ComputedBuffer)
op49.writes = [MemoryDep('buf49', c0, {c0: ((s3 + 8)//9)})]
op49.unmet_dependencies = [MemoryDep('buf48', c0, {c0: 5*(((s3 + 8)//9))})]
op49.met_dependencies = []
op49.outputs = [
    buf49: ComputedBuffer
    buf49.layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, ((s3 + 8)//9)])
    buf49.users = [NodeUser(node=SchedulerNode(name='op50'), can_inplace=False, is_weak=False)]
]
op49.group.device = cuda:0
op49.group.iteration = (((s3 + 8)//9), 5)
op49.sizes = ([((s3 + 8)//9)], [5])
buf48_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1, 5], stride=[5, 5*(((s3 + 8)//9)), 1])
buf49_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, ((s3 + 8)//9)])
class op49_loop_body:
    var_ranges = {p0: ((s3 + 8)//9), p1: 5}
    index0 = 5*p0 + p1
    index1 = p0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf48', get_index)
        reduction = ops.reduction(torch.float32, torch.float32, 'sum', load)
        get_index_1 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf49', get_index_1, reduction)
        return store_reduction


op50: SchedulerNode(ComputedBuffer)
op50.writes = [MemoryDep('buf50', c0, {c0: 262400*(((s3 + 8)//9))})]
op50.unmet_dependencies = 
    [   MemoryDep('buf47', c0, {c0: 262400*(((s3 + 8)//9))}),
        MemoryDep('buf49', c0, {c0: ((s3 + 8)//9)})]
op50.met_dependencies = 
    [   MemoryDep('amax_default_7', c0, {c0: ((s3 + 8)//9)}),
        MemoryDep('convert_element_type_45', 0, {}),
        MemoryDep('log_1', c0, {c0: ((s3 + 8)//9)}),
        MemoryDep('mm_1', c0, {c0: 262400*(((s3 + 8)//9))}),
        MemoryDep('ne_69', c0, {c0: ((s3 + 8)//9)}),
        MemoryDep('tangents_1', 0, {})]
op50.outputs = [
    buf50: ComputedBuffer
    buf50.layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
    buf50.users = [NodeUser(node=SchedulerNode(name='op51'), can_inplace=True, is_weak=False)]
]
op50.group.device = cuda:0
op50.group.iteration = (262400*(((s3 + 8)//9)), 1)
op50.sizes = ([((s3 + 8)//9), 262400], [])
buf47_layout = MutationLayoutSHOULDREMOVE('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
ne_69_layout = FixedLayout('cuda:0', torch.bool, size=[((s0 + 8)//9), 1], stride=[1, 1])
tangents_1_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
convert_element_type_45_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
mm_1_layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
amax_default_7_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, 1])
log_1_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, 1])
buf49_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, ((s3 + 8)//9)])
buf50_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
class op50_loop_body:
    var_ranges = {p0: ((s3 + 8)//9), p1: 262400}
    index0 = 262400*p0 + p1
    index1 = p0
    index2 = 0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf46', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('ne_69', get_index_1)
        get_index_2 = self.get_index('index2')
        load_2 = ops.load('tangents_1', get_index_2)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('convert_element_type_45', get_index_3)
        truediv = ops.truediv(load_2, load_3)
        constant = ops.constant(0.0, torch.float32)
        where = ops.where(load_1, truediv, constant)
        mul = ops.mul(load, where)
        get_index_4 = self.get_index('index0')
        load_4 = ops.load('mm_1', get_index_4)
        constant_1 = ops.constant(0.03333333333333333, torch.bfloat16)
        mul_1 = ops.mul(load_4, constant_1)
        tanh = ops.tanh(mul_1)
        to_dtype = ops.to_dtype(tanh, torch.float32, src_dtype = torch.bfloat16)
        constant_2 = ops.constant(1.0, torch.float32)
        mul_2 = ops.mul(to_dtype, constant_2)
        get_index_5 = self.get_index('index1')
        load_5 = ops.load('amax_default_7', get_index_5)
        sub = ops.sub(mul_2, load_5)
        constant_3 = ops.constant(30.0, torch.float32)
        mul_3 = ops.mul(sub, constant_3)
        get_index_6 = self.get_index('index1')
        load_6 = ops.load('log_1', get_index_6)
        sub_1 = ops.sub(mul_3, load_6)
        exp = ops.exp(sub_1)
        get_index_7 = self.get_index('index1')
        load_7 = ops.load('buf49', get_index_7)
        mul_4 = ops.mul(exp, load_7)
        sub_2 = ops.sub(mul, mul_4)
        to_dtype_1 = ops.to_dtype(sub_2, torch.bfloat16, src_dtype = torch.float32)
        constant_4 = ops.constant(30.0, torch.bfloat16)
        mul_5 = ops.mul(to_dtype_1, constant_4)
        to_dtype_2 = ops.to_dtype(mul_5, torch.float32, src_dtype = torch.bfloat16)
        get_index_8 = self.get_index('index0')
        load_8 = ops.load('mm_1', get_index_8)
        constant_5 = ops.constant(0.03333333333333333, torch.bfloat16)
        mul_6 = ops.mul(load_8, constant_5)
        tanh_1 = ops.tanh(mul_6)
        to_dtype_3 = ops.to_dtype(tanh_1, torch.float32, src_dtype = torch.bfloat16)
        get_index_9 = self.get_index('index0')
        load_9 = ops.load('mm_1', get_index_9)
        constant_6 = ops.constant(0.03333333333333333, torch.bfloat16)
        mul_7 = ops.mul(load_9, constant_6)
        tanh_2 = ops.tanh(mul_7)
        to_dtype_4 = ops.to_dtype(tanh_2, torch.float32, src_dtype = torch.bfloat16)
        mul_8 = ops.mul(to_dtype_3, to_dtype_4)
        constant_7 = ops.constant(1.0, torch.float32)
        sub_3 = ops.sub(constant_7, mul_8)
        mul_9 = ops.mul(to_dtype_2, sub_3)
        get_index_10 = self.get_index('index0')
        store = ops.store('buf50', get_index_10, mul_9, None)
        return store


op51: SchedulerNode(ComputedBuffer)
op51.writes = [MemoryDep('buf51', c0, {c0: 262400*(((s3 + 8)//9))})]
op51.unmet_dependencies = [MemoryDep('buf50', c0, {c0: 262400*(((s3 + 8)//9))})]
op51.met_dependencies = []
op51.outputs = [
    buf51: ComputedBuffer
    buf51.layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
    buf51.users = [NodeUser(node=ExternKernelSchedulerNode(name='op52'), can_inplace=False, is_weak=False)]
]
op51.group.device = cuda:0
op51.group.iteration = (262400*(((s3 + 8)//9)), 1)
op51.sizes = ([262400*(((s3 + 8)//9))], [])
buf50_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
buf51_layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
class op51_loop_body:
    var_ranges = {p0: 262400*(((s3 + 8)//9))}
    index0 = p0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf50', get_index)
        to_dtype = ops.to_dtype(load, torch.bfloat16, src_dtype = torch.float32)
        constant = ops.constant(0.03333333333333333, torch.bfloat16)
        mul = ops.mul(to_dtype, constant)
        get_index_1 = self.get_index('index0')
        store = ops.store('buf51', get_index_1, mul, None)
        return store


op52: ExternKernelSchedulerNode(ExternKernelOut)
op52.writes = [StarDep(name='buf52', mode=None)]
op52.unmet_dependencies = [StarDep(name='buf51', mode=None)]
op52.met_dependencies = [StarDep(name='permute_10', mode=None)]
op52.outputs = [
    buf52: ExternKernelOut
    buf52.layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 2048], stride=[2048, 1])
    buf52.users = [NodeUser(node=SchedulerNode(name='op61'), can_inplace=False, is_weak=False)]
]
op52.node.kernel = extern_kernels.mm


op53: SchedulerNode(ComputedBuffer)
op53.writes = [MemoryDep('buf53', c0, {c0: 262400*(((s3 + 8)//9))})]
op53.unmet_dependencies = []
op53.met_dependencies = []
op53.outputs = [
    buf53: ComputedBuffer
    buf53.layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
    buf53.users = [NodeUser(node=SchedulerNode(name='op54'), can_inplace=False, is_weak=False)]
]
op53.group.device = cuda:0
op53.group.iteration = (262400*(((s3 + 8)//9)), 1)
op53.sizes = ([262400*(((s3 + 8)//9))], [])
buf53_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
class op53_loop_body:
    var_ranges = {p0: 262400*(((s3 + 8)//9))}
    index0 = p0
    def body(self, ops):
        constant = ops.constant(0.0, torch.float32)
        get_index = self.get_index('index0')
        store = ops.store('buf53', get_index, constant, None)
        return store


op54: SchedulerNode(ComputedBuffer)
op54.writes = [MemoryDep('buf54', 262400*c0 + tmp0, {c0: ((s0 + 8)//9)})]
op54.unmet_dependencies = [StarDep(name='buf53', mode=None)]
op54.met_dependencies = [MemoryDep('where_34', c0, {c0: ((s0 + 8)//9)})]
op54.outputs = [
    buf54: ComputedBuffer
    buf54.layout = MutationLayoutSHOULDREMOVE('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
    buf54.mutations = ['buf53']
    buf54.users = [
        NodeUser(node=SchedulerNode(name='op55'), can_inplace=False, is_weak=False),
        NodeUser(node=SchedulerNode(name='op57'), can_inplace=True, is_weak=False),
    ]
]
op54.group.device = cuda:0
op54.group.iteration = (((s0 + 8)//9), 1)
op54.sizes = ([((s0 + 8)//9)], [])
where_34_layout = FixedLayout('cuda:0', torch.int64, size=[((s0 + 8)//9), 1], stride=[1, 1])
buf53_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
buf54_layout = MutationLayoutSHOULDREMOVE('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
class op54_loop_body:
    var_ranges = {p0: ((s0 + 8)//9)}
    index0 = p0
    index1 = indirect0 + 262400*p0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('where_34', get_index)
        set_indirect0 = self.set_indirect0(load)
        constant = ops.constant(-1.0, torch.float32)
        get_index_1 = self.get_index('index1')
        store = ops.store('buf54', get_index_1, constant, None)
        return store


op55: SchedulerNode(ComputedBuffer)
op55.writes = [MemoryDep('buf55', c0, {c0: 5*(((s3 + 8)//9))})]
op55.unmet_dependencies = [MemoryDep('buf54', c0, {c0: 262400*(((s3 + 8)//9))})]
op55.met_dependencies = 
    [   MemoryDep('convert_element_type_45', 0, {}),
        MemoryDep('ne_71', c0, {c0: ((s3 + 8)//9)}),
        MemoryDep('tangents_1', 0, {})]
op55.outputs = [
    buf55: ComputedBuffer
    buf55.layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1, 5], stride=[5, 5*(((s3 + 8)//9)), 1])
    buf55.users = [NodeUser(node=SchedulerNode(name='op56'), can_inplace=False, is_weak=False)]
]
op55.group.device = cuda:0
op55.group.iteration = (5*(((s3 + 8)//9)), 52480)
op55.sizes = ([((s3 + 8)//9), 5], [52480])
buf54_layout = MutationLayoutSHOULDREMOVE('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
ne_71_layout = FixedLayout('cuda:0', torch.bool, size=[((s0 + 8)//9), 1], stride=[1, 1])
tangents_1_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
convert_element_type_45_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
buf55_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1, 5], stride=[5, 5*(((s3 + 8)//9)), 1])
class op55_loop_body:
    var_ranges = {p0: ((s3 + 8)//9), p1: 5, p2: 52480}
    index0 = 262400*p0 + 52480*p1 + p2
    index1 = p0
    index2 = 0
    index3 = 5*p0 + p1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf53', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('ne_71', get_index_1)
        get_index_2 = self.get_index('index2')
        load_2 = ops.load('tangents_1', get_index_2)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('convert_element_type_45', get_index_3)
        truediv = ops.truediv(load_2, load_3)
        constant = ops.constant(0.0, torch.float32)
        where = ops.where(load_1, truediv, constant)
        mul = ops.mul(load, where)
        reduction = ops.reduction(torch.float32, torch.float32, 'sum', mul)
        get_index_4 = self.get_index('index3')
        store_reduction = ops.store_reduction('buf55', get_index_4, reduction)
        return store_reduction


op56: SchedulerNode(ComputedBuffer)
op56.writes = [MemoryDep('buf56', c0, {c0: ((s3 + 8)//9)})]
op56.unmet_dependencies = [MemoryDep('buf55', c0, {c0: 5*(((s3 + 8)//9))})]
op56.met_dependencies = []
op56.outputs = [
    buf56: ComputedBuffer
    buf56.layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, ((s3 + 8)//9)])
    buf56.users = [NodeUser(node=SchedulerNode(name='op57'), can_inplace=False, is_weak=False)]
]
op56.group.device = cuda:0
op56.group.iteration = (((s3 + 8)//9), 5)
op56.sizes = ([((s3 + 8)//9)], [5])
buf55_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1, 5], stride=[5, 5*(((s3 + 8)//9)), 1])
buf56_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, ((s3 + 8)//9)])
class op56_loop_body:
    var_ranges = {p0: ((s3 + 8)//9), p1: 5}
    index0 = 5*p0 + p1
    index1 = p0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf55', get_index)
        reduction = ops.reduction(torch.float32, torch.float32, 'sum', load)
        get_index_1 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf56', get_index_1, reduction)
        return store_reduction


op57: SchedulerNode(ComputedBuffer)
op57.writes = [MemoryDep('buf57', c0, {c0: 262400*(((s3 + 8)//9))})]
op57.unmet_dependencies = 
    [   MemoryDep('buf54', c0, {c0: 262400*(((s3 + 8)//9))}),
        MemoryDep('buf56', c0, {c0: ((s3 + 8)//9)})]
op57.met_dependencies = 
    [   MemoryDep('amax_default_8', c0, {c0: ((s3 + 8)//9)}),
        MemoryDep('convert_element_type_45', 0, {}),
        MemoryDep('log', c0, {c0: ((s3 + 8)//9)}),
        MemoryDep('mm', c0, {c0: 262400*(((s3 + 8)//9))}),
        MemoryDep('ne_71', c0, {c0: ((s3 + 8)//9)}),
        MemoryDep('tangents_1', 0, {})]
op57.outputs = [
    buf57: ComputedBuffer
    buf57.layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
    buf57.users = [NodeUser(node=SchedulerNode(name='op58'), can_inplace=True, is_weak=False)]
]
op57.group.device = cuda:0
op57.group.iteration = (262400*(((s3 + 8)//9)), 1)
op57.sizes = ([((s3 + 8)//9), 262400], [])
buf54_layout = MutationLayoutSHOULDREMOVE('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
ne_71_layout = FixedLayout('cuda:0', torch.bool, size=[((s0 + 8)//9), 1], stride=[1, 1])
tangents_1_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
convert_element_type_45_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
mm_layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
amax_default_8_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, 1])
log_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, 1])
buf56_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 1], stride=[1, ((s3 + 8)//9)])
buf57_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
class op57_loop_body:
    var_ranges = {p0: ((s3 + 8)//9), p1: 262400}
    index0 = 262400*p0 + p1
    index1 = p0
    index2 = 0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf53', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('ne_71', get_index_1)
        get_index_2 = self.get_index('index2')
        load_2 = ops.load('tangents_1', get_index_2)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('convert_element_type_45', get_index_3)
        truediv = ops.truediv(load_2, load_3)
        constant = ops.constant(0.0, torch.float32)
        where = ops.where(load_1, truediv, constant)
        mul = ops.mul(load, where)
        get_index_4 = self.get_index('index0')
        load_4 = ops.load('mm', get_index_4)
        constant_1 = ops.constant(0.03333333333333333, torch.bfloat16)
        mul_1 = ops.mul(load_4, constant_1)
        tanh = ops.tanh(mul_1)
        to_dtype = ops.to_dtype(tanh, torch.float32, src_dtype = torch.bfloat16)
        constant_2 = ops.constant(1.0, torch.float32)
        mul_2 = ops.mul(to_dtype, constant_2)
        get_index_5 = self.get_index('index1')
        load_5 = ops.load('amax_default_8', get_index_5)
        sub = ops.sub(mul_2, load_5)
        constant_3 = ops.constant(30.0, torch.float32)
        mul_3 = ops.mul(sub, constant_3)
        get_index_6 = self.get_index('index1')
        load_6 = ops.load('log', get_index_6)
        sub_1 = ops.sub(mul_3, load_6)
        exp = ops.exp(sub_1)
        get_index_7 = self.get_index('index1')
        load_7 = ops.load('buf56', get_index_7)
        mul_4 = ops.mul(exp, load_7)
        sub_2 = ops.sub(mul, mul_4)
        to_dtype_1 = ops.to_dtype(sub_2, torch.bfloat16, src_dtype = torch.float32)
        constant_4 = ops.constant(30.0, torch.bfloat16)
        mul_5 = ops.mul(to_dtype_1, constant_4)
        to_dtype_2 = ops.to_dtype(mul_5, torch.float32, src_dtype = torch.bfloat16)
        get_index_8 = self.get_index('index0')
        load_8 = ops.load('mm', get_index_8)
        constant_5 = ops.constant(0.03333333333333333, torch.bfloat16)
        mul_6 = ops.mul(load_8, constant_5)
        tanh_1 = ops.tanh(mul_6)
        to_dtype_3 = ops.to_dtype(tanh_1, torch.float32, src_dtype = torch.bfloat16)
        get_index_9 = self.get_index('index0')
        load_9 = ops.load('mm', get_index_9)
        constant_6 = ops.constant(0.03333333333333333, torch.bfloat16)
        mul_7 = ops.mul(load_9, constant_6)
        tanh_2 = ops.tanh(mul_7)
        to_dtype_4 = ops.to_dtype(tanh_2, torch.float32, src_dtype = torch.bfloat16)
        mul_8 = ops.mul(to_dtype_3, to_dtype_4)
        constant_7 = ops.constant(1.0, torch.float32)
        sub_3 = ops.sub(constant_7, mul_8)
        mul_9 = ops.mul(to_dtype_2, sub_3)
        get_index_10 = self.get_index('index0')
        store = ops.store('buf57', get_index_10, mul_9, None)
        return store


op58: SchedulerNode(ComputedBuffer)
op58.writes = [MemoryDep('buf58', c0, {c0: 262400*(((s3 + 8)//9))})]
op58.unmet_dependencies = [MemoryDep('buf57', c0, {c0: 262400*(((s3 + 8)//9))})]
op58.met_dependencies = []
op58.outputs = [
    buf58: ComputedBuffer
    buf58.layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
    buf58.users = [NodeUser(node=ExternKernelSchedulerNode(name='op59'), can_inplace=False, is_weak=False)]
]
op58.group.device = cuda:0
op58.group.iteration = (262400*(((s3 + 8)//9)), 1)
op58.sizes = ([262400*(((s3 + 8)//9))], [])
buf57_layout = FixedLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
buf58_layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 262400], stride=[262400, 1])
class op58_loop_body:
    var_ranges = {p0: 262400*(((s3 + 8)//9))}
    index0 = p0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf57', get_index)
        to_dtype = ops.to_dtype(load, torch.bfloat16, src_dtype = torch.float32)
        constant = ops.constant(0.03333333333333333, torch.bfloat16)
        mul = ops.mul(to_dtype, constant)
        get_index_1 = self.get_index('index0')
        store = ops.store('buf58', get_index_1, mul, None)
        return store


op59: ExternKernelSchedulerNode(ExternKernelOut)
op59.writes = [StarDep(name='buf59', mode=None)]
op59.unmet_dependencies = [StarDep(name='buf58', mode=None)]
op59.met_dependencies = [StarDep(name='permute_10', mode=None)]
op59.outputs = [
    buf59: ExternKernelOut
    buf59.layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 2048], stride=[2048, 1])
    buf59.users = [NodeUser(node=SchedulerNode(name='op60'), can_inplace=False, is_weak=False)]
]
op59.node.kernel = extern_kernels.mm


op60: SchedulerNode(ComputedBuffer)
op60.writes = [MemoryDep('buf60', c0, {c0: 2048*(((s3 + 8)//9))})]
op60.unmet_dependencies = [MemoryDep('buf59', c0, {c0: 2048*(((s3 + 8)//9))})]
op60.met_dependencies = []
op60.outputs = [
    buf60: ComputedBuffer
    buf60.layout = NonOwningLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 2048], stride=[2048, 1])
    buf60.aliases = ['buf69']
    buf60.users = [
        NodeUser(node=NopKernelSchedulerNode(name='op69'), can_inplace=False, is_weak=False),
        NodeUser(node=OUTPUT, can_inplace=False, is_weak=False),
    ]
]
op60.group.device = cuda:0
op60.group.iteration = (2048*(((s3 + 8)//9)), 1)
op60.sizes = ([2048*(((s3 + 8)//9))], [])
buf59_layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 2048], stride=[2048, 1])
buf60_layout = NonOwningLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 2048], stride=[2048, 1])
class op60_loop_body:
    var_ranges = {p0: 2048*(((s3 + 8)//9))}
    index0 = p0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf59', get_index)
        to_dtype = ops.to_dtype(load, torch.float32, src_dtype = torch.bfloat16)
        get_index_1 = self.get_index('index0')
        store = ops.store('buf60', get_index_1, to_dtype, None)
        return store


op61: SchedulerNode(ComputedBuffer)
op61.writes = [MemoryDep('buf61', c0, {c0: 2048*(((s3 + 8)//9))})]
op61.unmet_dependencies = [MemoryDep('buf52', c0, {c0: 2048*(((s3 + 8)//9))})]
op61.met_dependencies = []
op61.outputs = [
    buf61: ComputedBuffer
    buf61.layout = NonOwningLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 2048], stride=[2048, 1])
    buf61.aliases = ['buf69']
    buf61.users = [
        NodeUser(node=NopKernelSchedulerNode(name='op69'), can_inplace=False, is_weak=False),
        NodeUser(node=OUTPUT, can_inplace=False, is_weak=False),
    ]
]
op61.group.device = cuda:0
op61.group.iteration = (2048*(((s3 + 8)//9)), 1)
op61.sizes = ([2048*(((s3 + 8)//9))], [])
buf52_layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 2048], stride=[2048, 1])
buf61_layout = NonOwningLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 2048], stride=[2048, 1])
class op61_loop_body:
    var_ranges = {p0: 2048*(((s3 + 8)//9))}
    index0 = p0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf52', get_index)
        to_dtype = ops.to_dtype(load, torch.float32, src_dtype = torch.bfloat16)
        get_index_1 = self.get_index('index0')
        store = ops.store('buf61', get_index_1, to_dtype, None)
        return store


op62: SchedulerNode(ComputedBuffer)
op62.writes = [MemoryDep('buf62', c0, {c0: 2048*(((s3 + 8)//9))})]
op62.unmet_dependencies = [MemoryDep('buf45', c0, {c0: 2048*(((s3 + 8)//9))})]
op62.met_dependencies = []
op62.outputs = [
    buf62: ComputedBuffer
    buf62.layout = NonOwningLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 2048], stride=[2048, 1])
    buf62.aliases = ['buf69']
    buf62.users = [
        NodeUser(node=NopKernelSchedulerNode(name='op69'), can_inplace=False, is_weak=False),
        NodeUser(node=OUTPUT, can_inplace=False, is_weak=False),
    ]
]
op62.group.device = cuda:0
op62.group.iteration = (2048*(((s3 + 8)//9)), 1)
op62.sizes = ([2048*(((s3 + 8)//9))], [])
buf45_layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 2048], stride=[2048, 1])
buf62_layout = NonOwningLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 2048], stride=[2048, 1])
class op62_loop_body:
    var_ranges = {p0: 2048*(((s3 + 8)//9))}
    index0 = p0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf45', get_index)
        to_dtype = ops.to_dtype(load, torch.float32, src_dtype = torch.bfloat16)
        get_index_1 = self.get_index('index0')
        store = ops.store('buf62', get_index_1, to_dtype, None)
        return store


op63: SchedulerNode(ComputedBuffer)
op63.writes = [MemoryDep('buf63', c0, {c0: 2048*(((s3 + 8)//9))})]
op63.unmet_dependencies = [MemoryDep('buf38', c0, {c0: 2048*(((s3 + 8)//9))})]
op63.met_dependencies = []
op63.outputs = [
    buf63: ComputedBuffer
    buf63.layout = NonOwningLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 2048], stride=[2048, 1])
    buf63.aliases = ['buf69']
    buf63.users = [
        NodeUser(node=NopKernelSchedulerNode(name='op69'), can_inplace=False, is_weak=False),
        NodeUser(node=OUTPUT, can_inplace=False, is_weak=False),
    ]
]
op63.group.device = cuda:0
op63.group.iteration = (2048*(((s3 + 8)//9)), 1)
op63.sizes = ([2048*(((s3 + 8)//9))], [])
buf38_layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 2048], stride=[2048, 1])
buf63_layout = NonOwningLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 2048], stride=[2048, 1])
class op63_loop_body:
    var_ranges = {p0: 2048*(((s3 + 8)//9))}
    index0 = p0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf38', get_index)
        to_dtype = ops.to_dtype(load, torch.float32, src_dtype = torch.bfloat16)
        get_index_1 = self.get_index('index0')
        store = ops.store('buf63', get_index_1, to_dtype, None)
        return store


op64: SchedulerNode(ComputedBuffer)
op64.writes = [MemoryDep('buf64', c0, {c0: 2048*(((s3 + 8)//9))})]
op64.unmet_dependencies = [MemoryDep('buf31', c0, {c0: 2048*(((s3 + 8)//9))})]
op64.met_dependencies = []
op64.outputs = [
    buf64: ComputedBuffer
    buf64.layout = NonOwningLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 2048], stride=[2048, 1])
    buf64.aliases = ['buf69']
    buf64.users = [
        NodeUser(node=NopKernelSchedulerNode(name='op69'), can_inplace=False, is_weak=False),
        NodeUser(node=OUTPUT, can_inplace=False, is_weak=False),
    ]
]
op64.group.device = cuda:0
op64.group.iteration = (2048*(((s3 + 8)//9)), 1)
op64.sizes = ([2048*(((s3 + 8)//9))], [])
buf31_layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 2048], stride=[2048, 1])
buf64_layout = NonOwningLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 2048], stride=[2048, 1])
class op64_loop_body:
    var_ranges = {p0: 2048*(((s3 + 8)//9))}
    index0 = p0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf31', get_index)
        to_dtype = ops.to_dtype(load, torch.float32, src_dtype = torch.bfloat16)
        get_index_1 = self.get_index('index0')
        store = ops.store('buf64', get_index_1, to_dtype, None)
        return store


op65: SchedulerNode(ComputedBuffer)
op65.writes = [MemoryDep('buf65', c0, {c0: 2048*(((s3 + 8)//9))})]
op65.unmet_dependencies = [MemoryDep('buf24', c0, {c0: 2048*(((s3 + 8)//9))})]
op65.met_dependencies = []
op65.outputs = [
    buf65: ComputedBuffer
    buf65.layout = NonOwningLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 2048], stride=[2048, 1])
    buf65.aliases = ['buf69']
    buf65.users = [
        NodeUser(node=NopKernelSchedulerNode(name='op69'), can_inplace=False, is_weak=False),
        NodeUser(node=OUTPUT, can_inplace=False, is_weak=False),
    ]
]
op65.group.device = cuda:0
op65.group.iteration = (2048*(((s3 + 8)//9)), 1)
op65.sizes = ([2048*(((s3 + 8)//9))], [])
buf24_layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 2048], stride=[2048, 1])
buf65_layout = NonOwningLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 2048], stride=[2048, 1])
class op65_loop_body:
    var_ranges = {p0: 2048*(((s3 + 8)//9))}
    index0 = p0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf24', get_index)
        to_dtype = ops.to_dtype(load, torch.float32, src_dtype = torch.bfloat16)
        get_index_1 = self.get_index('index0')
        store = ops.store('buf65', get_index_1, to_dtype, None)
        return store


op66: SchedulerNode(ComputedBuffer)
op66.writes = [MemoryDep('buf66', c0, {c0: 2048*(((s3 + 8)//9))})]
op66.unmet_dependencies = [MemoryDep('buf17', c0, {c0: 2048*(((s3 + 8)//9))})]
op66.met_dependencies = []
op66.outputs = [
    buf66: ComputedBuffer
    buf66.layout = NonOwningLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 2048], stride=[2048, 1])
    buf66.aliases = ['buf69']
    buf66.users = [
        NodeUser(node=NopKernelSchedulerNode(name='op69'), can_inplace=False, is_weak=False),
        NodeUser(node=OUTPUT, can_inplace=False, is_weak=False),
    ]
]
op66.group.device = cuda:0
op66.group.iteration = (2048*(((s3 + 8)//9)), 1)
op66.sizes = ([2048*(((s3 + 8)//9))], [])
buf17_layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 2048], stride=[2048, 1])
buf66_layout = NonOwningLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 2048], stride=[2048, 1])
class op66_loop_body:
    var_ranges = {p0: 2048*(((s3 + 8)//9))}
    index0 = p0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf17', get_index)
        to_dtype = ops.to_dtype(load, torch.float32, src_dtype = torch.bfloat16)
        get_index_1 = self.get_index('index0')
        store = ops.store('buf66', get_index_1, to_dtype, None)
        return store


op67: SchedulerNode(ComputedBuffer)
op67.writes = [MemoryDep('buf67', c0, {c0: 2048*(((s3 + 8)//9))})]
op67.unmet_dependencies = [MemoryDep('buf10', c0, {c0: 2048*(((s3 + 8)//9))})]
op67.met_dependencies = []
op67.outputs = [
    buf67: ComputedBuffer
    buf67.layout = NonOwningLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 2048], stride=[2048, 1])
    buf67.aliases = ['buf69']
    buf67.users = [
        NodeUser(node=NopKernelSchedulerNode(name='op69'), can_inplace=False, is_weak=False),
        NodeUser(node=OUTPUT, can_inplace=False, is_weak=False),
    ]
]
op67.group.device = cuda:0
op67.group.iteration = (2048*(((s3 + 8)//9)), 1)
op67.sizes = ([2048*(((s3 + 8)//9))], [])
buf10_layout = FixedLayout('cuda:0', torch.bfloat16, size=[((s3 + 8)//9), 2048], stride=[2048, 1])
buf67_layout = NonOwningLayout('cuda:0', torch.float32, size=[((s3 + 8)//9), 2048], stride=[2048, 1])
class op67_loop_body:
    var_ranges = {p0: 2048*(((s3 + 8)//9))}
    index0 = p0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf10', get_index)
        to_dtype = ops.to_dtype(load, torch.float32, src_dtype = torch.bfloat16)
        get_index_1 = self.get_index('index0')
        store = ops.store('buf67', get_index_1, to_dtype, None)
        return store


op68: SchedulerNode(ComputedBuffer)
op68.writes = [MemoryDep('buf68', c0, {c0: 2048*s3 - 16384*(((s3 + 8)//9))})]
op68.unmet_dependencies = [MemoryDep('buf3', c0, {c0: 2048*s3 - 16384*(((s3 + 8)//9))})]
op68.met_dependencies = []
op68.outputs = [
    buf68: ComputedBuffer
    buf68.layout = NonOwningLayout('cuda:0', torch.float32, size=[s3 - 8*(((s3 + 8)//9)), 2048], stride=[2048, 1])
    buf68.aliases = ['buf69']
    buf68.users = [
        NodeUser(node=NopKernelSchedulerNode(name='op69'), can_inplace=False, is_weak=False),
        NodeUser(node=OUTPUT, can_inplace=False, is_weak=False),
    ]
]
op68.group.device = cuda:0
op68.group.iteration = (2048*s3 - 16384*(((s3 + 8)//9)), 1)
op68.sizes = ([2048*s3 - 16384*(((s3 + 8)//9))], [])
buf3_layout = FixedLayout('cuda:0', torch.bfloat16, size=[s3 - 8*(((s3 + 8)//9)), 2048], stride=[2048, 1])
buf68_layout = NonOwningLayout('cuda:0', torch.float32, size=[s3 - 8*(((s3 + 8)//9)), 2048], stride=[2048, 1])
class op68_loop_body:
    var_ranges = {p0: 2048*s3 - 16384*(((s3 + 8)//9))}
    index0 = p0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf3', get_index)
        to_dtype = ops.to_dtype(load, torch.float32, src_dtype = torch.bfloat16)
        get_index_1 = self.get_index('index0')
        store = ops.store('buf68', get_index_1, to_dtype, None)
        return store


op69: NopKernelSchedulerNode(ConcatKernel)
op69.writes = [StarDep(name='buf69', mode=None)]
op69.unmet_dependencies = 
    [   StarDep(name='buf60', mode=None),
        StarDep(name='buf61', mode=None),
        StarDep(name='buf62', mode=None),
        StarDep(name='buf63', mode=None),
        StarDep(name='buf64', mode=None),
        StarDep(name='buf65', mode=None),
        StarDep(name='buf66', mode=None),
        StarDep(name='buf67', mode=None),
        StarDep(name='buf68', mode=None)]
op69.met_dependencies = []
op69.outputs = [
    buf69: ConcatKernel
    buf69.layout = FixedLayout('cuda:0', torch.float32, size=[s3, 2048], stride=[2048, 1])
    buf69.users = [
        NodeUser(node=NopKernelSchedulerNode(name='op69'), can_inplace=False, is_weak=False),
        NodeUser(node=OUTPUT, can_inplace=False, is_weak=False),
    ]
]


