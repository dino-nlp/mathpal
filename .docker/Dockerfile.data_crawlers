FROM  public.ecr.aws/lambda/python:3.12 as build

# Install chrome driver and browser
RUN dnf install -y unzip && \
    curl -Lo "/tmp/chromedriver.zip" "https://storage.googleapis.com/chrome-for-testing-public/126.0.6478.126/linux64/chromedriver-linux64.zip" && \
    curl -Lo "/tmp/chrome-linux.zip" "https://storage.googleapis.com/chrome-for-testing-public/126.0.6478.126/linux64/chrome-linux64.zip" && \
    unzip /tmp/chromedriver.zip -d /opt/ && \
    unzip /tmp/chrome-linux.zip -d /opt/

FROM  public.ecr.aws/lambda/python:3.12

ENV POETRY_VERSION=1.8.3

# Install basic dependencies and required libraries for Playwright
RUN dnf install -y \
    wget \
    git \
    nss \
    nspr \
    dbus-libs \
    atk \
    gtk3 \
    libXcomposite \
    libXdamage \
    libXext \
    libXfixes \
    libXrandr \
    libgbm \
    cairo \
    pango \
    alsa-lib


COPY --from=build /opt/chrome-linux64 /opt/chrome
COPY --from=build /opt/chromedriver-linux64 /opt/

COPY ./.docker/requirements-crawler.txt ./requirements.txt

# Install dependencies directly without Poetry to avoid version conflicts
RUN python -m pip install --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt --target "${LAMBDA_TASK_ROOT}" && \
    rm requirements.txt

# Install Playwright browsers only (skip install-deps as it's not compatible with Amazon Linux)
RUN cd "${LAMBDA_TASK_ROOT}" && python -m playwright install chromium

# Set Chrome path for crawl4ai to use the pre-installed Chrome
ENV CHROME_PATH="/opt/chrome/chrome"
ENV CHROME_DRIVER_PATH="/opt/chromedriver-linux64/chromedriver"

# Optional TLS CA only if you plan to store the extracted data into Document DB
RUN wget https://truststore.pki.rds.amazonaws.com/global/global-bundle.pem -P ${LAMBDA_TASK_ROOT}
ENV PYTHONPATH="${LAMBDA_TASK_ROOT}/data_crawling:${LAMBDA_TASK_ROOT}"

# Copy function code
COPY ./src/data_crawling ${LAMBDA_TASK_ROOT}/data_crawling
COPY ./src/core ${LAMBDA_TASK_ROOT}/core

# Set the CMD to your handler (could also be done as a parameter override outside of the Dockerfile)
CMD ["data_crawling.main.handler"]
