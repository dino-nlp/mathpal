# Gemma3N Fine-tuning Requirements
# Optimized for T4 GPU training with Unsloth and Comet ML

# Core ML frameworks
torch>=2.1.0,<2.6.0
transformers>=4.52.4,<4.53.0  # Avoid 4.53.0+ issue with Unsloth
datasets>=2.19.0
accelerate>=0.20.0

# Unsloth and related
unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git
# Alternative for different CUDA/PyTorch versions:
# unsloth[cu118-torch240] @ git+https://github.com/unslothai/unsloth.git
# unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git

# TRL for SFTTrainer
trl>=0.7.0
peft>=0.8.0

# Quantization
bitsandbytes>=0.41.0

# Experiment tracking
comet-ml>=3.35.0

# Data processing
pandas>=1.5.0
numpy>=1.21.0
tokenizers>=0.15.0

# Utilities
tqdm>=4.65.0
psutil>=5.9.0
packaging>=21.0

# Image processing (for multimodal capabilities if needed)
Pillow>=9.0.0

# Development and testing
pytest>=7.0.0
black>=23.0.0
flake8>=6.0.0

# Optional: For better performance
xformers>=0.0.29  # Memory efficient attention
triton>=2.0.0     # For optimized kernels

# Optional: For GGUF conversion
# llama-cpp-python>=0.2.0

# Optional: For advanced optimization
# deepspeed>=0.12.0
