# Gemma3N Fine-tuning for Vietnamese 6th Grade Math Tutoring

Dá»± Ã¡n fine-tuning mÃ´ hÃ¬nh Gemma3N Ä‘á»ƒ táº¡o ra má»™t gia sÆ° toÃ¡n há»c tiáº¿ng Viá»‡t cho há»c sinh lá»›p 6. Sá»­ dá»¥ng Unsloth Ä‘á»ƒ tá»‘i Æ°u hÃ³a tá»‘c Ä‘á»™ training vÃ  Comet ML Ä‘á»ƒ theo dÃµi experiments.

## âœ¨ TÃ­nh nÄƒng chÃ­nh

- ğŸš€ **Tá»‘i Æ°u hÃ³a cho GPU T4**: Cáº¥u hÃ¬nh Ä‘áº·c biá»‡t cho GPU T4 vá»›i 1000 samples
- ğŸ§  **Unsloth Integration**: TÄƒng tá»‘c 2x so vá»›i training thÃ´ng thÆ°á»ng
- ğŸ“Š **Comet ML Tracking**: Theo dÃµi Ä‘áº§y Ä‘á»§ experiments vÃ  model registry
- ğŸ”§ **Highly Configurable**: Dá»… dÃ ng thay Ä‘á»•i hyperparameters
- ğŸ“š **LoRA Fine-tuning**: Memory-efficient vá»›i Parameter-Efficient Fine-Tuning
- ğŸ’¾ **Multiple Save Formats**: Há»— trá»£ LoRA, merged 16bit/4bit, GGUF
- â˜ï¸ **HuggingFace Hub**: Tá»± Ä‘á»™ng push models lÃªn Hub

## ğŸ› ï¸ CÃ i Ä‘áº·t

### 1. Clone repository
```bash
git clone <your-repo-url>
cd mathpal
```

### 2. CÃ i Ä‘áº·t dependencies
```bash
# Táº¡o virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# hoáº·c venv\\Scripts\\activate  # Windows

# CÃ i Ä‘áº·t packages
pip install -r requirements.txt
```

### 3. Setup Comet ML
```bash
# Set environment variables
export COMET_API_KEY="your-api-key"
export COMET_WORKSPACE="your-workspace" 
export COMET_PROJECT="mathpal-gemma3n"

# Hoáº·c táº¡o file .env
echo "COMET_API_KEY=your-api-key" > .env
echo "COMET_WORKSPACE=your-workspace" >> .env
echo "COMET_PROJECT=mathpal-gemma3n" >> .env
```

### 4. Setup HuggingFace (optional)
```bash
export HF_TOKEN="your-hf-token"
```

## ğŸš€ Sá»­ dá»¥ng

### Training cÆ¡ báº£n
```bash
python train_gemma3n.py
```

### Training vá»›i custom config
```bash
python train_gemma3n.py --config custom_config.json
```

### Test nhanh vá»›i Ã­t samples
```bash
python train_gemma3n.py --test-run --max-samples 50
```

### Training cho GPU lá»›n hÆ¡n
```bash
python train_gemma3n.py --gpu-type a100
```

### Push model lÃªn HuggingFace Hub
```bash
python train_gemma3n.py --push-to-hub your-username/gemma3n-math-tutor
```

### Resume tá»« checkpoint
```bash
python train_gemma3n.py --resume-from-checkpoint outputs/checkpoint-100
```

## âš™ï¸ Cáº¥u hÃ¬nh

### Cáº¥u hÃ¬nh máº·c Ä‘á»‹nh cho T4 GPU

ÄÆ°á»£c tá»‘i Æ°u hÃ³a cho:
- GPU T4 (16GB VRAM)
- Dataset ~1000 samples  
- Training time ~2-3 giá»

```python
# Batch settings
per_device_train_batch_size = 1
gradient_accumulation_steps = 8  # Effective batch = 8

# Model settings
max_seq_length = 1536
load_in_4bit = True

# LoRA settings
lora_r = 16
lora_alpha = 16
lora_dropout = 0.0

# Training settings
num_train_epochs = 2
learning_rate = 2e-4
warmup_ratio = 0.1
```

### Customization

Táº¡o file config JSON:
```json
{
  "model": {
    "model_name": "unsloth/gemma-3n-E4B-it",
    "max_seq_length": 2048,
    "lora_r": 32
  },
  "training": {
    "num_train_epochs": 3,
    "learning_rate": 1e-4,
    "per_device_train_batch_size": 2
  },
  "comet": {
    "experiment_name": "my-experiment",
    "tags": ["custom", "experiment"]
  }
}
```

## ğŸ“Š Dataset

Sá»­ dá»¥ng dataset `ngohongthai/exam-sixth_grade-instruct-dataset` vá»›i format:

```json
{
  "question": "TÃ­nh 15 + 23 = ?",
  "solution": "15 + 23 = 38"
}
```

Dataset Ä‘Æ°á»£c convert tá»± Ä‘á»™ng sang Ä‘á»‹nh dáº¡ng Gemma3N conversation:
```json
{
  "conversations": [
    {
      "role": "user", 
      "content": [{"type": "text", "text": "TÃ­nh 15 + 23 = ?"}]
    },
    {
      "role": "assistant",
      "content": [{"type": "text", "text": "15 + 23 = 38"}]
    }
  ]
}
```

## ğŸ“ˆ Monitoring vá»›i Comet ML

### Experiment Tracking
- Training/validation loss
- Learning rate schedule  
- GPU memory usage
- Hyperparameters
- Model artifacts

### Model Registry
- Tá»± Ä‘á»™ng log models Ä‘Ã£ train
- Version management
- Metadata vÃ  metrics
- Easy deployment

### Truy cáº­p experiments
```python
import comet_ml

# Get experiment
api = comet_ml.API()
experiment = api.get_experiment(
    workspace="your-workspace",
    project_name="mathpal-gemma3n", 
    experiment_id="experiment-id"
)

# Download model
experiment.download_model("model-name", "./downloaded_model")
```

## ğŸ’¾ Model Formats

Script há»— trá»£ nhiá»u Ä‘á»‹nh dáº¡ng save:

### LoRA Adapters (máº·c Ä‘á»‹nh)
```bash
python train_gemma3n.py --save-formats lora
```
- Chá»‰ save LoRA weights (~100MB)
- Nhanh vÃ  tiáº¿t kiá»‡m storage
- Requires base model Ä‘á»ƒ inference

### Merged 16-bit
```bash
python train_gemma3n.py --save-formats merged_16bit
```
- Full model precision cao (~14GB)
- Ready cho production
- TÆ°Æ¡ng thÃ­ch vá»›i VLLM

### Merged 4-bit  
```bash
python train_gemma3n.py --save-formats merged_4bit
```
- Quantized model (~4GB)
- CÃ¢n báº±ng size/quality
- PhÃ¹ há»£p cho deployment

### GGUF Format
```bash
python train_gemma3n.py --save-formats gguf_q8_0 gguf_q4_k_m
```
- Cho llama.cpp inference
- Ráº¥t nhá» gá»n
- CPU inference friendly

## ğŸ§ª Testing vÃ  Inference

### Cháº¡y inference test
```bash
python train_gemma3n.py --inference-only
```

### Test vá»›i model Ä‘Ã£ train
```python
from model_manager import create_model_manager
from config import ModelConfig, InferenceConfig

# Load model
config = ModelConfig()
manager = create_model_manager(config)
model, tokenizer = manager.load_base_model()
model = manager.apply_lora()

# Load checkpoint náº¿u cÃ³
# manager.load_adapter("path/to/checkpoint")

# Generate response
response = manager.generate_response(
    question="TÃ­nh 5 Ã— 7 = ?",
    inference_config=InferenceConfig()
)
print(response)
```

## ğŸ”§ Advanced Usage

### Custom LoRA Configuration
```python
from config import ModelConfig

config = ModelConfig()
config.lora_r = 32              # Higher rank for better quality
config.lora_alpha = 32          # Match vá»›i lora_r
config.target_modules = [       # More modules for comprehensive training
    "q_proj", "k_proj", "v_proj", "o_proj",
    "gate_proj", "up_proj", "down_proj",
    "lm_head", "embed_tokens"   # Include output layers
]
```

### Memory Optimization
```python
from config import TrainingConfig

config = TrainingConfig()
config.gradient_accumulation_steps = 16    # Larger effective batch
config.per_device_train_batch_size = 1     # Smaller per-device batch
config.fp16 = True                          # Half precision
config.dataloader_num_workers = 0          # Reduce memory usage
```

### Early Stopping
```python
config.early_stopping_patience = 3
config.load_best_model_at_end = True
config.metric_for_best_model = "eval_loss"
config.eval_strategy = "steps"
config.eval_steps = 50
```

## ğŸ“ Project Structure

```
mathpal/
â”œâ”€â”€ train_gemma3n.py          # Main training script
â”œâ”€â”€ config.py                 # Configuration classes
â”œâ”€â”€ model_manager.py          # Model operations
â”œâ”€â”€ data_processor.py         # Dataset processing  
â”œâ”€â”€ trainer_wrapper.py        # Training with Comet ML
â”œâ”€â”€ requirements.txt          # Dependencies
â”œâ”€â”€ README.md                 # This file
â””â”€â”€ outputs/                  # Training outputs
    â”œâ”€â”€ model_lora/           # LoRA adapters
    â”œâ”€â”€ model_merged_16bit/   # Merged models
    â”œâ”€â”€ checkpoints/          # Training checkpoints
    â””â”€â”€ logs/                 # Training logs
```

## ğŸš¨ Troubleshooting

### GPU Memory Issues
```bash
# Reduce batch size
python train_gemma3n.py --batch-size 1 --max-seq-length 1024

# Use 4-bit quantization
# (ÄÃ£ enable máº·c Ä‘á»‹nh trong T4 config)
```

### Comet ML Connection Issues  
```bash
# Check API key
echo $COMET_API_KEY

# Test connection
python -c "import comet_ml; print('Comet ML OK')"
```

### Unsloth Installation Issues
```bash
# Auto-detect vÃ  install phiÃªn báº£n phÃ¹ há»£p
wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -

# Hoáº·c manual cho CUDA 12.1 + PyTorch 2.4.0
pip install "unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git"
```

### Dataset Loading Issues
```bash
# Test dataset loading
python -c "
from data_processor import test_data_processor
test_data_processor()
"
```

## ğŸ“š TÃ i liá»‡u tham kháº£o

- [Unsloth Documentation](https://github.com/unslothai/unsloth)
- [Comet ML Documentation](https://www.comet.com/docs/)
- [Gemma Model Documentation](https://ai.google.dev/gemma)
- [LoRA Paper](https://arxiv.org/abs/2106.09685)
- [QLoRA Paper](https://arxiv.org/abs/2305.14314)

## ğŸ¤ Contributing

1. Fork repository
2. Táº¡o feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Táº¡o Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Unsloth Team** - Cho framework tá»‘i Æ°u hÃ³a training
- **Comet ML** - Cho platform experiment tracking
- **Google** - Cho Gemma models
- **HuggingFace** - Cho transformers library vÃ  model hub
- **Dataset Contributors** - Cho Vietnamese math dataset

---

**Happy Fine-tuning! ğŸš€**
