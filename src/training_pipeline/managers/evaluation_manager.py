"""Evaluation and testing management."""

import random
from typing import Dict, Any, List, Optional, Tuple
import torch

from ..core.exceptions import InferenceError
from ..config.config_manager import ConfigManager
from ..utils import get_logger

logger = get_logger()


class EvaluationManager:
    """Manages model evaluation and testing."""
    
    def __init__(self, config: ConfigManager):
        self.config = config
        
    def run_evaluation(self, model: Any, tokenizer: Any) -> Dict[str, Any]:
        """
        Run comprehensive model evaluation.
        
        Args:
            model: Trained model
            tokenizer: Tokenizer
            
        Returns:
            Dictionary containing evaluation results
        """
        try:
            logger.info("üß™ Starting model evaluation...")
            
            results = {
                "inference_tests": [],
                "vietnamese_math_tests": [],
                "generation_quality": {},
                "performance_metrics": {}
            }
            
            # Run basic inference tests
            results["inference_tests"] = self._run_inference_tests(model, tokenizer)
            
            # Run Vietnamese math-specific tests
            results["vietnamese_math_tests"] = self._run_vietnamese_math_tests(model, tokenizer)
            
            # Evaluate generation quality
            results["generation_quality"] = self._evaluate_generation_quality(model, tokenizer)
            
            # Performance metrics
            results["performance_metrics"] = self._measure_performance(model, tokenizer)
            
            logger.info("‚úÖ Evaluation completed")
            return results
            
        except Exception as e:
            raise InferenceError(f"Evaluation failed: {e}")
    
    def _run_inference_tests(self, model: Any, tokenizer: Any) -> List[Dict[str, Any]]:
        """Run basic inference tests."""
        try:
            # Prepare model for inference
            if hasattr(model, 'eval'):
                model.eval()
            
            # Enable Unsloth's fast inference if available
            try:
                from unsloth import FastLanguageModel
                FastLanguageModel.for_inference(model)
                logger.info("‚ö° Enabled Unsloth fast inference")
            except:
                pass
            
            test_cases = self._get_test_cases()
            results = []
            
            for i, test_case in enumerate(test_cases[:self.config.inference.num_test_examples]):
                try:
                    logger.info(f"üß™ Running test case {i+1}/{len(test_cases)}")
                    
                    # Generate response
                    response = self._generate_response(
                        model, tokenizer, test_case["input"]
                    )
                    
                    result = {
                        "test_id": i + 1,
                        "input": test_case["input"],
                        "expected": test_case.get("expected", ""),
                        "generated": response,
                        "status": "success"
                    }
                    
                    results.append(result)
                    
                    # Log result
                    logger.info(f"   Input: {test_case['input'][:100]}...")
                    logger.info(f"   Output: {response[:100]}...")
                    
                except Exception as e:
                    results.append({
                        "test_id": i + 1,
                        "input": test_case["input"],
                        "error": str(e),
                        "status": "failed"
                    })
                    logger.error(f"   Test case {i+1} failed: {e}")
            
            return results
            
        except Exception as e:
            raise InferenceError(f"Inference tests failed: {e}")
    
    def _run_vietnamese_math_tests(self, model: Any, tokenizer: Any) -> List[Dict[str, Any]]:
        """Run Vietnamese math-specific tests."""
        vietnamese_tests = [
            {
                "input": "T√≠nh 15 + 27 = ?",
                "expected_type": "arithmetic"
            },
            {
                "input": "M·ªôt h√¨nh ch·ªØ nh·∫≠t c√≥ chi·ªÅu d√†i 8cm v√† chi·ªÅu r·ªông 5cm. T√≠nh chu vi c·ªßa h√¨nh ch·ªØ nh·∫≠t n√†y.",
                "expected_type": "geometry"
            },
            {
                "input": "Lan c√≥ 24 c√°i k·∫πo, Lan cho b·∫°n 8 c√°i. H·ªèi Lan c√≤n l·∫°i bao nhi√™u c√°i k·∫πo?",
                "expected_type": "word_problem"
            },
            {
                "input": "T√¨m x bi·∫øt: 3x + 5 = 14",
                "expected_type": "algebra"
            },
            {
                "input": "Vi·∫øt ph√¢n s·ªë 3/4 d∆∞·ªõi d·∫°ng s·ªë th·∫≠p ph√¢n.",
                "expected_type": "fractions"
            }
        ]
        
        results = []
        for i, test in enumerate(vietnamese_tests):
            try:
                response = self._generate_response(model, tokenizer, test["input"])
                
                # Basic quality checks for Vietnamese math
                quality_score = self._assess_vietnamese_math_quality(
                    test["input"], response, test["expected_type"]
                )
                
                results.append({
                    "test_id": f"vn_math_{i+1}",
                    "input": test["input"],
                    "output": response,
                    "expected_type": test["expected_type"],
                    "quality_score": quality_score,
                    "contains_vietnamese": self._contains_vietnamese(response),
                    "has_mathematical_content": self._has_mathematical_content(response)
                })
                
            except Exception as e:
                results.append({
                    "test_id": f"vn_math_{i+1}",
                    "input": test["input"],
                    "error": str(e),
                    "status": "failed"
                })
        
        return results
    
    def _evaluate_generation_quality(self, model: Any, tokenizer: Any) -> Dict[str, Any]:
        """Evaluate overall generation quality."""
        try:
            # Test different generation parameters
            test_input = "Gi·∫£i th√≠ch c√°ch t√≠nh di·ªán t√≠ch h√¨nh vu√¥ng c√≥ c·∫°nh 6cm."
            
            quality_metrics = {}
            
            # Test with different temperatures
            for temp in [0.1, 0.7, 1.0]:
                response = self._generate_response(
                    model, tokenizer, test_input, temperature=temp
                )
                
                quality_metrics[f"temp_{temp}"] = {
                    "response_length": len(response),
                    "contains_vietnamese": self._contains_vietnamese(response),
                    "has_math_content": self._has_mathematical_content(response),
                    "coherence_score": self._assess_coherence(response)
                }
            
            return quality_metrics
            
        except Exception as e:
            logger.warning(f"Generation quality evaluation failed: {e}")
            return {}
    
    def _measure_performance(self, model: Any, tokenizer: Any) -> Dict[str, Any]:
        """Measure performance metrics."""
        try:
            import time
            import torch
            
            test_input = "T√≠nh 10 + 20 = ?"
            
            # Measure inference time
            start_time = time.time()
            for _ in range(5):  # Average over 5 runs
                _ = self._generate_response(model, tokenizer, test_input)
            avg_time = (time.time() - start_time) / 5
            
            # Memory usage
            memory_used = 0
            if torch.cuda.is_available():
                memory_used = torch.cuda.max_memory_allocated() / 1024**3  # GB
            
            return {
                "avg_inference_time": avg_time,
                "memory_usage_gb": memory_used,
                "model_size": self._estimate_model_size(model)
            }
            
        except Exception as e:
            logger.warning(f"Performance measurement failed: {e}")
            return {}
    
    def _generate_response(self, model: Any, tokenizer: Any, prompt: str, 
                          temperature: Optional[float] = None) -> str:
        """Generate response from model."""
        try:
            # Use generation config from settings
            gen_config = self.config.inference.generation.copy()
            if temperature is not None:
                gen_config["temperature"] = temperature
            
            # Format prompt properly
            formatted_prompt = self._format_prompt(prompt)
            
            # Tokenize input
            inputs = tokenizer(
                formatted_prompt,
                return_tensors="pt",
                truncation=True,
                max_length=self.config.model.max_seq_length - gen_config["max_new_tokens"]
            )
            
            # Move to device if needed
            if hasattr(model, 'device'):
                inputs = {k: v.to(model.device) for k, v in inputs.items()}
            
            # Generate
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=gen_config["max_new_tokens"],
                    temperature=gen_config["temperature"],
                    top_p=gen_config["top_p"],
                    do_sample=gen_config["do_sample"],
                    pad_token_id=tokenizer.eos_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                )
            
            # Decode response
            full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Extract only the new part (after the prompt)
            response = full_response[len(formatted_prompt):].strip()
            
            return response
            
        except Exception as e:
            raise InferenceError(f"Generation failed: {e}")
    
    def _format_prompt(self, prompt: str) -> str:
        """Format prompt according to model's chat template."""
        # Basic Gemma/instruction format
        return f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

B·∫°n l√† m·ªôt tr·ª£ l√Ω gi√°o d·ª•c chuy√™n v·ªÅ to√°n h·ªçc cho h·ªçc sinh l·ªõp 6 t·∫°i Vi·ªát Nam. H√£y gi·∫£i th√≠ch chi ti·∫øt v√† d·ªÖ hi·ªÉu.<|eot_id|><|start_header_id|>user<|end_header_id|>

{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""
    
    def _get_test_cases(self) -> List[Dict[str, str]]:
        """Get test cases for evaluation."""
        return [
            {
                "input": "Xin ch√†o, b·∫°n c√≥ th·ªÉ gi√∫p t√¥i h·ªçc to√°n kh√¥ng?",
                "expected": "greeting_response"
            },
            {
                "input": "T√≠nh 5 + 3 = ?",
                "expected": "8"
            },
            {
                "input": "Gi·∫£i th√≠ch c√°ch nh√¢n hai s·ªë t·ª± nhi√™n.",
                "expected": "explanation"
            },
            {
                "input": "M·ªôt h√¨nh tam gi√°c c√≥ ba c·∫°nh b·∫±ng nhau g·ªçi l√† g√¨?",
                "expected": "tam gi√°c ƒë·ªÅu"
            },
            {
                "input": "Chuy·ªÉn ƒë·ªïi 0.75 th√†nh ph√¢n s·ªë.",
                "expected": "3/4"
            }
        ]
    
    def _assess_vietnamese_math_quality(self, input_text: str, response: str, expected_type: str) -> float:
        """Assess quality of Vietnamese math response."""
        score = 0.0
        
        # Check if response is in Vietnamese
        if self._contains_vietnamese(response):
            score += 0.3
        
        # Check for mathematical content
        if self._has_mathematical_content(response):
            score += 0.3
        
        # Check response length (not too short, not too long)
        if 20 <= len(response) <= 500:
            score += 0.2
        
        # Check for explanation structure
        if any(word in response.lower() for word in ["v√¨", "do", "n√™n", "t√≠nh", "gi·∫£i"]):
            score += 0.2
        
        return min(score, 1.0)
    
    def _contains_vietnamese(self, text: str) -> bool:
        """Check if text contains Vietnamese characters."""
        vietnamese_chars = "√†√°√£·∫°·∫£ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªÉ·ªÖ·ªáƒë√¨√≠ƒ©·ªâ·ªã√≤√≥√µ·ªç·ªè√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫≈©·ª•·ªß∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥·ªµ·ª∑·ªπ√Ω"
        return any(char in text.lower() for char in vietnamese_chars)
    
    def _has_mathematical_content(self, text: str) -> bool:
        """Check if text contains mathematical content."""
        math_indicators = [
            "+", "-", "√ó", "√∑", "=", "%", 
            "t√≠nh", "b·∫±ng", "c·ªông", "tr·ª´", "nh√¢n", "chia",
            "ph√¢n s·ªë", "th·∫≠p ph√¢n", "h√¨nh", "di·ªán t√≠ch", "chu vi",
            "cm", "m", "km", "kg", "g"
        ]
        return any(indicator in text.lower() for indicator in math_indicators)
    
    def _assess_coherence(self, text: str) -> float:
        """Basic coherence assessment."""
        # Simple heuristics for coherence
        if len(text.strip()) < 10:
            return 0.0
        
        sentences = text.split('.')
        if len(sentences) < 2:
            return 0.5
        
        # Check for repetition
        words = text.lower().split()
        unique_words = set(words)
        repetition_ratio = len(unique_words) / len(words) if words else 0
        
        return min(repetition_ratio * 1.2, 1.0)
    
    def _estimate_model_size(self, model: Any) -> str:
        """Estimate model size."""
        try:
            total_params = sum(p.numel() for p in model.parameters())
            size_gb = total_params * 4 / (1024**3)  # Assume fp32
            return f"{size_gb:.2f} GB"
        except:
            return "Unknown"
