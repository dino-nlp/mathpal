"""Evaluation and testing management."""

import random
from typing import Dict, Any, List, Optional, Tuple
import torch

from training_pipeline.utils.exceptions import InferenceError
from training_pipeline.config.config_manager import ConfigManager
from training_pipeline.utils import get_logger
from training_pipeline.inference import InferenceEngine

logger = get_logger()


class EvaluationManager:
    """Manages model evaluation and testing."""
    
    def __init__(self, config: ConfigManager):
        self.config = config
        
    def run_evaluation(self, model: Any, tokenizer: Any) -> Dict[str, Any]:
        """
        Run comprehensive model evaluation.
        
        Args:
            model: Trained model
            tokenizer: Tokenizer
            
        Returns:
            Dictionary containing evaluation results
        """
        try:
            logger.info("ðŸ§ª Starting model evaluation...")
            
            results = {
                "inference_tests": [],
                "vietnamese_math_tests": [],
                "generation_quality": {},
                "performance_metrics": {}
            }
            
            inference_engine = InferenceEngine(model=model, 
                                            tokenizer=tokenizer,
                                            generation_config=self.config.generation,
                                            device="cuda")
            
            # Run basic inference tests
            results["inference_tests"] = self._run_inference_tests(inference_engine)
            
            # Run Vietnamese math-specific tests
            results["vietnamese_math_tests"] = self._run_vietnamese_math_tests(inference_engine)
            
            # Evaluate generation quality
            results["generation_quality"] = self._evaluate_generation_quality(inference_engine)
            
            # Performance metrics
            results["performance_metrics"] = self._measure_performance(inference_engine)
            
            logger.info("âœ… Evaluation completed")
            return results
            
        except Exception as e:
            raise InferenceError(f"Evaluation failed: {e}")
    
    def _run_inference_tests(self, inference_engine) -> List[Dict[str, Any]]:
        """Run basic inference tests."""
        try:
            # Prepare model for inference
            test_cases = self._get_test_cases()
            results = []
            
            for i, test_case in enumerate(test_cases[:self.config.evaluation.num_tc_examples]):
                try:
                    logger.info(f"ðŸ§ª Running test case {i+1}/{len(test_cases)}")
                    response = inference_engine.generate(test_case["input"])
                    result = {
                        "test_id": i + 1,
                        "input": test_case["input"],
                        "expected": test_case.get("expected", ""),
                        "generated": response,
                        "status": "success"
                    }
                    
                    results.append(result)
                    
                    # Log result
                    logger.info(f"   Input: {test_case['input'][:100]}...")
                    logger.info(f"   Output: {response[:100]}...")
                    
                except Exception as e:
                    results.append({
                        "test_id": i + 1,
                        "input": test_case["input"],
                        "error": str(e),
                        "status": "failed"
                    })
                    logger.error(f"   Test case {i+1} failed: {e}")
            
            return results
            
        except Exception as e:
            raise InferenceError(f"Inference tests failed: {e}")
    
    def _run_vietnamese_math_tests(self, inference_engine) -> List[Dict[str, Any]]:
        """Run Vietnamese math-specific tests."""
        vietnamese_tests = [
            {
                "input": "TÃ­nh 15 + 27 = ?",
                "expected_type": "arithmetic"
            },
            {
                "input": "Má»™t hÃ¬nh chá»¯ nháº­t cÃ³ chiá»u dÃ i 8cm vÃ  chiá»u rá»™ng 5cm. TÃ­nh chu vi cá»§a hÃ¬nh chá»¯ nháº­t nÃ y.",
                "expected_type": "geometry"
            },
            {
                "input": "Lan cÃ³ 24 cÃ¡i káº¹o, Lan cho báº¡n 8 cÃ¡i. Há»i Lan cÃ²n láº¡i bao nhiÃªu cÃ¡i káº¹o?",
                "expected_type": "word_problem"
            },
            {
                "input": "TÃ¬m x biáº¿t: 3x + 5 = 14",
                "expected_type": "algebra"
            },
            {
                "input": "Viáº¿t phÃ¢n sá»‘ 3/4 dÆ°á»›i dáº¡ng sá»‘ tháº­p phÃ¢n.",
                "expected_type": "fractions"
            }
        ]
        
        results = []
        for i, test in enumerate(vietnamese_tests):
            try:
                response = inference_engine.generate(test["input"])
                
                # Basic quality checks for Vietnamese math
                quality_score = self._assess_vietnamese_math_quality(
                    test["input"], response, test["expected_type"]
                )
                
                results.append({
                    "test_id": f"vn_math_{i+1}",
                    "input": test["input"],
                    "output": response,
                    "expected_type": test["expected_type"],
                    "quality_score": quality_score,
                    "contains_vietnamese": self._contains_vietnamese(response),
                    "has_mathematical_content": self._has_mathematical_content(response)
                })
                
            except Exception as e:
                results.append({
                    "test_id": f"vn_math_{i+1}",
                    "input": test["input"],
                    "error": str(e),
                    "status": "failed"
                })
        
        return results
    
    def _evaluate_generation_quality(self, inference_engine) -> Dict[str, Any]:
        """Evaluate overall generation quality."""
        try:
            # Test different generation parameters
            test_input = "Giáº£i thÃ­ch cÃ¡ch tÃ­nh diá»‡n tÃ­ch hÃ¬nh vuÃ´ng cÃ³ cáº¡nh 6cm."
            
            quality_metrics = {}
            
            # Test with different temperatures
            for temp in [0.1, 0.7, 1.0]:
                response = inference_engine.generate(question=test_input,
                                                    generation_config={"temperature": temp})
                
                quality_metrics[f"temp_{temp}"] = {
                    "response_length": len(response),
                    "contains_vietnamese": self._contains_vietnamese(response),
                    "has_math_content": self._has_mathematical_content(response),
                    "coherence_score": self._assess_coherence(response)
                }
            
            return quality_metrics
            
        except Exception as e:
            logger.warning(f"Generation quality evaluation failed: {e}")
            return {}
    
    def _measure_performance(self, inference_engine) -> Dict[str, Any]:
        """Measure performance metrics."""
        try:
            import time
            import torch
            
            test_input = "TÃ­nh 10 + 20 = ?"
            
            # Measure inference time
            start_time = time.time()
            for _ in range(5):  # Average over 5 runs
                _ = inference_engine.generate(test_input)
            avg_time = (time.time() - start_time) / 5
            
            # Memory usage
            memory_used = 0
            if torch.cuda.is_available():
                memory_used = torch.cuda.max_memory_allocated() / 1024**3  # GB
            
            return {
                "avg_inference_time": avg_time,
                "memory_usage_gb": memory_used,
                "model_size": self._estimate_model_size(model)
            }
            
        except Exception as e:
            logger.warning(f"Performance measurement failed: {e}")
            return {}
    

    def _get_test_cases(self) -> List[Dict[str, str]]:
        """Get test cases for evaluation."""
        return [
            {
                "input": "Xin chÃ o, báº¡n cÃ³ thá»ƒ giÃºp tÃ´i há»c toÃ¡n khÃ´ng?",
                "expected": "greeting_response"
            },
            {
                "input": "TÃ­nh 5 + 3 = ?",
                "expected": "8"
            },
            {
                "input": "Giáº£i thÃ­ch cÃ¡ch nhÃ¢n hai sá»‘ tá»± nhiÃªn.",
                "expected": "explanation"
            },
            {
                "input": "Má»™t hÃ¬nh tam giÃ¡c cÃ³ ba cáº¡nh báº±ng nhau gá»i lÃ  gÃ¬?",
                "expected": "tam giÃ¡c Ä‘á»u"
            },
            {
                "input": "Chuyá»ƒn Ä‘á»•i 0.75 thÃ nh phÃ¢n sá»‘.",
                "expected": "3/4"
            }
        ]
    
    def _assess_vietnamese_math_quality(self, input_text: str, response: str, expected_type: str) -> float:
        """Assess quality of Vietnamese math response."""
        score = 0.0
        
        # Check if response is in Vietnamese
        if self._contains_vietnamese(response):
            score += 0.3
        
        # Check for mathematical content
        if self._has_mathematical_content(response):
            score += 0.3
        
        # Check response length (not too short, not too long)
        if 20 <= len(response) <= 500:
            score += 0.2
        
        # Check for explanation structure
        if any(word in response.lower() for word in ["vÃ¬", "do", "nÃªn", "tÃ­nh", "giáº£i"]):
            score += 0.2
        
        return min(score, 1.0)
    
    def _contains_vietnamese(self, text: str) -> bool:
        """Check if text contains Vietnamese characters."""
        vietnamese_chars = "Ã Ã¡Ã£áº¡áº£Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã¨Ã©áº¹áº»áº½Ãªá»áº¿á»ƒá»…á»‡Ä‘Ã¬Ã­Ä©á»‰á»‹Ã²Ã³Ãµá»á»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã¹ÃºÅ©á»¥á»§Æ°á»©á»«á»­á»¯á»±á»³á»µá»·á»¹Ã½"
        return any(char in text.lower() for char in vietnamese_chars)
    
    def _has_mathematical_content(self, text: str) -> bool:
        """Check if text contains mathematical content."""
        math_indicators = [
            "+", "-", "Ã—", "Ã·", "=", "%", 
            "tÃ­nh", "báº±ng", "cá»™ng", "trá»«", "nhÃ¢n", "chia",
            "phÃ¢n sá»‘", "tháº­p phÃ¢n", "hÃ¬nh", "diá»‡n tÃ­ch", "chu vi",
            "cm", "m", "km", "kg", "g"
        ]
        return any(indicator in text.lower() for indicator in math_indicators)
    
    def _assess_coherence(self, text: str) -> float:
        """Basic coherence assessment."""
        # Simple heuristics for coherence
        if len(text.strip()) < 10:
            return 0.0
        
        sentences = text.split('.')
        if len(sentences) < 2:
            return 0.5
        
        # Check for repetition
        words = text.lower().split()
        unique_words = set(words)
        repetition_ratio = len(unique_words) / len(words) if words else 0
        
        return min(repetition_ratio * 1.2, 1.0)
    
    def _estimate_model_size(self, model: Any) -> str:
        """Estimate model size."""
        try:
            total_params = sum(p.numel() for p in model.parameters())
            size_gb = total_params * 4 / (1024**3)  # Assume fp32
            return f"{size_gb:.2f} GB"
        except:
            return "Unknown"
