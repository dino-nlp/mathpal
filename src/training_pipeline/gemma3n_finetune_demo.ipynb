{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸš€ Gemma3N Fine-tuning Demo\n",
        "## Vietnamese 6th Grade Math Tutoring vá»›i Unsloth + Comet ML\n",
        "\n",
        "**Notebook nÃ y demo há»‡ thá»‘ng fine-tuning Gemma3N hoÃ n chá»‰nh:**\n",
        "- âœ… Tá»‘i Æ°u cho Google Colab (GPU T4)\n",
        "- âœ… TÃ­ch há»£p Comet ML cho experiment tracking\n",
        "- âœ… Sá»­ dá»¥ng Unsloth Ä‘á»ƒ tÄƒng tá»‘c 2x\n",
        "- âœ… LoRA fine-tuning memory-efficient\n",
        "- âœ… Model registry tá»± Ä‘á»™ng\n",
        "- âœ… Multiple save formats\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“¦ Setup & Installation\n",
        "\n",
        "**CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t cho Google Colab:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install dependencies for Google Colab\n",
        "import os\n",
        "\n",
        "# Check if running in Colab\n",
        "if \"COLAB_\" in \"\".join(os.environ.keys()):\n",
        "    print(\"ðŸ”§ Installing for Google Colab...\")\n",
        "    \n",
        "    # Install Unsloth for Colab\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "    \n",
        "    # Install transformers (specific version for Unsloth compatibility)\n",
        "    !pip install --no-deps --upgrade timm\n",
        "    !pip install \"transformers>=4.52.4,<4.53.0\"\n",
        "    \n",
        "    # Install Comet ML\n",
        "    !pip install comet-ml\n",
        "    \n",
        "    # Other dependencies\n",
        "    !pip install pandas numpy tqdm psutil packaging Pillow\n",
        "    \n",
        "else:\n",
        "    print(\"ðŸ”§ Installing for local environment...\")\n",
        "    !pip install unsloth comet-ml transformers datasets trl peft bitsandbytes\n",
        "\n",
        "print(\"âœ… Installation completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“š Import Libraries & Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import io\n",
        "import json\n",
        "import time\n",
        "import warnings\n",
        "from typing import Dict, List, Any, Optional, Tuple\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "# ML libraries\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Transformers & Training\n",
        "from transformers import TrainingArguments, EarlyStoppingCallback\n",
        "from datasets import Dataset, load_dataset\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "# Unsloth\n",
        "from unsloth import FastModel, get_chat_template\n",
        "from unsloth.chat_templates import train_on_responses_only\n",
        "\n",
        "# Comet ML\n",
        "import comet_ml\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "print(\"ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\")\n",
        "print(\"ðŸ“Š Comet ML: Experiment tracking ready.\")\n",
        "print(\"âœ… All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âš™ï¸ Configuration Classes\n",
        "\n",
        "**Äá»‹nh nghÄ©a cÃ¡c class cáº¥u hÃ¬nh tá»‘i Æ°u cho Colab:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class CometConfig:\n",
        "    \"\"\"Comet ML configuration vá»›i tÃ­nh nÄƒng model registry\"\"\"\n",
        "    # API Keys - sáº½ Ä‘Æ°á»£c set tá»« Colab secrets\n",
        "    api_key: Optional[str] = None\n",
        "    workspace: Optional[str] = None\n",
        "    project: Optional[str] = None\n",
        "    \n",
        "    # Experiment settings\n",
        "    experiment_name: str = \"gemma3n-math-tutor-colab-demo\"\n",
        "    tags: List[str] = field(default_factory=lambda: [\n",
        "        \"gemma3n\", \"math-tutor\", \"vietnamese\", \"sixth-grade\", \n",
        "        \"fine-tuning\", \"unsloth\", \"colab-demo\"\n",
        "    ])\n",
        "    \n",
        "    # Logging settings\n",
        "    auto_metric_logging: bool = True\n",
        "    auto_param_logging: bool = True\n",
        "    auto_histogram_weight_logging: bool = True\n",
        "    auto_histogram_gradient_logging: bool = True\n",
        "    auto_histogram_activation_logging: bool = False\n",
        "    \n",
        "    # Model Registry\n",
        "    log_model: bool = True\n",
        "    model_registry_name: str = \"gemma3n-math-tutor-colab\"\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    \"\"\"Model configuration tá»‘i Æ°u cho Colab T4\"\"\"\n",
        "    # Base model\n",
        "    model_name: str = \"unsloth/gemma-3n-E4B-it\"\n",
        "    max_seq_length: int = 1536  # Optimized for T4\n",
        "    load_in_4bit: bool = True\n",
        "    full_finetuning: bool = False\n",
        "    \n",
        "    # LoRA settings\n",
        "    lora_r: int = 16\n",
        "    lora_alpha: int = 16\n",
        "    lora_dropout: float = 0.0\n",
        "    lora_bias: str = \"none\"\n",
        "    \n",
        "    # Target modules\n",
        "    target_modules: List[str] = field(default_factory=lambda: [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "    ])\n",
        "    \n",
        "    # Memory optimization\n",
        "    use_gradient_checkpointing: str = \"unsloth\"\n",
        "    random_state: int = 42\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    \"\"\"Training configuration cho Colab demo\"\"\"\n",
        "    # Output\n",
        "    output_dir: str = \"outputs/gemma3n-math-colab\"\n",
        "    run_name: str = \"gemma3n-colab-demo\"\n",
        "    \n",
        "    # Training schedule\n",
        "    num_train_epochs: int = 2\n",
        "    max_steps: int = -1\n",
        "    eval_strategy: str = \"steps\"\n",
        "    eval_steps: int = 25\n",
        "    \n",
        "    # Batch settings - optimized for T4\n",
        "    per_device_train_batch_size: int = 1\n",
        "    per_device_eval_batch_size: int = 1\n",
        "    gradient_accumulation_steps: int = 8\n",
        "    \n",
        "    # Learning rate\n",
        "    learning_rate: float = 2e-4\n",
        "    warmup_ratio: float = 0.1\n",
        "    lr_scheduler_type: str = \"cosine\"\n",
        "    \n",
        "    # Optimization\n",
        "    optim: str = \"adamw_8bit\"\n",
        "    weight_decay: float = 0.01\n",
        "    max_grad_norm: float = 1.0\n",
        "    \n",
        "    # Precision\n",
        "    fp16: bool = True\n",
        "    bf16: bool = False\n",
        "    \n",
        "    # Logging & Saving\n",
        "    logging_steps: int = 5\n",
        "    save_strategy: str = \"steps\"\n",
        "    save_steps: int = 50\n",
        "    save_total_limit: int = 2\n",
        "    \n",
        "    # Early stopping\n",
        "    load_best_model_at_end: bool = True\n",
        "    metric_for_best_model: str = \"eval_loss\"\n",
        "    greater_is_better: bool = False\n",
        "    early_stopping_patience: int = 3\n",
        "    \n",
        "    # Memory\n",
        "    remove_unused_columns: bool = True\n",
        "    dataloader_num_workers: int = 2\n",
        "    dataloader_pin_memory: bool = True\n",
        "    \n",
        "    # Reproducibility\n",
        "    seed: int = 42\n",
        "    data_seed: int = 42\n",
        "    \n",
        "    # Report\n",
        "    report_to: str = \"comet_ml\"\n",
        "\n",
        "@dataclass\n",
        "class DatasetConfig:\n",
        "    \"\"\"Dataset configuration\"\"\"\n",
        "    dataset_name: str = \"ngohongthai/exam-sixth_grade-instruct-dataset\"\n",
        "    train_split: str = \"train\"\n",
        "    test_split: str = \"test\"\n",
        "    max_samples: Optional[int] = 200  # Limit for demo\n",
        "    dataset_text_field: str = \"text\"\n",
        "    dataset_num_proc: int = 2\n",
        "\n",
        "@dataclass\n",
        "class DemoConfig:\n",
        "    \"\"\"Complete demo configuration\"\"\"\n",
        "    comet: CometConfig = field(default_factory=CometConfig)\n",
        "    model: ModelConfig = field(default_factory=ModelConfig)\n",
        "    training: TrainingConfig = field(default_factory=TrainingConfig)\n",
        "    dataset: DatasetConfig = field(default_factory=DatasetConfig)\n",
        "\n",
        "print(\"âœ… Configuration classes defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ” Setup Comet ML Credentials\n",
        "\n",
        "**Äá»ƒ cháº¡y demo nÃ y, báº¡n cáº§n:**\n",
        "1. Táº¡o tÃ i khoáº£n táº¡i [Comet ML](https://www.comet.com/signup)\n",
        "2. Táº¡o workspace vÃ  project\n",
        "3. Láº¥y API key tá»« Settings\n",
        "4. Add vÃ o Colab Secrets (ðŸ”‘ icon bÃªn trÃ¡i):\n",
        "   - `COMET_API_KEY`: Your API key\n",
        "   - `COMET_WORKSPACE`: Your workspace name\n",
        "   - `COMET_PROJECT`: Your project name (optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup Comet ML credentials tá»« Colab secrets\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    \n",
        "    # Load tá»« Colab secrets\n",
        "    COMET_API_KEY = userdata.get('COMET_API_KEY')\n",
        "    COMET_WORKSPACE = userdata.get('COMET_WORKSPACE') \n",
        "    COMET_PROJECT = userdata.get('COMET_PROJECT', 'mathpal-gemma3n-demo')\n",
        "    \n",
        "    # Set environment variables\n",
        "    os.environ['COMET_API_KEY'] = COMET_API_KEY\n",
        "    os.environ['COMET_WORKSPACE'] = COMET_WORKSPACE\n",
        "    os.environ['COMET_PROJECT'] = COMET_PROJECT\n",
        "    \n",
        "    print(\"âœ… Comet ML credentials loaded from Colab secrets\")\n",
        "    print(f\"   Workspace: {COMET_WORKSPACE}\")\n",
        "    print(f\"   Project: {COMET_PROJECT}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(\"âš ï¸ Could not load Comet ML credentials from secrets\")\n",
        "    print(\"   Please add COMET_API_KEY, COMET_WORKSPACE to Colab secrets\")\n",
        "    print(\"   Or set them manually below:\")\n",
        "    \n",
        "    # Manual setup (uncomment and fill in)\n",
        "    # os.environ['COMET_API_KEY'] = \"your-api-key-here\"\n",
        "    # os.environ['COMET_WORKSPACE'] = \"your-workspace-here\"\n",
        "    # os.environ['COMET_PROJECT'] = \"mathpal-gemma3n-demo\"\n",
        "    \n",
        "    COMET_API_KEY = os.getenv('COMET_API_KEY')\n",
        "    COMET_WORKSPACE = os.getenv('COMET_WORKSPACE')\n",
        "    COMET_PROJECT = os.getenv('COMET_PROJECT', 'mathpal-gemma3n-demo')\n",
        "\n",
        "# Optional: HuggingFace token for model upload\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "    if HF_TOKEN:\n",
        "        os.environ['HF_TOKEN'] = HF_TOKEN\n",
        "        print(\"âœ… HuggingFace token loaded\")\n",
        "except:\n",
        "    print(\"â„¹ï¸ HuggingFace token not found (optional)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”§ Create & Display Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create configuration\n",
        "config = DemoConfig()\n",
        "\n",
        "# Update Comet ML settings\n",
        "config.comet.api_key = COMET_API_KEY\n",
        "config.comet.workspace = COMET_WORKSPACE\n",
        "config.comet.project = COMET_PROJECT\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(config.training.output_dir, exist_ok=True)\n",
        "\n",
        "def print_config_summary(config: DemoConfig):\n",
        "    \"\"\"Print configuration summary\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"ðŸ”§ GEMMA3N DEMO CONFIGURATION\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    print(f\"ðŸ“Š Experiment: {config.comet.experiment_name}\")\n",
        "    print(f\"ðŸ¤– Model: {config.model.model_name}\")\n",
        "    print(f\"ðŸ“š Dataset: {config.dataset.dataset_name}\")\n",
        "    print(f\"ðŸ’¾ Output: {config.training.output_dir}\")\n",
        "    \n",
        "    print(f\"\\nâš™ï¸ Training Settings:\")\n",
        "    print(f\"   Epochs: {config.training.num_train_epochs}\")\n",
        "    print(f\"   Batch Size: {config.training.per_device_train_batch_size}\")\n",
        "    print(f\"   Gradient Accumulation: {config.training.gradient_accumulation_steps}\")\n",
        "    print(f\"   Effective Batch Size: {config.training.per_device_train_batch_size * config.training.gradient_accumulation_steps}\")\n",
        "    print(f\"   Learning Rate: {config.training.learning_rate}\")\n",
        "    print(f\"   Max Samples: {config.dataset.max_samples}\")\n",
        "    \n",
        "    print(f\"\\nðŸ§¬ LoRA Settings:\")\n",
        "    print(f\"   Rank (r): {config.model.lora_r}\")\n",
        "    print(f\"   Alpha: {config.model.lora_alpha}\")\n",
        "    print(f\"   Dropout: {config.model.lora_dropout}\")\n",
        "    \n",
        "    print(f\"\\nâ˜ï¸ Comet ML:\")\n",
        "    print(f\"   Workspace: {config.comet.workspace}\")\n",
        "    print(f\"   Project: {config.comet.project}\")\n",
        "    print(f\"   API Key: {'âœ… Set' if config.comet.api_key else 'âŒ Not set'}\")\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "\n",
        "print_config_summary(config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ¯ Complete Training Pipeline\n",
        "\n",
        "**Function cháº¡y toÃ n bá»™ pipeline tá»« setup Ä‘áº¿n training:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_complete_pipeline(config: DemoConfig):\n",
        "    \"\"\"Run complete training pipeline\"\"\"\n",
        "    print(\"ðŸŽ¯ Starting Complete Gemma3N Fine-tuning Pipeline\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Step 1: Setup Comet ML\n",
        "    def setup_comet_experiment(config: DemoConfig):\n",
        "        if not config.comet.api_key:\n",
        "            print(\"âš ï¸ Comet ML API key not set, skipping experiment tracking\")\n",
        "            return None\n",
        "        \n",
        "        try:\n",
        "            print(\"ðŸ“Š Setting up Comet ML experiment...\")\n",
        "            experiment = comet_ml.Experiment(\n",
        "                api_key=config.comet.api_key,\n",
        "                workspace=config.comet.workspace,\n",
        "                project_name=config.comet.project,\n",
        "                experiment_name=config.comet.experiment_name,\n",
        "                auto_metric_logging=config.comet.auto_metric_logging,\n",
        "                auto_param_logging=config.comet.auto_param_logging,\n",
        "            )\n",
        "            \n",
        "            # Add tags\n",
        "            for tag in config.comet.tags:\n",
        "                experiment.add_tag(tag)\n",
        "            \n",
        "            # Log configuration\n",
        "            experiment.log_parameter(\"model_name\", config.model.model_name)\n",
        "            experiment.log_parameter(\"max_seq_length\", config.model.max_seq_length)\n",
        "            experiment.log_parameter(\"lora_r\", config.model.lora_r)\n",
        "            experiment.log_parameter(\"learning_rate\", config.training.learning_rate)\n",
        "            experiment.log_parameter(\"batch_size\", config.training.per_device_train_batch_size)\n",
        "            experiment.log_parameter(\"num_epochs\", config.training.num_train_epochs)\n",
        "            experiment.log_parameter(\"max_samples\", config.dataset.max_samples)\n",
        "            \n",
        "            # Set environment variables for transformers integration\n",
        "            os.environ[\"COMET_PROJECT_NAME\"] = config.comet.project\n",
        "            if config.comet.workspace:\n",
        "                os.environ[\"COMET_WORKSPACE\"] = config.comet.workspace\n",
        "            \n",
        "            print(f\"âœ… Comet ML experiment initialized\")\n",
        "            print(f\"ðŸ”— Experiment URL: {experiment.url}\")\n",
        "            return experiment\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Failed to setup Comet ML: {e}\")\n",
        "            return None\n",
        "\n",
        "    experiment = setup_comet_experiment(config)\n",
        "    \n",
        "    # Step 2: Setup Model\n",
        "    print(\"\\nðŸ¤– STEP 2: Setting up model...\")\n",
        "    model, tokenizer = FastModel.from_pretrained(\n",
        "        model_name=config.model.model_name,\n",
        "        max_seq_length=config.model.max_seq_length,\n",
        "        load_in_4bit=config.model.load_in_4bit,\n",
        "        full_finetuning=config.model.full_finetuning,\n",
        "    )\n",
        "    \n",
        "    # Setup chat template\n",
        "    tokenizer = get_chat_template(tokenizer, \"gemma-3n\")\n",
        "    \n",
        "    # Apply LoRA\n",
        "    model = FastModel.get_peft_model(\n",
        "        model,\n",
        "        finetune_vision_layers=False,\n",
        "        finetune_language_layers=True,\n",
        "        finetune_attention_modules=True,\n",
        "        finetune_mlp_modules=True,\n",
        "        r=config.model.lora_r,\n",
        "        lora_alpha=config.model.lora_alpha,\n",
        "        lora_dropout=config.model.lora_dropout,\n",
        "        bias=config.model.lora_bias,\n",
        "        target_modules=config.model.target_modules,\n",
        "        use_gradient_checkpointing=config.model.use_gradient_checkpointing,\n",
        "        random_state=config.model.random_state,\n",
        "    )\n",
        "    \n",
        "    # Prepare for training\n",
        "    FastModel.for_training(model)\n",
        "    print(\"âœ… Model setup complete\")\n",
        "    \n",
        "    # Step 3: Prepare Data\n",
        "    print(\"\\nðŸ“š STEP 3: Preparing datasets...\")\n",
        "    \n",
        "    def process_sample(sample: Dict[str, str]) -> Dict[str, List[Dict[str, Any]]]:\n",
        "        \"\"\"Convert sample to Gemma3N conversation format\"\"\"\n",
        "        conversations = [\n",
        "            {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": sample[\"question\"]}]},\n",
        "            {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": sample[\"solution\"]}]}\n",
        "        ]\n",
        "        return {\"conversations\": conversations}\n",
        "    \n",
        "    raw_dataset = load_dataset(config.dataset.dataset_name)\n",
        "    \n",
        "    # Limit samples for demo\n",
        "    train_dataset = raw_dataset[config.dataset.train_split]\n",
        "    if config.dataset.max_samples and len(train_dataset) > config.dataset.max_samples:\n",
        "        train_dataset = train_dataset.select(range(config.dataset.max_samples))\n",
        "    \n",
        "    # Process dataset\n",
        "    train_dataset = train_dataset.map(process_sample, desc=\"Converting to conversations\")\n",
        "    \n",
        "    # Apply chat template\n",
        "    def format_conversations(examples):\n",
        "        convos = examples[\"conversations\"]\n",
        "        texts = []\n",
        "        for convo in convos:\n",
        "            try:\n",
        "                formatted_text = tokenizer.apply_chat_template(\n",
        "                    convo, tokenize=False, add_generation_prompt=False\n",
        "                ).removeprefix('<bos>')\n",
        "                texts.append(formatted_text)\n",
        "            except:\n",
        "                # Fallback format\n",
        "                user_text = convo[0][\"content\"][0][\"text\"]\n",
        "                assistant_text = convo[1][\"content\"][0][\"text\"]\n",
        "                fallback = f\"<start_of_turn>user\\\\n{user_text}<end_of_turn>\\\\n<start_of_turn>model\\\\n{assistant_text}<end_of_turn>\"\n",
        "                texts.append(fallback)\n",
        "        return {\"text\": texts}\n",
        "    \n",
        "    train_dataset = train_dataset.map(format_conversations, batched=True, desc=\"Formatting conversations\")\n",
        "    \n",
        "    # Create eval split\n",
        "    train_test = train_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "    train_dataset = train_test[\"train\"]\n",
        "    eval_dataset = train_test[\"test\"]\n",
        "    \n",
        "    print(f\"âœ… Datasets prepared: Train={len(train_dataset)}, Eval={len(eval_dataset)}\")\n",
        "    \n",
        "    # Show sample\n",
        "    print(\"\\\\nðŸ“– Sample formatted data:\")\n",
        "    print(train_dataset[0]['text'][:300] + \"...\")\n",
        "    \n",
        "    return model, tokenizer, train_dataset, eval_dataset, experiment\n",
        "\n",
        "print(\"âœ… Pipeline setup function defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
