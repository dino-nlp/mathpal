# MathPal Evaluation Pipeline

H·ªá th·ªëng evaluation cho MathPal v·ªõi progress tracking v√† c√°c metrics t√πy ch·ªânh.

## üöÄ T√≠nh nƒÉng m·ªõi

### Progress Tracking
- **Progress Bar**: Hi·ªÉn th·ªã ti·∫øn tr√¨nh evaluation v·ªõi tqdm
- **Real-time Updates**: C·∫≠p nh·∫≠t th·ªùi gian th·ª±c v·ªÅ ti·∫øn ƒë·ªô x·ª≠ l√Ω
- **Detailed Logging**: Log chi ti·∫øt cho t·ª´ng sample v√† metric
- **Performance Metrics**: Th·ªëng k√™ th·ªùi gian x·ª≠ l√Ω v√† hi·ªáu su·∫•t

### Custom Metrics v·ªõi Progress Tracking
- **ProgressLevenshteinRatio**: T√≠nh to√°n ƒë·ªô t∆∞∆°ng ƒë·ªìng v·ªõi progress tracking
- **ProgressHallucination**: Ph√°t hi·ªán hallucination v·ªõi progress tracking  
- **ProgressModeration**: Ki·ªÉm duy·ªát n·ªôi dung v·ªõi progress tracking

## üìä C√°ch s·ª≠ d·ª•ng

### 1. Evaluation c∆° b·∫£n
```bash
# Evaluation ti√™u chu·∫©n
make evaluate-llm

# Evaluation v·ªõi progress tracking
make evaluate-llm-progress

# Evaluation nhanh (5 samples) v·ªõi progress tracking
make evaluate-llm-quick

# Evaluation nhanh kh√¥ng c√≥ progress tracking
make evaluate-llm-fast
```

### 2. Evaluation t√πy ch·ªânh
```bash
# Evaluation v·ªõi s·ªë l∆∞·ª£ng samples t√πy ch·ªânh
make evaluate-llm-custom SAMPLES=10 EXPERIMENT="My Custom Test"

# Ho·∫∑c ch·∫°y tr·ª±c ti·∫øp
cd src/inference_pipeline && python -m evaluation.evaluate \
    --max_samples 10 \
    --experiment_name "My Custom Test" \
    --use_progress_metrics
```

### 3. C√°c t√πy ch·ªçn command line
```bash
python -m evaluation.evaluate --help
```

**C√°c t√πy ch·ªçn c√≥ s·∫µn:**
- `--dataset_name`: T√™n dataset (m·∫∑c ƒë·ªãnh: "mathpal-testset")
- `--max_samples`: S·ªë l∆∞·ª£ng samples t·ªëi ƒëa ƒë·ªÉ evaluate
- `--experiment_name`: T√™n experiment
- `--use_progress_metrics`: S·ª≠ d·ª•ng custom progress metrics
- `--no_progress_tracking`: T·∫Øt progress tracking (nhanh h∆°n)

## üîß C·∫•u tr√∫c code

### EvaluationProgressCallback
Class callback ƒë·ªÉ theo d√µi ti·∫øn tr√¨nh evaluation:

```python
class EvaluationProgressCallback:
    def on_evaluation_start(self):
        # Kh·ªüi t·∫°o progress bar
    
    def on_sample_start(self, sample_idx, sample_data):
        # C·∫≠p nh·∫≠t khi b·∫Øt ƒë·∫ßu x·ª≠ l√Ω sample
    
    def on_sample_complete(self, sample_idx, result):
        # C·∫≠p nh·∫≠t khi ho√†n th√†nh sample
    
    def on_evaluation_complete(self, results):
        # Hi·ªÉn th·ªã k·∫øt qu·∫£ cu·ªëi c√πng
```

### ProgressTrackingMetric
Base class cho c√°c metrics c√≥ progress tracking:

```python
class ProgressTrackingMetric(base_metric.BaseMetric):
    def _init_progress_bar(self, total, desc):
        # Kh·ªüi t·∫°o progress bar cho metric
    
    def _update_progress(self, additional_info):
        # C·∫≠p nh·∫≠t progress
    
    def _close_progress_bar(self):
        # ƒê√≥ng progress bar
```

## üìà Metrics c√≥ s·∫µn

### Standard Metrics (Opik)
- `LevenshteinRatio`: T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng chu·ªói
- `Hallucination`: Ph√°t hi·ªán hallucination
- `Moderation`: Ki·ªÉm duy·ªát n·ªôi dung
- `Style`: ƒê√°nh gi√° phong c√°ch vi·∫øt

### Progress Metrics (Custom)
- `ProgressLevenshteinRatio`: Levenshtein v·ªõi progress tracking
- `ProgressHallucination`: Hallucination detection v·ªõi progress tracking
- `ProgressModeration`: Content moderation v·ªõi progress tracking

## üéØ V√≠ d·ª• s·ª≠ d·ª•ng

### 1. Evaluation nhanh v·ªõi progress tracking
```bash
make evaluate-llm-quick
```

Output:
```
üìä MathPal Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:30<00:00,  6.12s/sample]
üéâ MathPal Evaluation completed!
‚è±Ô∏è  Total time: 30.61s
üìà Average time per sample: 6.12s
üìä Total samples evaluated: 5
```

### 2. Evaluation t√πy ch·ªânh
```bash
make evaluate-llm-custom SAMPLES=3 EXPERIMENT="Quick Test"
```

### 3. Evaluation kh√¥ng c√≥ progress tracking (nhanh)
```bash
make evaluate-llm-fast
```

## üîç Debug v√† Troubleshooting

### 1. Ki·ªÉm tra environment
```bash
make test-env
```

### 2. Xem logs chi ti·∫øt
```bash
# Set log level to DEBUG
export LOG_LEVEL=DEBUG
make evaluate-llm-progress
```

### 3. Clean up cache
```bash
make clean
```

## üìä K·∫øt qu·∫£ evaluation

K·∫øt qu·∫£ evaluation s·∫Ω ƒë∆∞·ª£c hi·ªÉn th·ªã trong:
1. **Console**: Progress bar v√† summary
2. **Opik Dashboard**: Chi ti·∫øt metrics v√† traces
3. **Logs**: Th√¥ng tin debug v√† error

### V√≠ d·ª• output:
```
üìä Final Evaluation Results:
   levenshtein_ratio: 0.8234
   hallucination: 0.1567
   moderation: 0.0234
   style: 0.7890
```

## üöÄ Performance Tips

1. **S·ª≠ d·ª•ng `--no_progress_tracking`** cho evaluation nhanh
2. **Gi·ªõi h·∫°n s·ªë samples** v·ªõi `--max_samples` cho testing
3. **S·ª≠ d·ª•ng GPU** ƒë·ªÉ tƒÉng t·ªëc inference
4. **Clean cache** th∆∞·ªùng xuy√™n v·ªõi `make clean`

## üîó Links h·ªØu √≠ch

- [Opik Documentation](https://www.comet.com/docs/opik/)
- [TQDM Documentation](https://tqdm.github.io/)
- [MathPal Project](https://github.com/your-repo/mathpal)
