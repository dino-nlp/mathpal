#!/usr/bin/env python3
"""
Script to test model loading safely.
"""

import os
import sys
import torch
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

def test_model_loading():
    """Test model loading with safe configuration"""
    print("ğŸ” Testing Model Loading")
    print("=" * 40)
    
    try:
        # Import required modules
        from unsloth import FastModel
        from inference_pipeline.config import settings
        
        print(f"ğŸ“Š Model ID: {settings.MODEL_ID}")
        print(f"ğŸ“Š Max Input Tokens: {settings.MAX_INPUT_TOKENS}")
        
        # Test model loading
        print("ğŸ”„ Loading model...")
        model, processor = FastModel.from_pretrained(
            model_name=settings.MODEL_ID,
            dtype=None,  # Auto-detect
            max_seq_length=settings.MAX_INPUT_TOKENS,
            load_in_4bit=True,
            load_in_8bit=False,
            device_map="auto" if torch.cuda.is_available() else "cpu"
        )
        
        print("âœ… Model loaded successfully!")
        
        # Test processor
        print("ğŸ”„ Testing processor...")
        from unsloth import get_chat_template
        processor = get_chat_template(processor, "gemma-3n")
        print("âœ… Processor configured successfully!")
        
        # Test inference preparation
        print("ğŸ”„ Preparing for inference...")
        FastModel.for_inference(model)
        model.eval()
        print("âœ… Model prepared for inference!")
        
        # Test basic generation
        print("ğŸ”„ Testing basic generation...")
        messages = [{
            "role": "user",
            "content": [{
                "type": "text",
                "text": "Hello, how are you?",
            }]
        }]
        
        input_ids = processor.apply_chat_template(
            messages,
            add_generation_prompt=True,
            tokenize=True, 
            return_dict=True,
            return_tensors="pt",
        )
        
        # Determine device
        if hasattr(model, 'device'):
            device = model.device
        elif hasattr(model, 'hf_device_map'):
            device_map = model.hf_device_map
            if device_map:
                device = list(device_map.values())[0]
            else:
                device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # Move input_ids to device
        input_ids = {k: v.to(device) if hasattr(v, 'to') else v for k, v in input_ids.items()}
        
        with torch.no_grad():
            response = model.generate(
                **input_ids,
                max_new_tokens=10,  # Short for testing
                do_sample=True,
                temperature=1.0,
                top_p=0.95,
                top_k=64,
                pad_token_id=processor.eos_token_id,
                eos_token_id=processor.eos_token_id,
            )
        
        answer = processor.batch_decode(response, skip_special_tokens=False)[0]
        print(f"âœ… Generation test successful!")
        print(f"ğŸ“ Response: {answer[:100]}...")
        
        # Clean up
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        print("\nğŸ‰ All tests passed! Model is ready for evaluation.")
        return True
        
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """Run model loading test"""
    print("ğŸ” MathPal Model Loading Test")
    print("=" * 50)
    
    # Set environment variables
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    os.environ["TORCH_COMPILE_DISABLE"] = "1"
    os.environ["TORCH_LOGS"] = "off"
    
    # Disable TorchDynamo
    torch._dynamo.config.suppress_errors = True
    torch._dynamo.config.disable = True
    
    success = test_model_loading()
    
    if success:
        print("\nâœ… Model loading test completed successfully!")
        print("ğŸ¯ You can now run: make evaluate-llm-compatible")
    else:
        print("\nâŒ Model loading test failed!")
        print("ğŸ’¡ Check the error messages above for details")

if __name__ == "__main__":
    main()
